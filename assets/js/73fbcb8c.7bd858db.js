"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[53801],{28453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>a});var i=t(96540);const o={},r=i.createContext(o);function s(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(r.Provider,{value:n},e.children)}},74481:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"skills/sound_engineer/references/implementations","title":"Sound Engineering Implementation Reference","description":"Detailed code implementations for spatial audio, procedural sound, and middleware integration.","source":"@site/docs/skills/sound_engineer/references/implementations.md","sourceDirName":"skills/sound_engineer/references","slug":"/skills/sound_engineer/references/implementations","permalink":"/docs/skills/sound_engineer/references/implementations","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Sound Engineering Implementation Reference","sidebar_label":"Sound Engineering Implement...","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Sound Engineer","permalink":"/docs/skills/sound_engineer/"},"next":{"title":"Voice Audio Engineer","permalink":"/docs/skills/voice_audio_engineer/"}}');var o=t(74848),r=t(28453);const s={title:"Sound Engineering Implementation Reference",sidebar_label:"Sound Engineering Implement...",sidebar_position:1},a="Sound Engineering Implementation Reference",l={},c=[{value:"HRTF Spatial Audio Engine",id:"hrtf-spatial-audio-engine",level:2},{value:"Ambisonic Encoder/Decoder",id:"ambisonic-encoderdecoder",level:2},{value:"Procedural Footstep System",id:"procedural-footstep-system",level:2},{value:"Wind Synthesizer",id:"wind-synthesizer",level:2},{value:"Wwise Integration (Unreal)",id:"wwise-integration-unreal",level:2},{value:"Adaptive Music Manager",id:"adaptive-music-manager",level:2},{value:"Performance Benchmarks",id:"performance-benchmarks",level:2},{value:"Key References",id:"key-references",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"sound-engineering-implementation-reference",children:"Sound Engineering Implementation Reference"})}),"\n",(0,o.jsx)(n.p,{children:"Detailed code implementations for spatial audio, procedural sound, and middleware integration."}),"\n",(0,o.jsx)(n.h2,{id:"hrtf-spatial-audio-engine",children:"HRTF Spatial Audio Engine"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-cpp",children:"#include <vector>\n#include <complex>\n#include <cmath>\n\nclass SpatialAudioEngine {\nprivate:\n    struct HRTF_IR {\n        std::vector<float> left_ear;   // 512 samples typical\n        std::vector<float> right_ear;\n        float azimuth;    // Horizontal angle (0-360\xb0)\n        float elevation;  // Vertical angle (-90 to +90\xb0)\n    };\n\n    std::vector<HRTF_IR> hrtf_database;\n    int sample_rate = 48000;\n\npublic:\n    void load_hrtf_database(const std::string& path) {\n        // Load MIT KEMAR database (1800 IRs typical)\n    }\n\n    void process_spatial_sound(\n        const std::vector<float>& mono_input,\n        glm::vec3 source_position,\n        glm::vec3 listener_position,\n        glm::vec3 listener_forward,\n        glm::vec3 listener_up,\n        std::vector<float>& output_left,\n        std::vector<float>& output_right)\n    {\n        glm::vec3 to_source = glm::normalize(source_position - listener_position);\n        glm::vec3 listener_right = glm::cross(listener_forward, listener_up);\n\n        float azimuth = std::atan2(\n            glm::dot(to_source, listener_right),\n            glm::dot(to_source, listener_forward)\n        ) * 180.0f / M_PI;\n\n        float elevation = std::asin(glm::dot(to_source, listener_up)) * 180.0f / M_PI;\n\n        HRTF_IR hrtf = find_closest_hrtf(azimuth, elevation);\n        convolve(mono_input, hrtf.left_ear, output_left);\n        convolve(mono_input, hrtf.right_ear, output_right);\n\n        float distance = glm::length(source_position - listener_position);\n        float attenuation = calculate_distance_attenuation(distance);\n\n        for (auto& s : output_left) s *= attenuation;\n        for (auto& s : output_right) s *= attenuation;\n    }\n\nprivate:\n    float calculate_distance_attenuation(float distance) {\n        float reference = 1.0f;\n        if (distance < reference) return 1.0f;\n        return reference / (reference + (distance - reference));\n    }\n};\n"})}),"\n",(0,o.jsx)(n.h2,{id:"ambisonic-encoderdecoder",children:"Ambisonic Encoder/Decoder"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-cpp",children:"class AmbisonicEncoder {\npublic:\n    struct AmbisonicSignal {\n        std::vector<float> W;  // Omnidirectional\n        std::vector<float> X;  // Front-back\n        std::vector<float> Y;  // Left-right\n        std::vector<float> Z;  // Up-down\n    };\n\n    AmbisonicSignal encode(const std::vector<float>& mono, glm::vec3 direction) {\n        AmbisonicSignal out;\n        out.W.resize(mono.size());\n        out.X.resize(mono.size());\n        out.Y.resize(mono.size());\n        out.Z.resize(mono.size());\n\n        direction = glm::normalize(direction);\n        float w = 0.707f;\n\n        for (size_t i = 0; i < mono.size(); ++i) {\n            out.W[i] = mono[i] * w;\n            out.X[i] = mono[i] * direction.x;\n            out.Y[i] = mono[i] * direction.y;\n            out.Z[i] = mono[i] * direction.z;\n        }\n        return out;\n    }\n\n    void decode_binaural(const AmbisonicSignal& amb, const glm::quat& head_rotation,\n                         std::vector<float>& out_left, std::vector<float>& out_right) {\n        AmbisonicSignal rotated = rotate_ambisonic(amb, head_rotation);\n        out_left.resize(amb.W.size());\n        out_right.resize(amb.W.size());\n\n        // Virtual speaker positions (cube configuration)\n        std::vector<glm::vec3> speakers = {\n            {1,0,0}, {-1,0,0}, {0,1,0}, {0,-1,0},\n            {0,0,1}, {0,0,-1}, {0.707,0.707,0}, {-0.707,0.707,0}\n        };\n\n        for (size_t i = 0; i < out_left.size(); ++i) {\n            out_left[i] = out_right[i] = 0.0f;\n            for (const auto& sp : speakers) {\n                float sig = decode_direction(rotated, sp, i);\n                float pan = (sp.x + 1.0f) * 0.5f;\n                out_left[i] += sig * (1.0f - pan);\n                out_right[i] += sig * pan;\n            }\n        }\n    }\n};\n"})}),"\n",(0,o.jsx)(n.h2,{id:"procedural-footstep-system",children:"Procedural Footstep System"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-cpp",children:"class ProceduralFootsteps {\npublic:\n    enum class SurfaceType { Concrete, Wood, Grass, Gravel, Metal, Water };\n\n    struct Params {\n        float impact_force;    // 0-1\n        SurfaceType surface;\n        float wetness;         // 0-1\n        float debris_amount;   // 0-1\n    };\n\n    std::vector<float> generate(const Params& p, int sample_rate) {\n        std::vector<float> out(sample_rate / 2, 0.0f);  // 0.5 sec\n\n        // Impact synthesis\n        int impact_dur = int(0.02 * sample_rate);\n        for (int i = 0; i < impact_dur && i < out.size(); ++i) {\n            float t = float(i) / impact_dur;\n            float env = std::exp(-10.0f * t);\n            float noise = random(-1.0f, 1.0f);\n            float freq = get_resonance(p.surface);\n            float tone = std::sin(2.0f * M_PI * freq * t);\n            out[i] = (0.7f * noise + 0.3f * tone) * env * p.impact_force;\n        }\n\n        add_surface_texture(out, p, sample_rate);\n        if (p.debris_amount > 0.1f) add_debris(out, p);\n        apply_surface_eq(out, p.surface);\n\n        return out;\n    }\n\nprivate:\n    float get_resonance(SurfaceType s) {\n        switch(s) {\n            case SurfaceType::Concrete: return 150.0f;\n            case SurfaceType::Wood: return 250.0f;\n            case SurfaceType::Metal: return 500.0f;\n            case SurfaceType::Gravel: return 300.0f;\n            default: return 200.0f;\n        }\n    }\n};\n"})}),"\n",(0,o.jsx)(n.h2,{id:"wind-synthesizer",children:"Wind Synthesizer"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-cpp",children:"class WindSynthesizer {\npublic:\n    std::vector<float> generate(float speed, float gust, float duration, int sr) {\n        int samples = int(duration * sr);\n        std::vector<float> out(samples);\n        auto pink = generate_pink_noise(samples);\n\n        for (int i = 0; i < samples; ++i) {\n            float t = i / float(sr);\n            float gust_env = 0.5f + 0.5f * std::sin(2.0f * M_PI * (0.2f + gust * 0.5f) * t);\n            out[i] = pink[i] * (speed / 30.0f) * gust_env;\n        }\n\n        apply_bandpass(out, 100.0f, 2000.0f);\n        if (speed > 15.0f) add_whistle(out, speed, sr);\n        return out;\n    }\n\nprivate:\n    std::vector<float> generate_pink_noise(int n) {\n        std::vector<float> out(n);\n        float b0=0, b1=0, b2=0, b3=0, b4=0, b5=0, b6=0;\n        for (int i = 0; i < n; ++i) {\n            float w = random(-1.0f, 1.0f);\n            b0 = 0.99886f * b0 + w * 0.0555179f;\n            b1 = 0.99332f * b1 + w * 0.0750759f;\n            b2 = 0.96900f * b2 + w * 0.1538520f;\n            b3 = 0.86650f * b3 + w * 0.3104856f;\n            b4 = 0.55000f * b4 + w * 0.5329522f;\n            b5 = -0.7616f * b5 - w * 0.0168980f;\n            out[i] = (b0 + b1 + b2 + b3 + b4 + b5 + b6 + w * 0.5362f) * 0.11f;\n            b6 = w * 0.115926f;\n        }\n        return out;\n    }\n};\n"})}),"\n",(0,o.jsx)(n.h2,{id:"wwise-integration-unreal",children:"Wwise Integration (Unreal)"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-cpp",children:'#include "AkGameplayStatics.h"\n\nclass SpatialSoundManager {\npublic:\n    void play_3d(UAkAudioEvent* ev, FVector loc, AActor* owner = nullptr) {\n        UAkGameplayStatics::PostEventAtLocation(ev, loc, FRotator::ZeroRotator,\n            owner ? owner->GetWorld() : nullptr);\n    }\n\n    void set_rtpc(const FString& name, float val, AActor* owner = nullptr) {\n        UAkGameplayStatics::SetRTPCValue(*name, val, 0, owner);\n    }\n\n    void set_switch(const FString& group, const FString& state, AActor* owner) {\n        UAkGameplayStatics::SetSwitch(*group, *state, owner);\n    }\n};\n\n// Character footstep integration\nvoid OnFootDown(AActor* character, FVector location) {\n    FHitResult hit;\n    if (GetWorld()->LineTraceSingleByChannel(hit, location, location - FVector(0,0,100), ECC_Visibility)) {\n        FString surface = DetermineSurface(hit.PhysMaterial);\n        UAkGameplayStatics::SetSwitch(TEXT("Surface"), *surface, character);\n\n        float speed = character->GetVelocity().Size();\n        UAkGameplayStatics::SetRTPCValue(TEXT("Impact_Force"), FMath::Clamp(speed/600.0f, 0.0f, 1.0f), 0, character);\n        UAkGameplayStatics::PostEvent(FootstepEvent, character);\n    }\n}\n'})}),"\n",(0,o.jsx)(n.h2,{id:"adaptive-music-manager",children:"Adaptive Music Manager"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-cpp",children:'class AdaptiveMusicManager {\npublic:\n    enum class Intensity { Ambient, Low, Medium, High, Combat };\n\n    void set_intensity(Intensity level) {\n        const char* states[] = {"Ambient", "Low", "Medium", "High", "Combat"};\n        UAkGameplayStatics::SetState(TEXT("Music_Intensity"), TEXT(states[int(level)]));\n    }\n\n    void update_from_gameplay(int enemies, float health, bool combat) {\n        Intensity target = Intensity::Ambient;\n        if (!combat) target = Intensity::Ambient;\n        else if (enemies == 0) target = Intensity::Low;\n        else if (enemies < 3 && health > 0.5f) target = Intensity::Medium;\n        else if (enemies < 5 || health > 0.25f) target = Intensity::High;\n        else target = Intensity::Combat;\n        set_intensity(target);\n    }\n};\n'})}),"\n",(0,o.jsx)(n.h2,{id:"performance-benchmarks",children:"Performance Benchmarks"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Operation"}),(0,o.jsx)(n.th,{children:"CPU Time"}),(0,o.jsx)(n.th,{children:"Notes"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"HRTF Convolution (512-tap)"}),(0,o.jsx)(n.td,{children:"~2ms/source"}),(0,o.jsx)(n.td,{children:"Use FFT overlap-add"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Ambisonic encode"}),(0,o.jsx)(n.td,{children:"~0.1ms/source"}),(0,o.jsx)(n.td,{children:"Very efficient"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Ambisonic decode (binaural)"}),(0,o.jsx)(n.td,{children:"~1ms total"}),(0,o.jsx)(n.td,{children:"Supports many sources"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Procedural footstep"}),(0,o.jsx)(n.td,{children:"~1-2ms"}),(0,o.jsx)(n.td,{children:"vs 500KB per sample"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Wind synthesis"}),(0,o.jsx)(n.td,{children:"~0.5ms/frame"}),(0,o.jsx)(n.td,{children:"Real-time streaming"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Wwise event post"}),(0,o.jsx)(n.td,{children:"<0.1ms"}),(0,o.jsx)(n.td,{children:"Negligible"})]})]})]}),"\n",(0,o.jsx)(n.h2,{id:"key-references",children:"Key References"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"MIT KEMAR HRTF Database (public domain)"}),"\n",(0,o.jsx)(n.li,{children:"Google Resonance Audio (open source)"}),"\n",(0,o.jsx)(n.li,{children:"Wwise/FMOD Documentation (2024)"}),"\n",(0,o.jsx)(n.li,{children:'Farnell: "Designing Sound" (MIT Press)'}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);