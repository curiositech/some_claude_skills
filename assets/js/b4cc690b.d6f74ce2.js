"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[19958],{28453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>t});var i=s(96540);const a={},o=i.createContext(a);function r(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(o.Provider,{value:n},e.children)}},63859:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>p,frontMatter:()=>r,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"skills/voice_audio_engineer/references/implementations","title":"Voice Audio Implementation Reference","description":"Detailed code implementations for voice processing, loudness measurement, and speech analysis.","source":"@site/docs/skills/voice_audio_engineer/references/implementations.md","sourceDirName":"skills/voice_audio_engineer/references","slug":"/skills/voice_audio_engineer/references/implementations","permalink":"/docs/skills/voice_audio_engineer/references/implementations","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Voice Audio Implementation Reference","sidebar_label":"Voice Audio Implementation ...","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Voice Audio Engineer","permalink":"/docs/skills/voice_audio_engineer/"},"next":{"title":"Clip Aware Embeddings","permalink":"/docs/skills/clip_aware_embeddings/"}}');var a=s(74848),o=s(28453);const r={title:"Voice Audio Implementation Reference",sidebar_label:"Voice Audio Implementation ...",sidebar_position:1},t="Voice Audio Implementation Reference",l={},d=[{value:"Biquad Filter (Audio EQ Cookbook)",id:"biquad-filter-audio-eq-cookbook",level:2},{value:"LUFS Loudness Measurement",id:"lufs-loudness-measurement",level:2},{value:"Compressor Implementation",id:"compressor-implementation",level:2},{value:"De-esser Implementation",id:"de-esser-implementation",level:2},{value:"Voice Activity Detection (VAD)",id:"voice-activity-detection-vad",level:2},{value:"Audio Analysis Report Generator",id:"audio-analysis-report-generator",level:2},{value:"Loudness Standards Reference",id:"loudness-standards-reference",level:2},{value:"Digital Audio Theory Reference",id:"digital-audio-theory-reference",level:2},{value:"Key References",id:"key-references",level:2}];function c(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"voice-audio-implementation-reference",children:"Voice Audio Implementation Reference"})}),"\n",(0,a.jsx)(n.p,{children:"Detailed code implementations for voice processing, loudness measurement, and speech analysis."}),"\n",(0,a.jsx)(n.h2,{id:"biquad-filter-audio-eq-cookbook",children:"Biquad Filter (Audio EQ Cookbook)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom scipy import signal\n\nclass AudioFilters:\n    """Production-ready filter implementations."""\n\n    @staticmethod\n    def biquad_coefficients(filter_type: str, fc: float, fs: float,\n                           Q: float = 0.707, gain_db: float = 0) -> tuple:\n        """\n        Calculate biquad filter coefficients.\n        Uses Audio EQ Cookbook formulas (Robert Bristow-Johnson)\n        """\n        A = 10 ** (gain_db / 40)\n        w0 = 2 * np.pi * fc / fs\n        cos_w0 = np.cos(w0)\n        sin_w0 = np.sin(w0)\n        alpha = sin_w0 / (2 * Q)\n\n        if filter_type == \'lowpass\':\n            b0 = (1 - cos_w0) / 2\n            b1 = 1 - cos_w0\n            b2 = (1 - cos_w0) / 2\n            a0 = 1 + alpha\n            a1 = -2 * cos_w0\n            a2 = 1 - alpha\n\n        elif filter_type == \'highpass\':\n            b0 = (1 + cos_w0) / 2\n            b1 = -(1 + cos_w0)\n            b2 = (1 + cos_w0) / 2\n            a0 = 1 + alpha\n            a1 = -2 * cos_w0\n            a2 = 1 - alpha\n\n        elif filter_type == \'peaking\':\n            b0 = 1 + alpha * A\n            b1 = -2 * cos_w0\n            b2 = 1 - alpha * A\n            a0 = 1 + alpha / A\n            a1 = -2 * cos_w0\n            a2 = 1 - alpha / A\n\n        elif filter_type == \'highshelf\':\n            b0 = A * ((A + 1) + (A - 1) * cos_w0 + 2 * np.sqrt(A) * alpha)\n            b1 = -2 * A * ((A - 1) + (A + 1) * cos_w0)\n            b2 = A * ((A + 1) + (A - 1) * cos_w0 - 2 * np.sqrt(A) * alpha)\n            a0 = (A + 1) - (A - 1) * cos_w0 + 2 * np.sqrt(A) * alpha\n            a1 = 2 * ((A - 1) - (A + 1) * cos_w0)\n            a2 = (A + 1) - (A - 1) * cos_w0 - 2 * np.sqrt(A) * alpha\n\n        # Normalize\n        b = np.array([b0/a0, b1/a0, b2/a0])\n        a = np.array([1, a1/a0, a2/a0])\n        return b, a\n'})}),"\n",(0,a.jsx)(n.h2,{id:"lufs-loudness-measurement",children:"LUFS Loudness Measurement"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom scipy import signal\n\ndef measure_lufs(audio: np.ndarray, fs: int) -> float:\n    """\n    Measure integrated loudness per ITU-R BS.1770-4.\n    """\n    # Stage 1: K-weighting filter\n    b1, a1 = signal.butter(2, 1500 / (fs/2), btype=\'high\')\n    b2, a2 = signal.butter(2, 38 / (fs/2), btype=\'high\')\n\n    filtered = signal.lfilter(b1, a1, audio)\n    filtered = signal.lfilter(b2, a2, filtered)\n\n    # Stage 2: Mean square with gating\n    block_size = int(0.4 * fs)  # 400ms blocks\n    hop_size = int(0.1 * fs)    # 100ms overlap\n\n    block_loudness = []\n    for i in range(0, len(filtered) - block_size, hop_size):\n        block = filtered[i:i+block_size]\n        mean_square = np.mean(block ** 2)\n        block_loudness.append(-0.691 + 10 * np.log10(mean_square + 1e-10))\n\n    # Absolute threshold gate (-70 LUFS)\n    gated = [l for l in block_loudness if l > -70]\n    if not gated:\n        return -70.0\n\n    # Relative threshold gate (-10 LU below ungated mean)\n    ungated_mean = np.mean(gated)\n    relative_threshold = ungated_mean - 10\n    final_gated = [l for l in gated if l > relative_threshold]\n\n    return np.mean(final_gated) if final_gated else ungated_mean\n'})}),"\n",(0,a.jsx)(n.h2,{id:"compressor-implementation",children:"Compressor Implementation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import numpy as np\n\nclass Compressor:\n    """Production-quality dynamics compressor."""\n\n    def __init__(self, fs: int):\n        self.fs = fs\n        self.envelope = 0.0\n\n    def process(self, audio: np.ndarray,\n                threshold_db: float = -20,\n                ratio: float = 4.0,\n                attack_ms: float = 10,\n                release_ms: float = 100,\n                knee_db: float = 6,\n                makeup_db: float = 0) -> np.ndarray:\n        """Apply compression to audio signal."""\n        attack_coef = np.exp(-1 / (self.fs * attack_ms / 1000))\n        release_coef = np.exp(-1 / (self.fs * release_ms / 1000))\n\n        output = np.zeros_like(audio)\n\n        for i in range(len(audio)):\n            input_level = 20 * np.log10(abs(audio[i]) + 1e-10)\n\n            # Envelope follower\n            if input_level > self.envelope:\n                self.envelope = attack_coef * self.envelope + (1 - attack_coef) * input_level\n            else:\n                self.envelope = release_coef * self.envelope + (1 - release_coef) * input_level\n\n            # Gain computer with soft knee\n            over_threshold = self.envelope - threshold_db\n\n            if knee_db > 0 and over_threshold > -knee_db/2 and over_threshold < knee_db/2:\n                knee_factor = (over_threshold + knee_db/2) ** 2 / (2 * knee_db)\n                gain_db = -knee_factor * (1 - 1/ratio)\n            elif over_threshold >= knee_db/2:\n                gain_db = -(over_threshold - knee_db/2) * (1 - 1/ratio) - (knee_db/2) * (1 - 1/ratio)\n            else:\n                gain_db = 0\n\n            gain_linear = 10 ** ((gain_db + makeup_db) / 20)\n            output[i] = audio[i] * gain_linear\n\n        return output\n'})}),"\n",(0,a.jsx)(n.h2,{id:"de-esser-implementation",children:"De-esser Implementation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom scipy import signal\n\nclass DeEsser:\n    """Frequency-selective de-esser for sibilance control."""\n\n    def __init__(self, fs: int):\n        self.fs = fs\n\n    def process(self, audio: np.ndarray,\n                frequency: float = 6000,\n                threshold_db: float = -20,\n                reduction_db: float = 6,\n                q: float = 2.0) -> np.ndarray:\n        """\n        Reduce sibilance in voice recordings.\n\n        Args:\n            frequency: Center frequency for sibilance detection (5-8kHz typical)\n            threshold_db: Level above which de-essing activates\n            reduction_db: Maximum gain reduction\n            q: Bandwidth (higher = narrower)\n        """\n        # Create bandpass to detect sibilance\n        nyq = self.fs / 2\n        low = (frequency - frequency/q) / nyq\n        high = (frequency + frequency/q) / nyq\n        b, a = signal.butter(2, [low, high], btype=\'band\')\n\n        # Detect sibilance envelope\n        sibilance = signal.lfilter(b, a, audio)\n        envelope = np.abs(sibilance)\n\n        # Smooth envelope\n        smooth_coef = 0.99\n        smoothed = np.zeros_like(envelope)\n        smoothed[0] = envelope[0]\n        for i in range(1, len(envelope)):\n            smoothed[i] = smooth_coef * smoothed[i-1] + (1 - smooth_coef) * envelope[i]\n\n        # Calculate gain reduction\n        threshold_linear = 10 ** (threshold_db / 20)\n        reduction_linear = 10 ** (-reduction_db / 20)\n\n        output = audio.copy()\n        for i in range(len(audio)):\n            if smoothed[i] > threshold_linear:\n                gain = 1.0 - (1.0 - reduction_linear) * (smoothed[i] - threshold_linear) / smoothed[i]\n                output[i] *= gain\n\n        return output\n'})}),"\n",(0,a.jsx)(n.h2,{id:"voice-activity-detection-vad",children:"Voice Activity Detection (VAD)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import numpy as np\n\nclass VoiceActivityDetector:\n    """Simple energy-based VAD for voice detection."""\n\n    def __init__(self, fs: int, frame_ms: float = 20):\n        self.fs = fs\n        self.frame_size = int(fs * frame_ms / 1000)\n\n    def detect(self, audio: np.ndarray,\n               energy_threshold_db: float = -40,\n               min_speech_ms: float = 100) -> list[tuple[int, int]]:\n        """\n        Detect speech segments in audio.\n\n        Returns: List of (start_sample, end_sample) tuples\n        """\n        num_frames = len(audio) // self.frame_size\n        is_speech = np.zeros(num_frames, dtype=bool)\n\n        for i in range(num_frames):\n            frame = audio[i * self.frame_size:(i + 1) * self.frame_size]\n            energy_db = 20 * np.log10(np.sqrt(np.mean(frame ** 2)) + 1e-10)\n            is_speech[i] = energy_db > energy_threshold_db\n\n        # Merge short gaps, remove short segments\n        min_frames = int(min_speech_ms / (self.frame_size / self.fs * 1000))\n\n        # Simple hangover\n        for i in range(1, len(is_speech) - 1):\n            if is_speech[i-1] and is_speech[i+1]:\n                is_speech[i] = True\n\n        # Extract segments\n        segments = []\n        in_segment = False\n        start = 0\n\n        for i, speech in enumerate(is_speech):\n            if speech and not in_segment:\n                start = i * self.frame_size\n                in_segment = True\n            elif not speech and in_segment:\n                end = i * self.frame_size\n                if (end - start) / self.fs * 1000 >= min_speech_ms:\n                    segments.append((start, end))\n                in_segment = False\n\n        if in_segment:\n            segments.append((start, len(audio)))\n\n        return segments\n'})}),"\n",(0,a.jsx)(n.h2,{id:"audio-analysis-report-generator",children:"Audio Analysis Report Generator"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import numpy as np\nfrom scipy.fft import rfft, rfftfreq\n\ndef analyze_voice_audio(audio: np.ndarray, fs: int) -> dict:\n    \"\"\"Comprehensive voice audio analysis.\"\"\"\n\n    # Mono for analysis\n    if len(audio.shape) > 1:\n        mono = np.mean(audio, axis=1)\n    else:\n        mono = audio\n\n    # Level measurements\n    peak_db = 20 * np.log10(np.max(np.abs(mono)) + 1e-10)\n    rms_db = 20 * np.log10(np.sqrt(np.mean(mono**2)) + 1e-10)\n    crest_factor = peak_db - rms_db\n    lufs = measure_lufs(mono, fs)\n    dc_offset = np.mean(mono)\n\n    # Spectral analysis\n    spectrum = np.abs(rfft(mono))\n    freqs = rfftfreq(len(mono), 1/fs)\n    spectral_centroid = np.sum(freqs * spectrum) / np.sum(spectrum)\n\n    # Voice-specific metrics\n    # Fundamental frequency estimation (simple autocorrelation)\n    autocorr = np.correlate(mono[:4096], mono[:4096], mode='full')\n    autocorr = autocorr[len(autocorr)//2:]\n    # Find first peak after initial decay\n    min_lag = int(fs / 500)  # 500Hz max\n    max_lag = int(fs / 50)   # 50Hz min\n    peak_lag = np.argmax(autocorr[min_lag:max_lag]) + min_lag\n    f0_estimate = fs / peak_lag if peak_lag > 0 else 0\n\n    return {\n        'peak_db': peak_db,\n        'rms_db': rms_db,\n        'crest_factor': crest_factor,\n        'lufs': lufs,\n        'dc_offset': dc_offset,\n        'spectral_centroid': spectral_centroid,\n        'f0_estimate': f0_estimate,\n        'duration_seconds': len(mono) / fs,\n        'sample_rate': fs\n    }\n\ndef generate_recommendations(analysis: dict) -> list[str]:\n    \"\"\"Generate processing recommendations from analysis.\"\"\"\n    recs = []\n\n    if analysis['peak_db'] > -0.5:\n        recs.append(\"Peaks near 0dBFS - risk of clipping; add limiter\")\n    if analysis['lufs'] > -14:\n        recs.append(\"Too loud for streaming (-14 LUFS target)\")\n    if analysis['lufs'] < -20:\n        recs.append(\"Consider increasing overall level\")\n    if analysis['crest_factor'] < 6:\n        recs.append(\"Low crest factor - may sound over-compressed\")\n    if abs(analysis['dc_offset']) > 0.01:\n        recs.append(\"DC offset detected - apply high-pass filter at 80Hz\")\n    if analysis['spectral_centroid'] < 1500:\n        recs.append(\"Voice sounds muddy - consider high shelf boost at 3kHz\")\n    if analysis['spectral_centroid'] > 4000:\n        recs.append(\"Voice sounds harsh - consider reducing 2-4kHz\")\n\n    return recs if recs else [\"Audio looks good!\"]\n"})}),"\n",(0,a.jsx)(n.h2,{id:"loudness-standards-reference",children:"Loudness Standards Reference"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"LOUDNESS UNITS (ITU-R BS.1770)\n\nLUFS (Loudness Units Full Scale)\n\u251c\u2500\u2500 Integrated: Average loudness over entire program\n\u251c\u2500\u2500 Short-term: 3-second sliding window\n\u251c\u2500\u2500 Momentary: 400ms sliding window\n\u2514\u2500\u2500 True Peak: Maximum sample value with intersample peaks\n\nDELIVERY STANDARDS\n\u251c\u2500\u2500 Streaming (Spotify, Apple): -14 LUFS, -1 dBTP\n\u251c\u2500\u2500 Broadcast (EBU R128): -23 LUFS \xb11, -1 dBTP\n\u251c\u2500\u2500 Broadcast (ATSC A/85): -24 LKFS \xb12, -2 dBTP\n\u251c\u2500\u2500 Podcast: -16 to -19 LUFS (dialogue norm)\n\u251c\u2500\u2500 YouTube: -14 LUFS (normalized)\n\u2514\u2500\u2500 Audiobook (ACX): -18 to -23 dBFS RMS, -3 dBFS peak\n\nLOUDNESS RANGE (LRA)\n\u251c\u2500\u2500 Classical: 15-20 LU\n\u251c\u2500\u2500 Film: 10-15 LU\n\u251c\u2500\u2500 Pop music: 5-8 LU\n\u2514\u2500\u2500 Broadcast speech: 3-6 LU\n"})}),"\n",(0,a.jsx)(n.h2,{id:"digital-audio-theory-reference",children:"Digital Audio Theory Reference"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"SAMPLE RATES\n\u251c\u2500\u2500 44.1kHz: CD standard, captures up to 22.05kHz\n\u251c\u2500\u2500 48kHz: Video standard (cleaner for frame sync)\n\u251c\u2500\u2500 96kHz: High-resolution, better for processing headroom\n\u2514\u2500\u2500 Why 44.1kHz? Derived from video: 44100 = 3\xd73\xd75\xd75\xd77\xd77\xd72\n\nBIT DEPTH \u2192 DYNAMIC RANGE\n\u251c\u2500\u2500 Dynamic Range (dB) \u2248 6.02 \xd7 bits + 1.76\n\u251c\u2500\u2500 16-bit: ~96 dB (CD quality)\n\u251c\u2500\u2500 24-bit: ~144 dB (professional)\n\u2514\u2500\u2500 32-bit float: ~1528 dB (effectively infinite)\n\nDITHERING\n\u251c\u2500\u2500 Required when reducing bit depth (24\u219216)\n\u251c\u2500\u2500 TPDF (triangular): Standard, mathematically optimal\n\u2514\u2500\u2500 Shaped: Noise pushed above hearing range\n"})}),"\n",(0,a.jsx)(n.h2,{id:"key-references",children:"Key References"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:'ITU-R BS.1770: "Algorithms to measure audio programme loudness"'}),"\n",(0,a.jsx)(n.li,{children:'EBU R128: "Loudness normalisation and permitted maximum level"'}),"\n",(0,a.jsx)(n.li,{children:'AES-6id: "Personal Sound Exposure"'}),"\n",(0,a.jsx)(n.li,{children:'Bristow-Johnson, R.: "Audio EQ Cookbook" (filter formulas)'}),"\n",(0,a.jsxs)(n.li,{children:["Blauert, J. (1997): ",(0,a.jsx)(n.em,{children:"Spatial Hearing"})," (MIT Press)"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}}}]);