"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[11905],{28453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>o});var t=i(96540);const a={},s=t.createContext(a);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),t.createElement(s.Provider,{value:n},e.children)}},79053:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"skills/wedding_immortalist/references/gaussian-splatting-pipeline","title":"3D Gaussian Splatting Pipeline for Weddings","description":"Complete Technical Pipeline","source":"@site/docs/skills/wedding_immortalist/references/gaussian-splatting-pipeline.md","sourceDirName":"skills/wedding_immortalist/references","slug":"/skills/wedding_immortalist/references/gaussian-splatting-pipeline","permalink":"/docs/skills/wedding_immortalist/references/gaussian-splatting-pipeline","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"3D Gaussian Splatting Pipeline for Weddings","sidebar_label":"3D Gaussian Splatting Pipel...","sidebar_position":2}}');var a=i(74848),s=i(28453);const r={title:"3D Gaussian Splatting Pipeline for Weddings",sidebar_label:"3D Gaussian Splatting Pipel...",sidebar_position:2},o="3D Gaussian Splatting Pipeline for Weddings",l={},c=[{value:"Complete Technical Pipeline",id:"complete-technical-pipeline",level:2},{value:"Overview",id:"overview",level:3},{value:"Phase 1: Data Ingestion",id:"phase-1-data-ingestion",level:2},{value:"Video Frame Extraction",id:"video-frame-extraction",level:3},{value:"Photo Organization",id:"photo-organization",level:3},{value:"Phase 2: COLMAP Structure from Motion",id:"phase-2-colmap-structure-from-motion",level:2},{value:"Feature Extraction",id:"feature-extraction",level:3},{value:"Handling Multiple Spaces",id:"handling-multiple-spaces",level:3},{value:"Phase 3: 3DGS Training",id:"phase-3-3dgs-training",level:2},{value:"Training Configuration",id:"training-configuration",level:3},{value:"Training Script",id:"training-script",level:3},{value:"Phase 4: Web Viewer Integration",id:"phase-4-web-viewer-integration",level:2},{value:"Viewer Architecture",id:"viewer-architecture",level:3},{value:"Hardware Requirements",id:"hardware-requirements",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Quality Checklist",id:"quality-checklist",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"3d-gaussian-splatting-pipeline-for-weddings",children:"3D Gaussian Splatting Pipeline for Weddings"})}),"\n",(0,a.jsx)(n.h2,{id:"complete-technical-pipeline",children:"Complete Technical Pipeline"}),"\n",(0,a.jsx)(n.h3,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(n.p,{children:"3D Gaussian Splatting (3DGS) creates photorealistic, real-time renderable 3D scenes from photos/video. For weddings, we're reconstructing entire venues as explorable memory spaces."}),"\n",(0,a.jsx)(n.h2,{id:"phase-1-data-ingestion",children:"Phase 1: Data Ingestion"}),"\n",(0,a.jsx)(n.h3,{id:"video-frame-extraction",children:"Video Frame Extraction"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import cv2\nimport os\nfrom pathlib import Path\n\ndef extract_frames(video_path: str, output_dir: str, fps: float = 2.0):\n    """\n    Extract frames from wedding video at optimal rate for 3DGS.\n\n    Why 2-3 fps?\n    - Wedding videos are typically 30fps\n    - Adjacent frames are nearly identical (redundant)\n    - 2-3fps maintains 80%+ overlap while reducing processing 10x\n    - More frames \u2260 better quality after sufficient overlap\n    """\n    cap = cv2.VideoCapture(video_path)\n    video_fps = cap.get(cv2.CAP_PROP_FPS)\n    frame_interval = int(video_fps / fps)\n\n    Path(output_dir).mkdir(parents=True, exist_ok=True)\n\n    frame_count = 0\n    saved_count = 0\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        if frame_count % frame_interval == 0:\n            # Check for blur before saving\n            laplacian_var = cv2.Laplacian(frame, cv2.CV_64F).var()\n            if laplacian_var > 100:  # Reject blurry frames\n                cv2.imwrite(f"{output_dir}/frame_{saved_count:06d}.jpg", frame)\n                saved_count += 1\n\n        frame_count += 1\n\n    cap.release()\n    return saved_count\n\n# Quality thresholds\nBLUR_THRESHOLD = 100  # Laplacian variance\nMIN_IMAGES_PER_SPACE = 50\nOPTIMAL_IMAGES_PER_SPACE = 150\nMAX_IMAGES_PER_SPACE = 300  # Diminishing returns after this\n'})}),"\n",(0,a.jsx)(n.h3,{id:"photo-organization",children:"Photo Organization"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from datetime import datetime\nfrom PIL import Image\nfrom PIL.ExifTags import TAGS\nimport shutil\n\ndef organize_wedding_photos(source_dir: str, output_dir: str):\n    """\n    Organize photos by time and location for multi-space reconstruction.\n    """\n    photos = []\n\n    for img_path in Path(source_dir).glob("**/*.{jpg,jpeg,JPG,JPEG,png,PNG}"):\n        try:\n            img = Image.open(img_path)\n            exif = img._getexif()\n\n            timestamp = None\n            gps = None\n\n            if exif:\n                for tag_id, value in exif.items():\n                    tag = TAGS.get(tag_id, tag_id)\n                    if tag == "DateTimeOriginal":\n                        timestamp = datetime.strptime(value, "%Y:%m:%d %H:%M:%S")\n                    elif tag == "GPSInfo":\n                        gps = value\n\n            photos.append({\n                \'path\': img_path,\n                \'timestamp\': timestamp,\n                \'gps\': gps,\n                \'resolution\': img.size\n            })\n        except Exception as e:\n            print(f"Skipping {img_path}: {e}")\n\n    # Sort by timestamp\n    photos.sort(key=lambda x: x[\'timestamp\'] or datetime.min)\n\n    # Cluster into spaces based on time gaps\n    spaces = cluster_into_spaces(photos)\n\n    return spaces\n\ndef cluster_into_spaces(photos, gap_threshold_minutes=15):\n    """\n    Cluster photos into distinct spaces/moments based on time gaps.\n\n    Typical wedding timeline:\n    - Getting ready (1-2 hours)\n    - Ceremony (30-60 min)\n    - Cocktail hour (1 hour)\n    - Reception entrance + dinner (1-2 hours)\n    - Dancing + party (2-3 hours)\n    """\n    spaces = []\n    current_space = []\n\n    for i, photo in enumerate(photos):\n        if i == 0:\n            current_space.append(photo)\n            continue\n\n        prev_time = photos[i-1][\'timestamp\']\n        curr_time = photo[\'timestamp\']\n\n        if prev_time and curr_time:\n            gap = (curr_time - prev_time).total_seconds() / 60\n            if gap > gap_threshold_minutes:\n                spaces.append(current_space)\n                current_space = []\n\n        current_space.append(photo)\n\n    if current_space:\n        spaces.append(current_space)\n\n    return spaces\n'})}),"\n",(0,a.jsx)(n.h2,{id:"phase-2-colmap-structure-from-motion",children:"Phase 2: COLMAP Structure from Motion"}),"\n",(0,a.jsx)(n.h3,{id:"feature-extraction",children:"Feature Extraction"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\n# colmap_sfm.sh - Structure from Motion pipeline\n\nWORKSPACE=$1\nIMAGE_PATH=$2\n\n# 1. Feature extraction with SIFT\ncolmap feature_extractor \\\n    --database_path $WORKSPACE/database.db \\\n    --image_path $IMAGE_PATH \\\n    --ImageReader.single_camera 0 \\\n    --ImageReader.camera_model OPENCV \\\n    --SiftExtraction.max_image_size 3200 \\\n    --SiftExtraction.max_num_features 8192 \\\n    --SiftExtraction.first_octave -1 \\\n    --SiftExtraction.num_threads -1\n\n# 2. Feature matching\n# For wedding photos with lots of similar views, exhaustive matching works best\ncolmap exhaustive_matcher \\\n    --database_path $WORKSPACE/database.db \\\n    --SiftMatching.guided_matching 1 \\\n    --SiftMatching.max_ratio 0.8 \\\n    --SiftMatching.max_distance 0.7\n\n# 3. Sparse reconstruction (SfM)\nmkdir -p $WORKSPACE/sparse\ncolmap mapper \\\n    --database_path $WORKSPACE/database.db \\\n    --image_path $IMAGE_PATH \\\n    --output_path $WORKSPACE/sparse \\\n    --Mapper.ba_refine_focal_length 1 \\\n    --Mapper.ba_refine_principal_point 1 \\\n    --Mapper.ba_refine_extra_params 1\n\n# 4. Undistort images for dense reconstruction\ncolmap image_undistorter \\\n    --image_path $IMAGE_PATH \\\n    --input_path $WORKSPACE/sparse/0 \\\n    --output_path $WORKSPACE/dense \\\n    --output_type COLMAP\n\necho "SfM complete. Check $WORKSPACE/sparse/0 for camera poses."\n'})}),"\n",(0,a.jsx)(n.h3,{id:"handling-multiple-spaces",children:"Handling Multiple Spaces"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"def merge_reconstructions(spaces: list, output_path: str):\n    \"\"\"\n    For weddings spanning multiple distinct spaces (ceremony, reception),\n    we have two options:\n\n    1. SEPARATE SCENES: Train individual 3DGS models per space\n       - Pros: Better quality per scene, simpler training\n       - Cons: Need scene transitions in viewer\n\n    2. MERGED SCENE: Use shared features to align all spaces\n       - Pros: Seamless navigation\n       - Cons: Harder to reconstruct, may need manual alignment\n\n    For weddings, SEPARATE SCENES is usually better.\n    \"\"\"\n\n    # Create navigation graph between spaces\n    navigation = {\n        'spaces': [],\n        'transitions': []\n    }\n\n    for i, space in enumerate(spaces):\n        navigation['spaces'].append({\n            'id': f'space_{i}',\n            'name': space['name'],  # e.g., \"ceremony\", \"reception\"\n            'model_path': f'{output_path}/space_{i}',\n            'entry_point': space.get('entry_camera'),  # Best starting view\n            'thumbnail': space.get('thumbnail')\n        })\n\n    # Define logical transitions\n    navigation['transitions'] = [\n        {'from': 'ceremony', 'to': 'cocktail', 'type': 'fade'},\n        {'from': 'cocktail', 'to': 'reception', 'type': 'walk'},\n        # etc.\n    ]\n\n    return navigation\n"})}),"\n",(0,a.jsx)(n.h2,{id:"phase-3-3dgs-training",children:"Phase 3: 3DGS Training"}),"\n",(0,a.jsx)(n.h3,{id:"training-configuration",children:"Training Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# wedding_3dgs_config.py\n\nWEDDING_3DGS_CONFIG = {\n    # Training iterations\n    'iterations': 50_000,  # High quality for permanent archive\n\n    # Densification settings\n    'densify_from_iter': 500,\n    'densify_until_iter': 15_000,\n    'densification_interval': 100,\n    'opacity_reset_interval': 3000,\n\n    # Gaussian parameters\n    'sh_degree': 3,  # Full spherical harmonics for complex lighting\n    'percent_dense': 0.01,\n    'densify_grad_threshold': 0.0002,\n\n    # Pruning\n    'min_opacity': 0.005,\n    'max_screen_size': 20,  # Max pixel size before splitting\n\n    # Learning rates\n    'position_lr_init': 0.00016,\n    'position_lr_final': 0.0000016,\n    'position_lr_delay_mult': 0.01,\n    'position_lr_max_steps': 30_000,\n    'feature_lr': 0.0025,\n    'opacity_lr': 0.05,\n    'scaling_lr': 0.005,\n    'rotation_lr': 0.001,\n\n    # Loss weights\n    'lambda_dssim': 0.2,  # Structural similarity weight\n\n    # Performance\n    'white_background': False,  # Wedding venues rarely have white bg\n    'data_device': 'cuda',\n    'convert_SHs_python': False,\n    'compute_cov3D_python': False,\n}\n\n# Quality presets\nQUALITY_PRESETS = {\n    'preview': {\n        'iterations': 7_000,\n        'densify_until_iter': 5_000,\n        'description': 'Quick preview in ~5 minutes'\n    },\n    'standard': {\n        'iterations': 30_000,\n        'densify_until_iter': 15_000,\n        'description': 'Good quality in ~30 minutes'\n    },\n    'high': {\n        'iterations': 50_000,\n        'densify_until_iter': 20_000,\n        'description': 'High quality in ~1 hour'\n    },\n    'archival': {\n        'iterations': 100_000,\n        'densify_until_iter': 30_000,\n        'description': 'Maximum quality in ~3 hours'\n    }\n}\n"})}),"\n",(0,a.jsx)(n.h3,{id:"training-script",children:"Training Script"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import torch\nfrom gaussian_splatting import GaussianModel, train\nfrom scene import Scene\nimport os\n\ndef train_wedding_scene(\n    source_path: str,\n    output_path: str,\n    quality: str = 'high'\n):\n    \"\"\"\n    Train 3DGS model for a wedding space.\n\n    Args:\n        source_path: COLMAP output directory\n        output_path: Where to save trained model\n        quality: 'preview', 'standard', 'high', or 'archival'\n    \"\"\"\n\n    config = {**WEDDING_3DGS_CONFIG, **QUALITY_PRESETS[quality]}\n\n    # Initialize Gaussian model\n    gaussians = GaussianModel(config['sh_degree'])\n\n    # Load scene from COLMAP\n    scene = Scene(source_path, gaussians)\n\n    # Training loop with wedding-specific optimizations\n    for iteration in range(config['iterations']):\n        # Render\n        render_pkg = render(\n            scene.getTrainCameras()[iteration % len(scene.getTrainCameras())],\n            gaussians,\n            background=torch.zeros(3).cuda()\n        )\n\n        # Loss\n        image = render_pkg['render']\n        gt_image = scene.getTrainCameras()[iteration % len(scene.getTrainCameras())].original_image\n\n        l1_loss = torch.abs(image - gt_image).mean()\n        ssim_loss = 1.0 - ssim(image, gt_image)\n        loss = (1 - config['lambda_dssim']) * l1_loss + config['lambda_dssim'] * ssim_loss\n\n        loss.backward()\n\n        # Densification\n        if iteration < config['densify_until_iter']:\n            if iteration > config['densify_from_iter'] and iteration % config['densification_interval'] == 0:\n                gaussians.densify_and_prune(\n                    config['densify_grad_threshold'],\n                    config['min_opacity'],\n                    scene.cameras_extent,\n                    config['max_screen_size']\n                )\n\n        # Optimizer step\n        gaussians.optimizer.step()\n        gaussians.optimizer.zero_grad()\n\n        # Logging\n        if iteration % 1000 == 0:\n            print(f\"Iteration {iteration}: Loss = {loss.item():.6f}\")\n\n        # Save checkpoint\n        if iteration % 10000 == 0:\n            torch.save(gaussians.capture(), f\"{output_path}/checkpoint_{iteration}.pth\")\n\n    # Final save\n    gaussians.save_ply(f\"{output_path}/point_cloud.ply\")\n\n    return output_path\n"})}),"\n",(0,a.jsx)(n.h2,{id:"phase-4-web-viewer-integration",children:"Phase 4: Web Viewer Integration"}),"\n",(0,a.jsx)(n.h3,{id:"viewer-architecture",children:"Viewer Architecture"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-typescript",children:"// WeddingViewer.tsx\nimport { useEffect, useRef, useState } from 'react';\nimport * as THREE from 'three';\nimport { SplatLoader } from '@mkkellogg/gaussian-splats-3d';\n\ninterface WeddingViewerProps {\n  spaces: WeddingSpace[];\n  moments: TheatreMoment[];\n  onMomentClick: (moment: TheatreMoment) => void;\n}\n\nexport function WeddingViewer({ spaces, moments, onMomentClick }: WeddingViewerProps) {\n  const containerRef = useRef<HTMLDivElement>(null);\n  const viewerRef = useRef<GaussianSplatViewer | null>(null);\n  const [currentSpace, setCurrentSpace] = useState(0);\n  const [loading, setLoading] = useState(true);\n\n  useEffect(() => {\n    if (!containerRef.current) return;\n\n    // Initialize Three.js scene\n    const scene = new THREE.Scene();\n    const camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);\n    const renderer = new THREE.WebGLRenderer({ antialias: true });\n\n    // Load Gaussian Splat\n    const loader = new SplatLoader();\n    loader.load(spaces[currentSpace].modelPath, (splat) => {\n      scene.add(splat);\n      setLoading(false);\n    });\n\n    // Add moment markers\n    moments.forEach(moment => {\n      const marker = createMomentMarker(moment);\n      marker.onClick = () => onMomentClick(moment);\n      scene.add(marker);\n    });\n\n    // Controls\n    const controls = new OrbitControls(camera, renderer.domElement);\n    controls.enableDamping = true;\n\n    // Animation loop\n    function animate() {\n      requestAnimationFrame(animate);\n      controls.update();\n      renderer.render(scene, camera);\n    }\n    animate();\n\n    return () => {\n      renderer.dispose();\n    };\n  }, [currentSpace, spaces, moments]);\n\n  return (\n    <div ref={containerRef} className=\"wedding-viewer\">\n      {loading && <LoadingOverlay theme={spaces[currentSpace].theme} />}\n      <SpaceNavigator\n        spaces={spaces}\n        current={currentSpace}\n        onChange={setCurrentSpace}\n      />\n    </div>\n  );\n}\n"})}),"\n",(0,a.jsx)(n.h2,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Component"}),(0,a.jsx)(n.th,{children:"Minimum"}),(0,a.jsx)(n.th,{children:"Recommended"}),(0,a.jsx)(n.th,{children:"Notes"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"GPU"}),(0,a.jsx)(n.td,{children:"RTX 3060 12GB"}),(0,a.jsx)(n.td,{children:"RTX 4080 16GB"}),(0,a.jsx)(n.td,{children:"VRAM is the bottleneck"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"RAM"}),(0,a.jsx)(n.td,{children:"32GB"}),(0,a.jsx)(n.td,{children:"64GB"}),(0,a.jsx)(n.td,{children:"For large photo sets"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Storage"}),(0,a.jsx)(n.td,{children:"100GB SSD"}),(0,a.jsx)(n.td,{children:"500GB NVMe"}),(0,a.jsx)(n.td,{children:"Fast I/O matters"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"CPU"}),(0,a.jsx)(n.td,{children:"8 cores"}),(0,a.jsx)(n.td,{children:"16+ cores"}),(0,a.jsx)(n.td,{children:"For COLMAP parallelization"})]})]})]}),"\n",(0,a.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,a.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Sparse point cloud"}),": Not enough image overlap"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Solution: Add more photos from in-between angles"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Floaters"}),": Random gaussians in empty space"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Solution: Increase opacity pruning, reduce learning rate"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Blurry reconstruction"}),": Motion blur in source images"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Solution: Filter frames with Laplacian variance < 100"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Memory errors"}),": Too many gaussians"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Solution: Reduce densification, increase pruning"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"quality-checklist",children:"Quality Checklist"}),"\n",(0,a.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Minimum 50 images per distinct space"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","60-80% overlap between adjacent views"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","No motion blur (Laplacian variance > 100)"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Consistent lighting (avoid mixed indoor/outdoor)"]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",disabled:!0})," ","All guests' faces visible in at least 3 angles"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);