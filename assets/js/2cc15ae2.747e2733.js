"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[29069],{10267:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"skills/llm_streaming_response_handler/references/vercel-ai-sdk","title":"Vercel AI SDK Integration Patterns","description":"Production patterns for using Vercel AI SDK to handle LLM streaming across providers.","source":"@site/docs/skills/llm_streaming_response_handler/references/vercel-ai-sdk.md","sourceDirName":"skills/llm_streaming_response_handler/references","slug":"/skills/llm_streaming_response_handler/references/vercel-ai-sdk","permalink":"/docs/skills/llm_streaming_response_handler/references/vercel-ai-sdk","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Vercel AI SDK Integration Patterns","sidebar_label":"Vercel AI SDK Integration P...","sidebar_position":3}}');var r=t(74848),i=t(28453);const a={title:"Vercel AI SDK Integration Patterns",sidebar_label:"Vercel AI SDK Integration P...",sidebar_position:3},o="Vercel AI SDK Integration Patterns",l={},c=[{value:"Why Vercel AI SDK?",id:"why-vercel-ai-sdk",level:2},{value:"Installation",id:"installation",level:2},{value:"Pattern 1: Basic Streaming (useChat Hook)",id:"pattern-1-basic-streaming-usechat-hook",level:2},{value:"Pattern 2: Streaming with Tools (Function Calling)",id:"pattern-2-streaming-with-tools-function-calling",level:2},{value:"Pattern 3: Multi-Provider Fallback",id:"pattern-3-multi-provider-fallback",level:2},{value:"Pattern 4: Custom useChat with Abort",id:"pattern-4-custom-usechat-with-abort",level:2},{value:"Pattern 5: Streaming Object (Structured Output)",id:"pattern-5-streaming-object-structured-output",level:2},{value:"Pattern 6: Token Counting &amp; Cost Tracking",id:"pattern-6-token-counting--cost-tracking",level:2},{value:"Pattern 7: Middleware (Logging, Auth)",id:"pattern-7-middleware-logging-auth",level:2},{value:"Pattern 8: Custom System Prompts",id:"pattern-8-custom-system-prompts",level:2},{value:"Pattern 9: Temperature &amp; Model Parameters",id:"pattern-9-temperature--model-parameters",level:2},{value:"Production Checklist",id:"production-checklist",level:2},{value:"Comparison: Vercel AI SDK vs Raw API",id:"comparison-vercel-ai-sdk-vs-raw-api",level:2},{value:"Resources",id:"resources",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"vercel-ai-sdk-integration-patterns",children:"Vercel AI SDK Integration Patterns"})}),"\n",(0,r.jsx)(n.p,{children:"Production patterns for using Vercel AI SDK to handle LLM streaming across providers."}),"\n",(0,r.jsx)(n.h2,{id:"why-vercel-ai-sdk",children:"Why Vercel AI SDK?"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Unified API"})," across OpenAI, Anthropic, Google, Cohere, Hugging Face:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Same streaming interface"}),"\n",(0,r.jsx)(n.li,{children:"Automatic retries"}),"\n",(0,r.jsx)(n.li,{children:"Built-in token counting"}),"\n",(0,r.jsx)(n.li,{children:"React hooks"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Timeline"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["2023: Released as ",(0,r.jsx)(n.code,{children:"ai"})," package"]}),"\n",(0,r.jsx)(n.li,{children:"2024: Became de facto standard for Next.js AI apps"}),"\n",(0,r.jsx)(n.li,{children:"2024+: Supports 15+ LLM providers"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"installation",children:"Installation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"npm install ai @ai-sdk/openai @ai-sdk/anthropic\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"pattern-1-basic-streaming-usechat-hook",children:"Pattern 1: Basic Streaming (useChat Hook)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"'use client';\n\nimport { useChat } from 'ai/react';\n\nexport function ChatInterface() {\n  const { messages, input, handleInputChange, handleSubmit, isLoading } = useChat({\n    api: '/api/chat'\n  });\n\n  return (\n    <div>\n      <div className=\"messages\">\n        {messages.map((message) => (\n          <div key={message.id} className={`message ${message.role}`}>\n            {message.content}\n          </div>\n        ))}\n      </div>\n\n      <form onSubmit={handleSubmit}>\n        <input\n          value={input}\n          onChange={handleInputChange}\n          disabled={isLoading}\n          placeholder=\"Type a message...\"\n        />\n        <button type=\"submit\" disabled={isLoading}>\n          Send\n        </button>\n      </form>\n    </div>\n  );\n}\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Server Route"})," (app/api/chat/route.ts):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"import { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nexport const runtime = 'edge';\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = await streamText({\n    model: openai('gpt-4-turbo'),\n    messages\n  });\n\n  return result.toAIStreamResponse();\n}\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"What you get"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Streaming automatically handled"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Message history managed"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Loading states"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Error handling"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Optimistic updates"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"pattern-2-streaming-with-tools-function-calling",children:"Pattern 2: Streaming with Tools (Function Calling)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"import { openai } from '@ai-sdk/openai';\nimport { streamText, tool } from 'ai';\nimport { z } from 'zod';\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = await streamText({\n    model: openai('gpt-4-turbo'),\n    messages,\n    tools: {\n      getWeather: tool({\n        description: 'Get weather for a location',\n        parameters: z.object({\n          location: z.string().describe('City name')\n        }),\n        execute: async ({ location }) => {\n          const weather = await fetchWeather(location);\n          return weather;\n        }\n      }),\n      createReminder: tool({\n        description: 'Create a reminder',\n        parameters: z.object({\n          text: z.string(),\n          time: z.string()\n        }),\n        execute: async ({ text, time }) => {\n          await db.reminders.create({ text, time });\n          return { success: true };\n        }\n      })\n    }\n  });\n\n  return result.toAIStreamResponse();\n}\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Client"})," (automatic tool execution):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"const { messages } = useChat({\n  api: '/api/chat',\n  onToolCall: ({ toolCall }) => {\n    console.log('Tool called:', toolCall.toolName, toolCall.args);\n  }\n});\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"pattern-3-multi-provider-fallback",children:"Pattern 3: Multi-Provider Fallback"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"import { openai } from '@ai-sdk/openai';\nimport { anthropic } from '@ai-sdk/anthropic';\nimport { streamText } from 'ai';\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  try {\n    // Try OpenAI first\n    const result = await streamText({\n      model: openai('gpt-4-turbo'),\n      messages\n    });\n\n    return result.toAIStreamResponse();\n  } catch (error) {\n    // Fallback to Anthropic\n    console.log('OpenAI failed, falling back to Claude');\n\n    const result = await streamText({\n      model: anthropic('claude-3-sonnet-20240229'),\n      messages\n    });\n\n    return result.toAIStreamResponse();\n  }\n}\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"pattern-4-custom-usechat-with-abort",children:"Pattern 4: Custom useChat with Abort"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"import { useChat } from 'ai/react';\n\nexport function ChatWithAbort() {\n  const {\n    messages,\n    input,\n    handleInputChange,\n    handleSubmit,\n    isLoading,\n    stop\n  } = useChat({\n    api: '/api/chat'\n  });\n\n  return (\n    <div>\n      {/* Messages */}\n      {messages.map((m) => (\n        <div key={m.id}>{m.content}</div>\n      ))}\n\n      {/* Form */}\n      <form onSubmit={handleSubmit}>\n        <input value={input} onChange={handleInputChange} />\n        <button type=\"submit\" disabled={isLoading}>\n          Send\n        </button>\n        {isLoading && (\n          <button type=\"button\" onClick={stop}>\n            Stop\n          </button>\n        )}\n      </form>\n    </div>\n  );\n}\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"pattern-5-streaming-object-structured-output",children:"Pattern 5: Streaming Object (Structured Output)"}),"\n",(0,r.jsx)(n.p,{children:"For structured data (not just text)."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"import { openai } from '@ai-sdk/openai';\nimport { streamObject } from 'ai';\nimport { z } from 'zod';\n\nconst recipeSchema = z.object({\n  name: z.string(),\n  ingredients: z.array(z.object({\n    name: z.string(),\n    amount: z.string()\n  })),\n  instructions: z.array(z.string())\n});\n\nexport async function POST(req: Request) {\n  const { prompt } = await req.json();\n\n  const result = await streamObject({\n    model: openai('gpt-4-turbo'),\n    schema: recipeSchema,\n    prompt: `Generate a recipe for: ${prompt}`\n  });\n\n  return result.toTextStreamResponse();\n}\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Client"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"'use client';\n\nimport { experimental_useObject as useObject } from 'ai/react';\n\nexport function RecipeGenerator() {\n  const { object, submit, isLoading } = useObject({\n    api: '/api/generate-recipe',\n    schema: recipeSchema\n  });\n\n  return (\n    <div>\n      <button onClick={() => submit('chocolate cake')}>\n        Generate Recipe\n      </button>\n\n      {object && (\n        <div>\n          <h2>{object.name}</h2>\n          <h3>Ingredients:</h3>\n          <ul>\n            {object.ingredients?.map((ing, i) => (\n              <li key={i}>{ing.amount} {ing.name}</li>\n            ))}\n          </ul>\n          <h3>Instructions:</h3>\n          <ol>\n            {object.instructions?.map((step, i) => (\n              <li key={i}>{step}</li>\n            ))}\n          </ol>\n        </div>\n      )}\n    </div>\n  );\n}\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"pattern-6-token-counting--cost-tracking",children:"Pattern 6: Token Counting & Cost Tracking"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"import { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = await streamText({\n    model: openai('gpt-4-turbo'),\n    messages,\n    onFinish: ({ usage }) => {\n      console.log('Token usage:', usage);\n      // { promptTokens: 50, completionTokens: 100, totalTokens: 150 }\n\n      const cost = calculateCost(usage, 'gpt-4-turbo');\n      await db.usage.create({ cost, tokens: usage.totalTokens });\n    }\n  });\n\n  return result.toAIStreamResponse();\n}\n\nfunction calculateCost(usage: any, model: string): number {\n  const pricing = {\n    'gpt-4-turbo': { input: 0.00001, output: 0.00003 } // per token\n  };\n\n  const rates = pricing[model];\n  return (\n    usage.promptTokens * rates.input +\n    usage.completionTokens * rates.output\n  );\n}\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"pattern-7-middleware-logging-auth",children:"Pattern 7: Middleware (Logging, Auth)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"import { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nexport async function POST(req: Request) {\n  // Authentication\n  const session = await getSession(req);\n  if (!session) {\n    return new Response('Unauthorized', { status: 401 });\n  }\n\n  // Rate limiting\n  const { allowed } = await rateLimit(session.user.id);\n  if (!allowed) {\n    return new Response('Rate limit exceeded', { status: 429 });\n  }\n\n  const { messages } = await req.json();\n\n  // Log request\n  await db.chatLog.create({\n    userId: session.user.id,\n    prompt: messages[messages.length - 1].content,\n    timestamp: new Date()\n  });\n\n  const result = await streamText({\n    model: openai('gpt-4-turbo'),\n    messages,\n    onFinish: async ({ text, usage }) => {\n      // Log response\n      await db.chatLog.update({\n        response: text,\n        tokens: usage.totalTokens\n      });\n    }\n  });\n\n  return result.toAIStreamResponse();\n}\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"pattern-8-custom-system-prompts",children:"Pattern 8: Custom System Prompts"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"const result = await streamText({\n  model: openai('gpt-4-turbo'),\n  system: `You are a helpful assistant for a cooking app.\n           Only provide recipes and cooking advice.\n           Keep responses under 200 words.`,\n  messages\n});\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"pattern-9-temperature--model-parameters",children:"Pattern 9: Temperature & Model Parameters"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-typescript",children:"const result = await streamText({\n  model: openai('gpt-4-turbo'),\n  messages,\n  temperature: 0.7,  // Creativity (0-2)\n  maxTokens: 500,    // Response length limit\n  topP: 0.9,         // Nucleus sampling\n  frequencyPenalty: 0.5,  // Reduce repetition\n  presencePenalty: 0.5    // Encourage new topics\n});\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"production-checklist",children:"Production Checklist"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u25a1 Rate limiting per user\n\u25a1 Token usage tracking\n\u25a1 Cost monitoring\n\u25a1 Error logging\n\u25a1 Authentication required\n\u25a1 System prompt defined\n\u25a1 Max tokens set\n\u25a1 onFinish handler for analytics\n\u25a1 Multi-provider fallback\n\u25a1 Tool calling validated (if used)\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"comparison-vercel-ai-sdk-vs-raw-api",children:"Comparison: Vercel AI SDK vs Raw API"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Feature"}),(0,r.jsx)(n.th,{children:"Vercel AI SDK"}),(0,r.jsx)(n.th,{children:"Raw OpenAI API"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Setup complexity"}),(0,r.jsx)(n.td,{children:"Low"}),(0,r.jsx)(n.td,{children:"Medium"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Provider switching"}),(0,r.jsx)(n.td,{children:"Easy"}),(0,r.jsx)(n.td,{children:"Manual"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"React hooks"}),(0,r.jsx)(n.td,{children:"Built-in"}),(0,r.jsx)(n.td,{children:"Custom"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Error handling"}),(0,r.jsx)(n.td,{children:"Automatic"}),(0,r.jsx)(n.td,{children:"Manual"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Retries"}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"No"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Token counting"}),(0,r.jsx)(n.td,{children:"Built-in"}),(0,r.jsx)(n.td,{children:"Manual"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Type safety"}),(0,r.jsx)(n.td,{children:"\u2705"}),(0,r.jsx)(n.td,{children:"\u26a0\ufe0f"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Bundle size"}),(0,r.jsx)(n.td,{children:"+50KB"}),(0,r.jsx)(n.td,{children:"+10KB (minimal)"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Use Vercel AI SDK when"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Building with Next.js"}),"\n",(0,r.jsx)(n.li,{children:"Need React hooks"}),"\n",(0,r.jsx)(n.li,{children:"Want provider flexibility"}),"\n",(0,r.jsx)(n.li,{children:"Prefer TypeScript safety"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Use raw API when"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Non-React framework"}),"\n",(0,r.jsx)(n.li,{children:"Bundle size critical"}),"\n",(0,r.jsx)(n.li,{children:"Need custom streaming logic"}),"\n",(0,r.jsx)(n.li,{children:"Provider-specific features"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"resources",children:"Resources"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://sdk.vercel.ai/docs",children:"Vercel AI SDK Docs"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/vercel/ai/tree/main/examples",children:"Examples"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://sdk.vercel.ai/docs/reference",children:"API Reference"})}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var s=t(96540);const r={},i=s.createContext(r);function a(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);