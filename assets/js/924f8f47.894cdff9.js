"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[65185],{13217:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>r,default:()=>u,frontMatter:()=>s,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"skills/speech_pathology_ai/references/acoustic-analysis","title":"Acoustic Analysis for Speech Pathology","description":"Speech Analysis with Signal Processing","source":"@site/docs/skills/speech_pathology_ai/references/acoustic-analysis.md","sourceDirName":"skills/speech_pathology_ai/references","slug":"/skills/speech_pathology_ai/references/acoustic-analysis","permalink":"/docs/skills/speech_pathology_ai/references/acoustic-analysis","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Acoustic Analysis for Speech Pathology","sidebar_label":"Acoustic Analysis for Speec...","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Speech Pathology Ai","permalink":"/docs/skills/speech_pathology_ai/"},"next":{"title":"AI Models for Speech Pathology","permalink":"/docs/skills/speech_pathology_ai/references/ai-models"}}');var a=t(74848),i=t(28453);const s={title:"Acoustic Analysis for Speech Pathology",sidebar_label:"Acoustic Analysis for Speec...",sidebar_position:1},r="Acoustic Analysis for Speech Pathology",c={},l=[{value:"Speech Analysis with Signal Processing",id:"speech-analysis-with-signal-processing",level:2},{value:"Vocal Tract Visualization",id:"vocal-tract-visualization",level:2}];function p(n){const e={code:"code",h1:"h1",h2:"h2",header:"header",pre:"pre",...(0,i.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"acoustic-analysis-for-speech-pathology",children:"Acoustic Analysis for Speech Pathology"})}),"\n",(0,a.jsx)(e.h2,{id:"speech-analysis-with-signal-processing",children:"Speech Analysis with Signal Processing"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import numpy as np\nimport librosa\nfrom scipy import signal\n\nclass PhonemeAnalyzer:\n    def __init__(self, sample_rate=16000):\n        self.sr = sample_rate\n\n    def extract_formants(self, audio, n_formants=4):\n        """\n        Extract formant frequencies using Linear Predictive Coding (LPC)\n\n        Formants are resonant frequencies of the vocal tract\n        F1, F2 determine vowel identity\n        """\n        # Pre-emphasis filter (boost high frequencies)\n        pre_emphasis = 0.97\n        emphasized = np.append(audio[0], audio[1:] - pre_emphasis * audio[:-1])\n\n        # Frame the signal\n        frame_length = int(0.025 * self.sr)  # 25ms frames\n        frame_step = int(0.010 * self.sr)    # 10ms step\n\n        # LPC analysis\n        lpc_order = 12  # Typical for formant extraction\n        formants_over_time = []\n\n        for i in range(0, len(emphasized) - frame_length, frame_step):\n            frame = emphasized[i:i + frame_length]\n\n            # Apply window\n            windowed = frame * np.hamming(len(frame))\n\n            # Compute LPC coefficients\n            lpc_coeffs = librosa.lpc(windowed, order=lpc_order)\n\n            # Find roots of LPC polynomial\n            roots = np.roots(lpc_coeffs)\n            roots = roots[np.imag(roots) >= 0]  # Keep positive frequencies\n\n            # Convert to frequencies\n            angles = np.arctan2(np.imag(roots), np.real(roots))\n            frequencies = angles * (self.sr / (2 * np.pi))\n\n            # Sort and extract formants\n            formants = sorted(frequencies)[:n_formants]\n            formants_over_time.append(formants)\n\n        return np.array(formants_over_time)\n\n    def compute_mfcc(self, audio, n_mfcc=13):\n        """\n        Mel-Frequency Cepstral Coefficients\n        Standard features for speech recognition\n        """\n        mfcc = librosa.feature.mfcc(\n            y=audio,\n            sr=self.sr,\n            n_mfcc=n_mfcc,\n            n_fft=512,\n            hop_length=160\n        )\n\n        # Delta and delta-delta features (velocity and acceleration)\n        mfcc_delta = librosa.feature.delta(mfcc)\n        mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n\n        return np.vstack([mfcc, mfcc_delta, mfcc_delta2])\n\n    def detect_voice_onset(self, audio, threshold_db=-40):\n        """\n        Detect Voice Onset Time (VOT) - critical for /p/ vs /b/ distinction\n        """\n        # Compute short-time energy\n        frame_length = int(0.010 * self.sr)  # 10ms\n        energy = np.array([\n            np.sum(audio[i:i+frame_length]**2)\n            for i in range(0, len(audio) - frame_length, frame_length//2)\n        ])\n\n        # Convert to dB\n        energy_db = 10 * np.log10(energy + 1e-10)\n\n        # Find first frame above threshold\n        onset_idx = np.argmax(energy_db > threshold_db)\n        onset_time = onset_idx * (frame_length // 2) / self.sr\n\n        return onset_time\n\n    def analyze_articulation_precision(self, audio, target_phoneme):\n        """\n        Measure how precisely a phoneme was articulated\n        """\n        formants = self.extract_formants(audio)\n\n        # Target formant values for common vowels\n        target_formants = {\n            \'/i/\': (280, 2250),  # F1, F2 for "ee"\n            \'/u/\': (300, 870),   # "oo"\n            \'/a/\': (730, 1090),  # "ah"\n            \'/\u025b/\': (530, 1840),  # "eh"\n        }\n\n        if target_phoneme in target_formants:\n            target_f1, target_f2 = target_formants[target_phoneme]\n\n            # Mean formants\n            mean_f1 = np.mean(formants[:, 0])\n            mean_f2 = np.mean(formants[:, 1])\n\n            # Euclidean distance in formant space\n            distance = np.sqrt(\n                ((mean_f1 - target_f1) / target_f1)**2 +\n                ((mean_f2 - target_f2) / target_f2)**2\n            )\n\n            # Convert to accuracy score (0-100)\n            accuracy = max(0, 100 * (1 - distance))\n\n            return {\n                \'accuracy\': accuracy,\n                \'measured_f1\': mean_f1,\n                \'measured_f2\': mean_f2,\n                \'target_f1\': target_f1,\n                \'target_f2\': target_f2\n            }\n\n        return None\n'})}),"\n",(0,a.jsx)(e.h2,{id:"vocal-tract-visualization",children:"Vocal Tract Visualization"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-javascript",children:"// WebGL visualization of articulatory positions\nclass VocalTractVisualizer {\n    constructor(canvas) {\n        this.scene = new THREE.Scene();\n        this.camera = new THREE.PerspectiveCamera(75, canvas.width / canvas.height, 0.1, 1000);\n        this.renderer = new THREE.WebGLRenderer({ canvas, alpha: true });\n\n        this.buildVocalTract();\n    }\n\n    buildVocalTract() {\n        // Simplified 2D sagittal view of vocal tract\n        const outline = new THREE.Shape();\n\n        // Palate (roof of mouth)\n        outline.moveTo(0, 3);\n        outline.quadraticCurveTo(2, 3.5, 4, 3);\n        outline.quadraticCurveTo(6, 2.5, 7, 1.5);\n\n        // Pharynx (throat)\n        outline.lineTo(7, -2);\n\n        // Tongue base\n        outline.quadraticCurveTo(6, -2.5, 4, -2.5);\n\n        // Chin\n        outline.lineTo(0, -2.5);\n        outline.lineTo(0, 3);\n\n        const geometry = new THREE.ShapeGeometry(outline);\n        const material = new THREE.MeshBasicMaterial({\n            color: 0xffc0cb,\n            side: THREE.DoubleSide,\n            transparent: true,\n            opacity: 0.3\n        });\n\n        this.vocalTract = new THREE.Mesh(geometry, material);\n        this.scene.add(this.vocalTract);\n\n        // Create movable tongue\n        this.createTongue();\n\n        // Create lips\n        this.createLips();\n    }\n\n    createTongue() {\n        const tongueShape = new THREE.Shape();\n        tongueShape.moveTo(0, -2);\n        tongueShape.quadraticCurveTo(2, -1.5, 4, -1);\n        tongueShape.quadraticCurveTo(5, -0.5, 5.5, 0);\n        tongueShape.quadraticCurveTo(5, -1, 4, -1.5);\n        tongueShape.quadraticCurveTo(2, -2, 0, -2);\n\n        const geometry = new THREE.ShapeGeometry(tongueShape);\n        const material = new THREE.MeshBasicMaterial({ color: 0xff6b6b });\n\n        this.tongue = new THREE.Mesh(geometry, material);\n        this.scene.add(this.tongue);\n    }\n\n    createLips() {\n        // Upper lip\n        const upperLip = new THREE.Mesh(\n            new THREE.BoxGeometry(0.5, 0.2, 0.3),\n            new THREE.MeshBasicMaterial({ color: 0xff8888 })\n        );\n        upperLip.position.set(-0.5, 2.5, 0);\n\n        // Lower lip\n        const lowerLip = new THREE.Mesh(\n            new THREE.BoxGeometry(0.5, 0.2, 0.3),\n            new THREE.MeshBasicMaterial({ color: 0xff8888 })\n        );\n        lowerLip.position.set(-0.5, -2, 0);\n\n        this.upperLip = upperLip;\n        this.lowerLip = lowerLip;\n\n        this.scene.add(upperLip);\n        this.scene.add(lowerLip);\n    }\n\n    animateArticulation(phoneme) {\n        // Articulatory positions for different phonemes\n        const positions = {\n            '/i/': {  // \"ee\"\n                tongueFront: 5.5,\n                tongueHeight: 2.5,\n                lipRounding: 0,\n                jawOpening: 0.3\n            },\n            '/u/': {  // \"oo\"\n                tongueFront: 6,\n                tongueHeight: 2,\n                lipRounding: 1,\n                jawOpening: 0.5\n            },\n            '/a/': {  // \"ah\"\n                tongueFront: 3,\n                tongueHeight: -1,\n                lipRounding: 0,\n                jawOpening: 2\n            },\n            '/s/': {  // \"s\"\n                tongueFront: 4.5,\n                tongueHeight: 1.5,\n                lipRounding: 0,\n                jawOpening: 0.5\n            }\n        };\n\n        if (phoneme in positions) {\n            const pos = positions[phoneme];\n\n            // Animate tongue using GSAP or custom tween\n            this.animateTongue(pos.tongueFront, pos.tongueHeight);\n            this.animateLips(pos.lipRounding, pos.jawOpening);\n        }\n    }\n\n    animateTongue(frontPos, height) {\n        // Morph tongue shape to target position\n        console.log(`Animating tongue to front: ${frontPos}, height: ${height}`);\n    }\n\n    animateLips(rounding, opening) {\n        // Animate lip position\n        this.lowerLip.position.y = -2 - opening;\n\n        // Lip rounding (move forward)\n        this.upperLip.position.x = -0.5 - rounding * 0.3;\n        this.lowerLip.position.x = -0.5 - rounding * 0.3;\n    }\n\n    render() {\n        this.renderer.render(this.scene, this.camera);\n    }\n}\n"})})]})}function u(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(p,{...n})}):p(n)}},28453:(n,e,t)=>{t.d(e,{R:()=>s,x:()=>r});var o=t(96540);const a={},i=o.createContext(a);function s(n){const e=o.useContext(i);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:s(n.components),o.createElement(i.Provider,{value:e},n.children)}}}]);