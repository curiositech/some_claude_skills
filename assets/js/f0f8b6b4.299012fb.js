"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[52581],{28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>c});var i=t(96540);const s={},r=i.createContext(s);function o(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(r.Provider,{value:n},e.children)}},68565:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>c,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"skills/photo_content_recognition_curation_expert/references/photo-indexing","title":"Photo Indexing Pipeline Reference","description":"Quick Indexing Before First Collage","source":"@site/docs/skills/photo_content_recognition_curation_expert/references/photo-indexing.md","sourceDirName":"skills/photo_content_recognition_curation_expert/references","slug":"/skills/photo_content_recognition_curation_expert/references/photo-indexing","permalink":"/docs/skills/photo_content_recognition_curation_expert/references/photo-indexing","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Photo Indexing Pipeline Reference","sidebar_label":"Photo Indexing Pipeline Ref...","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Perceptual Hashing Implemen...","permalink":"/docs/skills/photo_content_recognition_curation_expert/references/perceptual-hashing"},"next":{"title":"Event Detection Temporal Intelligence Expert","permalink":"/docs/skills/event_detection_temporal_intelligence_expert/"}}');var s=t(74848),r=t(28453);const o={title:"Photo Indexing Pipeline Reference",sidebar_label:"Photo Indexing Pipeline Ref...",sidebar_position:4},c="Photo Indexing Pipeline Reference",a={},l=[{value:"Quick Indexing Before First Collage",id:"quick-indexing-before-first-collage",level:2},{value:"PhotoIndex Container",id:"photoindex-container",level:2},{value:"Complete Curation Pipeline",id:"complete-curation-pipeline",level:2},{value:"Performance Benchmarks",id:"performance-benchmarks",level:2},{value:"Integration Points",id:"integration-points",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"photo-indexing-pipeline-reference",children:"Photo Indexing Pipeline Reference"})}),"\n",(0,s.jsx)(n.h2,{id:"quick-indexing-before-first-collage",children:"Quick Indexing Before First Collage"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Goal:"})," Efficiently index 10K+ photos before creating first collage."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Strategy:"})," Pipeline with caching, batching, and GPU acceleration."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class QuickPhotoIndexer:\n    """\n    Fast photo indexing pipeline for large libraries.\n\n    Extracts:\n    - Perceptual hashes (de-duplication)\n    - Face embeddings (people clustering)\n    - CLIP embeddings (semantic search)\n    - Color palettes\n    - Aesthetic scores\n\n    Optimized for 10K photos in < 5 minutes.\n    """\n\n    def __init__(self, cache_dir=\'./photo_cache\'):\n        self.cache_dir = Path(cache_dir)\n        self.cache_dir.mkdir(exist_ok=True)\n\n        self.dino_hasher = DINOHasher()\n        self.face_extractor = FaceEmbeddingExtractor()\n        self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")\n        self.aesthetic_scorer = NIMAPredictor()\n\n    def index_photo_library(self, photo_paths, batch_size=32):\n        """Index entire photo library."""\n        index = PhotoIndex()\n\n        # Check cache\n        cached_index = self.load_cache()\n        if cached_index:\n            print(f"Loaded {len(cached_index)} photos from cache")\n            index = cached_index\n\n        # Find new photos\n        new_photos = [p for p in photo_paths if p not in index.photos]\n\n        if not new_photos:\n            return index\n\n        print(f"Indexing {len(new_photos)} new photos...")\n\n        # Process in batches\n        for batch_start in range(0, len(new_photos), batch_size):\n            batch_paths = new_photos[batch_start:batch_start + batch_size]\n            batch_images = [Image.open(p).convert(\'RGB\') for p in batch_paths]\n\n            # BATCHED FEATURE EXTRACTION\n            hashes = [self.dino_hasher.compute_hash(img) for img in batch_images]\n            clip_embeddings = self.extract_clip_batch(batch_images)\n            faces_batch = [self.face_extractor.extract_faces(img) for img in batch_images]\n            palettes = [extract_palette(img) for img in batch_images]\n            aesthetic_scores = self.aesthetic_scorer.predict_batch(batch_images)\n\n            # Store in index\n            for i, photo_path in enumerate(batch_paths):\n                index.add_photo(\n                    photo_id=str(photo_path),\n                    perceptual_hash=hashes[i],\n                    clip_embedding=clip_embeddings[i],\n                    faces=faces_batch[i],\n                    color_palette=palettes[i],\n                    aesthetic_score=aesthetic_scores[i]\n                )\n\n            print(f"Indexed {batch_start + len(batch_paths)}/{len(new_photos)}")\n\n        self.save_cache(index)\n\n        # Post-processing\n        print("Clustering faces...")\n        index.cluster_faces()\n\n        print("Detecting duplicates...")\n        index.detect_duplicates()\n\n        print("Detecting events...")\n        index.detect_events()\n\n        return index\n\n    def extract_clip_batch(self, images):\n        """Extract CLIP embeddings in batch (GPU-accelerated)."""\n        from transformers import CLIPProcessor\n\n        processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")\n        inputs = processor(images=images, return_tensors="pt", padding=True)\n        inputs = {k: v.to(self.dino_hasher.device) for k, v in inputs.items()}\n\n        with torch.no_grad():\n            embeddings = self.clip_model.get_image_features(**inputs)\n\n        return embeddings.cpu().numpy()\n\n    def save_cache(self, index):\n        cache_path = self.cache_dir / \'photo_index.pkl\'\n        with open(cache_path, \'wb\') as f:\n            pickle.dump(index, f)\n\n    def load_cache(self):\n        cache_path = self.cache_dir / \'photo_index.pkl\'\n        if cache_path.exists():\n            with open(cache_path, \'rb\') as f:\n                return pickle.load(f)\n        return None\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"photoindex-container",children:"PhotoIndex Container"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'@dataclass\nclass PhotoIndex:\n    """Container for all photo features."""\n\n    photos: Dict[str, Dict] = field(default_factory=dict)\n    face_clusters: Dict = field(default_factory=dict)\n    duplicate_groups: List = field(default_factory=list)\n    events: List = field(default_factory=list)\n\n    def add_photo(self, photo_id, **features):\n        self.photos[photo_id] = features\n\n    def cluster_faces(self):\n        """Cluster all faces using HDBSCAN."""\n        all_faces = []\n        for photo_id, features in self.photos.items():\n            for face in features.get(\'faces\', []):\n                all_faces.append({\n                    \'photo_id\': photo_id,\n                    \'embedding\': face[\'embedding\']\n                })\n\n        if len(all_faces) < 3:\n            return\n\n        embeddings = [f[\'embedding\'] for f in all_faces]\n        clusterer = HDBSCANFaceClustering(min_cluster_size=3)\n        labels = clusterer.cluster_faces(embeddings)\n\n        for face, label in zip(all_faces, labels):\n            if label == -1:\n                continue\n            self.face_clusters.setdefault(label, []).append(face)\n\n    def detect_duplicates(self):\n        """Detect duplicate photo groups."""\n        detector = HybridDuplicateDetector()\n\n        for photo_id, features in self.photos.items():\n            detector.phash_index[photo_id] = features[\'perceptual_hash\']\n            detector.dinohash_index[photo_id] = features[\'perceptual_hash\']\n\n        self.duplicate_groups = detector.find_duplicates()\n\n    def detect_events(self):\n        """Detect temporal events (requires timestamps + GPS)."""\n        # Use ST-DBSCAN from event-detection-temporal-intelligence-expert\n        pass\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"complete-curation-pipeline",children:"Complete Curation Pipeline"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def curate_photos_for_collage(photo_library_path, target_count=100):\n    \"\"\"\n    Complete curation pipeline.\n\n    Steps:\n    1. Index all photos (quick indexing)\n    2. Filter inappropriate (NSFW, screenshots, mundane)\n    3. De-duplicate (keep best from each group)\n    4. Cluster by person (prioritize important people)\n    5. Detect events (prioritize significant events)\n    6. Select diverse set\n    \"\"\"\n    # 1. QUICK INDEXING\n    indexer = QuickPhotoIndexer()\n    photo_paths = list(Path(photo_library_path).glob('**/*.jpg'))\n    index = indexer.index_photo_library(photo_paths)\n\n    # 2. FILTERING\n    filtered_photos = []\n    for photo_id, features in index.photos.items():\n        if features.get('is_nsfw', False):\n            continue\n        if features.get('is_screenshot', False):\n            continue\n        if features['aesthetic_score'] < 0.3:\n            continue\n        filtered_photos.append(photo_id)\n\n    # 3. DE-DUPLICATION\n    duplicates_removed = set()\n    for dup_group in index.duplicate_groups:\n        if len(dup_group) < 2:\n            continue\n        best = max(dup_group, key=lambda pid: index.photos[pid]['aesthetic_score'])\n        for pid in dup_group:\n            if pid != best:\n                duplicates_removed.add(pid)\n\n    filtered_photos = [p for p in filtered_photos if p not in duplicates_removed]\n\n    # 4. PRIORITIZE IMPORTANT PEOPLE\n    person_importance = {}\n    for cluster_id, faces in index.face_clusters.items():\n        importance = min(1.0, len(faces) / 100)\n        person_importance[cluster_id] = importance\n\n    for photo_id in filtered_photos:\n        faces = index.photos[photo_id].get('faces', [])\n        for face in faces:\n            for cluster_id, cluster_faces in index.face_clusters.items():\n                if any(f['photo_id'] == photo_id for f in cluster_faces):\n                    boost = person_importance.get(cluster_id, 0) * 0.2\n                    index.photos[photo_id]['aesthetic_score'] += boost\n\n    # 5. EVENT-AWARE SELECTION\n    # Use event-detection-temporal-intelligence-expert\n\n    # 6. FINAL SELECTION\n    filtered_photos.sort(\n        key=lambda pid: index.photos[pid]['aesthetic_score'],\n        reverse=True\n    )\n\n    return filtered_photos[:target_count]\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"performance-benchmarks",children:"Performance Benchmarks"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Target Performance (Swift/Metal/Core ML):"})}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Operation"}),(0,s.jsx)(n.th,{children:"10K Photos"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Perceptual hashing"}),(0,s.jsx)(n.td,{children:"< 2 minutes"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"CLIP embeddings"}),(0,s.jsx)(n.td,{children:"< 3 minutes (GPU)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Face detection"}),(0,s.jsx)(n.td,{children:"< 4 minutes"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Color palettes"}),(0,s.jsx)(n.td,{children:"< 1 minute"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Aesthetic scoring"}),(0,s.jsx)(n.td,{children:"< 2 minutes (GPU)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Face clustering"}),(0,s.jsx)(n.td,{children:"< 30 seconds"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Duplicate detection"}),(0,s.jsx)(n.td,{children:"< 20 seconds"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Total (first run)"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"~13 minutes"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Incremental updates"})}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"< 1 minute"})})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"integration-points",children:"Integration Points"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"event-detection-temporal-intelligence-expert"}),": Temporal event clustering"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"color-theory-palette-harmony-expert"}),": Color extraction"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"collage-layout-expert"}),": Photo selection for collages"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"clip-aware-embeddings"}),": Semantic search and similarity"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);