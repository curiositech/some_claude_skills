"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[88190],{28453:(e,n,t)=>{t.d(n,{R:()=>d,x:()=>a});var i=t(96540);const r={},s=i.createContext(r);function d(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:d(e.components),i.createElement(s.Provider,{value:n},e.children)}},74119:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>d,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"skills/automatic_stateful_prompt_improver/references/embedding-architecture","title":"Embedding Architecture for Prompt Learning","description":"This document details the embedding and retrieval architecture for stateful prompt learning.","source":"@site/docs/skills/automatic_stateful_prompt_improver/references/embedding-architecture.md","sourceDirName":"skills/automatic_stateful_prompt_improver/references","slug":"/skills/automatic_stateful_prompt_improver/references/embedding-architecture","permalink":"/docs/skills/automatic_stateful_prompt_improver/references/embedding-architecture","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Embedding Architecture for Prompt Learning","sidebar_label":"Embedding Architecture for ...","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"DSPy Optimization Patterns ...","permalink":"/docs/skills/automatic_stateful_prompt_improver/references/dspy-patterns"},"next":{"title":"Iteration Strategy","permalink":"/docs/skills/automatic_stateful_prompt_improver/references/iteration-strategy"}}');var r=t(74848),s=t(28453);const d={title:"Embedding Architecture for Prompt Learning",sidebar_label:"Embedding Architecture for ...",sidebar_position:3},a="Embedding Architecture for Prompt Learning",c={},l=[{value:"Design Principles",id:"design-principles",level:2},{value:"Embedding Strategy",id:"embedding-strategy",level:2},{value:"Contextual Embedding Pipeline",id:"contextual-embedding-pipeline",level:3},{value:"Embedding Model Selection",id:"embedding-model-selection",level:3},{value:"Retrieval Architecture",id:"retrieval-architecture",level:2},{value:"Hybrid Search Pipeline",id:"hybrid-search-pipeline",level:3},{value:"Reranking (Optional but Recommended)",id:"reranking-optional-but-recommended",level:3},{value:"Performance Tracking",id:"performance-tracking",level:2},{value:"Metric Structure",id:"metric-structure",level:3},{value:"Temporal Weighting",id:"temporal-weighting",level:3},{value:"Drift Detection",id:"drift-detection",level:2},{value:"Distribution Shift Detection",id:"distribution-shift-detection",level:3},{value:"Vector Database Selection",id:"vector-database-selection",level:2},{value:"Comparison Matrix",id:"comparison-matrix",level:3},{value:"Qdrant Setup (Recommended)",id:"qdrant-setup-recommended",level:3},{value:"Chroma Setup (Quick Start)",id:"chroma-setup-quick-start",level:3},{value:"Caching Strategy",id:"caching-strategy",level:2},{value:"Embedding Cache",id:"embedding-cache",level:3},{value:"Query Result Cache",id:"query-result-cache",level:3},{value:"Performance Targets",id:"performance-targets",level:2}];function o(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"embedding-architecture-for-prompt-learning",children:"Embedding Architecture for Prompt Learning"})}),"\n",(0,r.jsx)(n.p,{children:"This document details the embedding and retrieval architecture for stateful prompt learning."}),"\n",(0,r.jsx)(n.h2,{id:"design-principles",children:"Design Principles"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Contextual Embeddings"}),": Add domain context before embedding (49% error reduction)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Hybrid Search"}),": Vector + BM25 for best recall"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Recency Weighting"}),": Balance historical stability with recent relevance"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Drift Detection"}),": Detect and adapt to distribution shifts"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"embedding-strategy",children:"Embedding Strategy"}),"\n",(0,r.jsx)(n.h3,{id:"contextual-embedding-pipeline",children:"Contextual Embedding Pipeline"}),"\n",(0,r.jsx)(n.p,{children:"Following Anthropic's contextual retrieval methodology:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class ContextualPromptEmbedder:\n    """\n    Create embeddings that include domain context for better retrieval.\n\n    Standard embedding: embed(prompt_text)\n    Contextual embedding: embed(context + prompt_text)\n\n    This reduces retrieval errors by 49% (with BM25) to 67% (with reranking).\n    """\n\n    def __init__(self, embedding_model: str = "text-embedding-3-large"):\n        self.model = embedding_model\n        self.context_cache = {}  # Cache context generation\n\n    async def create_embedding(\n        self,\n        prompt: str,\n        domain: str,\n        task_type: str\n    ) -> dict:\n        """\n        Create contextual embedding for a prompt.\n\n        Args:\n            prompt: The prompt text to embed\n            domain: Domain classification (e.g., \'code_review\', \'summarization\')\n            task_type: Task type (e.g., \'generation\', \'classification\')\n\n        Returns:\n            {\n                \'embedding\': [...],\n                \'contextualized_text\': str,\n                \'metadata\': dict\n            }\n        """\n        # Generate contextualizing information\n        context = await self._generate_context(prompt, domain, task_type)\n\n        # Combine context with prompt\n        contextualized = f"{context}\\n\\n{prompt}"\n\n        # Generate embedding\n        embedding = await self._embed(contextualized)\n\n        return {\n            \'embedding\': embedding,\n            \'contextualized_text\': contextualized,\n            \'metadata\': {\n                \'domain\': domain,\n                \'task_type\': task_type,\n                \'original_length\': len(prompt),\n                \'context_length\': len(context)\n            }\n        }\n\n    async def _generate_context(\n        self,\n        prompt: str,\n        domain: str,\n        task_type: str\n    ) -> str:\n        """\n        Generate 50-100 token context for a prompt.\n\n        Uses LLM to create contextualizing information that improves retrieval.\n        Results are cached to reduce API calls.\n        """\n        cache_key = f"{domain}:{task_type}:{hash(prompt[:100])}"\n\n        if cache_key in self.context_cache:\n            return self.context_cache[cache_key]\n\n        context_prompt = f"""\n        Generate a brief context (50-100 tokens) for this prompt to improve retrieval.\n\n        Domain: {domain}\n        Task Type: {task_type}\n        Prompt: {prompt[:500]}\n\n        Include:\n        - Key domain terminology\n        - Task intent\n        - Important entities/concepts\n\n        Context only, no explanation:\n        """\n\n        context = await self._call_llm(context_prompt)\n        self.context_cache[cache_key] = context\n        return context\n'})}),"\n",(0,r.jsx)(n.h3,{id:"embedding-model-selection",children:"Embedding Model Selection"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{children:"Dimensions"}),(0,r.jsx)(n.th,{children:"Strengths"}),(0,r.jsx)(n.th,{children:"Cost"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"text-embedding-3-large"})}),(0,r.jsx)(n.td,{children:"3072"}),(0,r.jsx)(n.td,{children:"Best quality, multi-lingual"}),(0,r.jsx)(n.td,{children:"$$$"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"text-embedding-3-small"})}),(0,r.jsx)(n.td,{children:"1536"}),(0,r.jsx)(n.td,{children:"Good balance"}),(0,r.jsx)(n.td,{children:"$$"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"text-embedding-ada-002"})}),(0,r.jsx)(n.td,{children:"1536"}),(0,r.jsx)(n.td,{children:"Legacy, good compatibility"}),(0,r.jsx)(n.td,{children:"$"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsxs)(n.td,{children:["Cohere ",(0,r.jsx)(n.code,{children:"embed-v3"})]}),(0,r.jsx)(n.td,{children:"1024"}),(0,r.jsx)(n.td,{children:"Good for retrieval"}),(0,r.jsx)(n.td,{children:"$$"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Recommendation"}),": Start with ",(0,r.jsx)(n.code,{children:"text-embedding-3-small"}),", upgrade to ",(0,r.jsx)(n.code,{children:"large"})," for production."]}),"\n",(0,r.jsx)(n.h2,{id:"retrieval-architecture",children:"Retrieval Architecture"}),"\n",(0,r.jsx)(n.h3,{id:"hybrid-search-pipeline",children:"Hybrid Search Pipeline"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class HybridPromptRetrieval:\n    """\n    Combine vector similarity with BM25 for optimal retrieval.\n\n    Research shows hybrid search outperforms either alone:\n    - Vector only: Good semantic matching\n    - BM25 only: Good keyword matching\n    - Hybrid: Best of both (5-10% improvement)\n    """\n\n    def __init__(\n        self,\n        vector_db: VectorDatabase,\n        bm25_index: BM25Index,\n        reranker: Optional[Reranker] = None\n    ):\n        self.vector_db = vector_db\n        self.bm25 = bm25_index\n        self.reranker = reranker\n\n    async def search(\n        self,\n        query: str,\n        query_embedding: list,\n        top_k: int = 10,\n        filters: dict = None\n    ) -> list:\n        """\n        Hybrid retrieval with optional reranking.\n\n        Pipeline:\n        1. Vector search (retrieve 3x candidates)\n        2. BM25 search (retrieve 3x candidates)\n        3. Reciprocal Rank Fusion (combine results)\n        4. Optional reranking (refine top candidates)\n        """\n        # Over-retrieve for fusion\n        fetch_k = top_k * 3\n\n        # Parallel retrieval\n        vector_results, bm25_results = await asyncio.gather(\n            self._vector_search(query_embedding, fetch_k, filters),\n            self._bm25_search(query, fetch_k, filters)\n        )\n\n        # Reciprocal Rank Fusion\n        fused = self._rrf_fusion(vector_results, bm25_results)\n\n        # Optional reranking\n        if self.reranker and len(fused) > 0:\n            reranked = await self.reranker.rerank(\n                query=query,\n                documents=fused[:top_k * 2],\n                top_k=top_k\n            )\n            return reranked\n\n        return fused[:top_k]\n\n    def _rrf_fusion(\n        self,\n        vector_results: list,\n        bm25_results: list,\n        k: int = 60\n    ) -> list:\n        """\n        Reciprocal Rank Fusion: 1/(k + rank) scoring.\n\n        Combines rankings from multiple retrieval methods.\n        k=60 is standard from RRF paper.\n        """\n        scores = {}\n        doc_map = {}\n\n        for rank, doc in enumerate(vector_results):\n            doc_id = doc[\'id\']\n            scores[doc_id] = scores.get(doc_id, 0) + 1 / (k + rank + 1)\n            doc_map[doc_id] = doc\n\n        for rank, doc in enumerate(bm25_results):\n            doc_id = doc[\'id\']\n            scores[doc_id] = scores.get(doc_id, 0) + 1 / (k + rank + 1)\n            doc_map[doc_id] = doc\n\n        # Sort by fused score\n        sorted_ids = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)\n\n        return [\n            {**doc_map[doc_id], \'rrf_score\': scores[doc_id]}\n            for doc_id in sorted_ids\n            if doc_id in doc_map\n        ]\n'})}),"\n",(0,r.jsx)(n.h3,{id:"reranking-optional-but-recommended",children:"Reranking (Optional but Recommended)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class CohereReranker:\n    """\n    Use Cohere\'s reranker for final result refinement.\n\n    Adds ~100ms latency but improves relevance significantly.\n    """\n\n    def __init__(self, api_key: str):\n        self.client = cohere.Client(api_key)\n\n    async def rerank(\n        self,\n        query: str,\n        documents: list,\n        top_k: int = 5\n    ) -> list:\n        """\n        Rerank documents using Cohere\'s reranker model.\n        """\n        response = self.client.rerank(\n            model="rerank-english-v2.0",\n            query=query,\n            documents=[d[\'prompt_text\'] for d in documents],\n            top_n=top_k\n        )\n\n        return [\n            {\n                **documents[r.index],\n                \'rerank_score\': r.relevance_score\n            }\n            for r in response.results\n        ]\n'})}),"\n",(0,r.jsx)(n.h2,{id:"performance-tracking",children:"Performance Tracking"}),"\n",(0,r.jsx)(n.h3,{id:"metric-structure",children:"Metric Structure"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'@dataclass\nclass PromptMetrics:\n    """\n    Performance metrics for a prompt.\n\n    Updated using exponential moving average for recency weighting.\n    """\n    success_rate: float       # 0-1, task completion rate\n    avg_latency_ms: float     # Response time\n    token_efficiency: float   # quality_score / tokens_used\n    coherence_score: float    # Logical consistency\n    observation_count: int    # How many times evaluated\n    last_updated: datetime    # For recency calculations\n\n    def update(self, outcome: dict, alpha: float = 0.3):\n        """\n        Exponential moving average update.\n\n        alpha = 0.3 means:\n        - 30% weight to new observation\n        - 70% weight to historical average\n        """\n        self.success_rate = alpha * float(outcome[\'success\']) + (1 - alpha) * self.success_rate\n        self.avg_latency_ms = alpha * outcome.get(\'latency_ms\', self.avg_latency_ms) + (1 - alpha) * self.avg_latency_ms\n        self.observation_count += 1\n        self.last_updated = datetime.utcnow()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"temporal-weighting",children:"Temporal Weighting"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class TemporalWeighting:\n    """\n    Apply time-decay to balance recency vs. historical performance.\n\n    half_life: Time for weight to decay to 50%\n    - 30 days: Standard for most use cases\n    - 7 days: Fast-changing domains\n    - 90 days: Stable domains\n    """\n\n    def __init__(self, half_life_days: int = 30):\n        self.half_life = timedelta(days=half_life_days)\n\n    def calculate_weight(self, observation_time: datetime) -> float:\n        """\n        Calculate time-decay weight.\n\n        weight = 0.5^(time_diff / half_life)\n        """\n        time_diff = datetime.utcnow() - observation_time\n        half_lives_elapsed = time_diff / self.half_life\n        return math.pow(0.5, half_lives_elapsed)\n\n    def weighted_score(\n        self,\n        observations: list[tuple[float, datetime]]\n    ) -> float:\n        """\n        Calculate weighted average with time decay.\n        """\n        total_weight = 0\n        weighted_sum = 0\n\n        for value, timestamp in observations:\n            weight = self.calculate_weight(timestamp)\n            weighted_sum += value * weight\n            total_weight += weight\n\n        return weighted_sum / total_weight if total_weight > 0 else 0\n'})}),"\n",(0,r.jsx)(n.h2,{id:"drift-detection",children:"Drift Detection"}),"\n",(0,r.jsx)(n.h3,{id:"distribution-shift-detection",children:"Distribution Shift Detection"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class DriftDetector:\n    """\n    Detect when prompt distribution shifts (new domains, patterns).\n\n    Triggers adaptive learning rate adjustment.\n    """\n\n    def __init__(\n        self,\n        window_size: int = 100,\n        drift_threshold: float = 0.15\n    ):\n        self.window_size = window_size\n        self.threshold = drift_threshold\n        self.embedding_history = deque(maxlen=window_size)\n\n    def check_drift(self, new_embedding: np.ndarray) -> bool:\n        """\n        Check if new embedding indicates distribution shift.\n\n        Uses Maximum Mean Discrepancy (simplified).\n        """\n        if len(self.embedding_history) < 50:\n            self.embedding_history.append(new_embedding)\n            return False\n\n        # Calculate centroid of historical embeddings\n        historical = np.array(list(self.embedding_history))\n        centroid = np.mean(historical, axis=0)\n\n        # Measure distance from centroid\n        distance = np.linalg.norm(new_embedding - centroid)\n\n        # Normalize by average distance in history\n        avg_distance = np.mean([\n            np.linalg.norm(e - centroid)\n            for e in historical\n        ])\n\n        normalized_distance = distance / avg_distance if avg_distance > 0 else 0\n\n        self.embedding_history.append(new_embedding)\n\n        return normalized_distance > (1 + self.threshold)\n\n    def get_drift_metrics(self) -> dict:\n        """\n        Get drift detection metrics for monitoring.\n        """\n        if len(self.embedding_history) < 10:\n            return {\'status\': \'insufficient_data\'}\n\n        historical = np.array(list(self.embedding_history))\n        centroid = np.mean(historical, axis=0)\n\n        distances = [np.linalg.norm(e - centroid) for e in historical]\n\n        return {\n            \'avg_distance\': np.mean(distances),\n            \'std_distance\': np.std(distances),\n            \'max_distance\': np.max(distances),\n            \'sample_size\': len(self.embedding_history)\n        }\n'})}),"\n",(0,r.jsx)(n.h2,{id:"vector-database-selection",children:"Vector Database Selection"}),"\n",(0,r.jsx)(n.h3,{id:"comparison-matrix",children:"Comparison Matrix"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Feature"}),(0,r.jsx)(n.th,{children:"Qdrant"}),(0,r.jsx)(n.th,{children:"Pinecone"}),(0,r.jsx)(n.th,{children:"Chroma"}),(0,r.jsx)(n.th,{children:"pgvector"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Self-hosted"})}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"Yes"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Managed"})}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"Depends"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Latency (p50)"})}),(0,r.jsx)(n.td,{children:"~30ms"}),(0,r.jsx)(n.td,{children:"~25ms"}),(0,r.jsx)(n.td,{children:"~50ms"}),(0,r.jsx)(n.td,{children:"~31ms"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Scale"})}),(0,r.jsx)(n.td,{children:"Billions"}),(0,r.jsx)(n.td,{children:"Billions"}),(0,r.jsx)(n.td,{children:"Millions"}),(0,r.jsx)(n.td,{children:"Millions"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Filtering"})}),(0,r.jsx)(n.td,{children:"Rich"}),(0,r.jsx)(n.td,{children:"Rich"}),(0,r.jsx)(n.td,{children:"Basic"}),(0,r.jsx)(n.td,{children:"SQL"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"BM25 Support"})}),(0,r.jsx)(n.td,{children:"Yes"}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"No"}),(0,r.jsx)(n.td,{children:"No (use pg_trgm)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Cost"})}),(0,r.jsx)(n.td,{children:"$"}),(0,r.jsx)(n.td,{children:"$$$"}),(0,r.jsx)(n.td,{children:"Free"}),(0,r.jsx)(n.td,{children:"$"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"qdrant-setup-recommended",children:"Qdrant Setup (Recommended)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams, PayloadSchemaType\n\nasync def setup_qdrant():\n    client = QdrantClient(url="http://localhost:6333")\n\n    # Create collection\n    await client.create_collection(\n        collection_name="prompt_embeddings",\n        vectors_config=VectorParams(\n            size=3072,  # text-embedding-3-large\n            distance=Distance.COSINE\n        )\n    )\n\n    # Create indexes for filtering\n    await client.create_payload_index(\n        collection_name="prompt_embeddings",\n        field_name="metrics.success_rate",\n        field_schema=PayloadSchemaType.FLOAT\n    )\n\n    await client.create_payload_index(\n        collection_name="prompt_embeddings",\n        field_name="domain",\n        field_schema=PayloadSchemaType.KEYWORD\n    )\n\n    await client.create_payload_index(\n        collection_name="prompt_embeddings",\n        field_name="created_at",\n        field_schema=PayloadSchemaType.DATETIME\n    )\n\n    return client\n'})}),"\n",(0,r.jsx)(n.h3,{id:"chroma-setup-quick-start",children:"Chroma Setup (Quick Start)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import chromadb\nfrom chromadb.config import Settings\n\ndef setup_chroma():\n    # Persistent storage\n    client = chromadb.PersistentClient(\n        path="./chroma_data",\n        settings=Settings(anonymized_telemetry=False)\n    )\n\n    collection = client.get_or_create_collection(\n        name="prompt_embeddings",\n        metadata={"hnsw:space": "cosine"}\n    )\n\n    return collection\n'})}),"\n",(0,r.jsx)(n.h2,{id:"caching-strategy",children:"Caching Strategy"}),"\n",(0,r.jsx)(n.h3,{id:"embedding-cache",children:"Embedding Cache"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class EmbeddingCache:\n    """\n    Cache embeddings to reduce API calls.\n\n    Cache hit rate typically 60-80% for repeated prompts.\n    """\n\n    def __init__(self, redis_client: Redis, ttl_hours: int = 24):\n        self.redis = redis_client\n        self.ttl = ttl_hours * 3600\n\n    async def get_or_create(\n        self,\n        text: str,\n        embed_fn: Callable\n    ) -> list:\n        """\n        Get embedding from cache or create new.\n        """\n        cache_key = f"emb:{hashlib.sha256(text.encode()).hexdigest()}"\n\n        # Try cache\n        cached = await self.redis.get(cache_key)\n        if cached:\n            return json.loads(cached)\n\n        # Generate new\n        embedding = await embed_fn(text)\n\n        # Cache with TTL\n        await self.redis.setex(\n            cache_key,\n            self.ttl,\n            json.dumps(embedding)\n        )\n\n        return embedding\n'})}),"\n",(0,r.jsx)(n.h3,{id:"query-result-cache",children:"Query Result Cache"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class QueryCache:\n    """\n    Cache frequent queries for faster retrieval.\n\n    Short TTL (5-15 min) since results may change.\n    """\n\n    def __init__(self, redis_client: Redis, ttl_minutes: int = 10):\n        self.redis = redis_client\n        self.ttl = ttl_minutes * 60\n\n    async def get_or_query(\n        self,\n        query: str,\n        filters: dict,\n        query_fn: Callable\n    ) -> list:\n        """\n        Get results from cache or execute query.\n        """\n        cache_key = f"query:{hashlib.sha256(f\'{query}:{json.dumps(filters)}\'.encode()).hexdigest()}"\n\n        cached = await self.redis.get(cache_key)\n        if cached:\n            return json.loads(cached)\n\n        results = await query_fn(query, filters)\n\n        await self.redis.setex(\n            cache_key,\n            self.ttl,\n            json.dumps(results)\n        )\n\n        return results\n'})}),"\n",(0,r.jsx)(n.h2,{id:"performance-targets",children:"Performance Targets"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Operation"}),(0,r.jsx)(n.th,{children:"Target Latency"}),(0,r.jsx)(n.th,{children:"Notes"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Embedding generation"}),(0,r.jsx)(n.td,{children:"<100ms (p99)"}),(0,r.jsx)(n.td,{children:"Use caching"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Vector search"}),(0,r.jsx)(n.td,{children:"<50ms (p99)"}),(0,r.jsx)(n.td,{children:"HNSW index"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Hybrid search"}),(0,r.jsx)(n.td,{children:"<100ms (p99)"}),(0,r.jsx)(n.td,{children:"Parallel retrieval"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"With reranking"}),(0,r.jsx)(n.td,{children:"<200ms (p99)"}),(0,r.jsx)(n.td,{children:"Only for top-k"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Metric update"}),(0,r.jsx)(n.td,{children:"<10ms (p99)"}),(0,r.jsx)(n.td,{children:"Async"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Full pipeline"}),(0,r.jsx)(n.td,{children:"<300ms (p99)"}),(0,r.jsx)(n.td,{children:"Cold"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Full pipeline"}),(0,r.jsx)(n.td,{children:"<100ms (p99)"}),(0,r.jsx)(n.td,{children:"Cached"})]})]})]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.p,{children:"This architecture provides a scalable foundation for stateful prompt learning that can grow from prototype to production."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(o,{...e})}):o(e)}}}]);