"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[61660],{14494:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>i,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"skills/llm_streaming_response_handler/index","title":"LLM Streaming Response Handler","description":"Build production LLM streaming UIs with Server-Sent Events, real-time token display, cancellation, error recovery. Handles OpenAI/Anthropic/Claude streaming APIs. Use for chatbots, AI assistants, real-time text generation. Activate on \\"LLM streaming\\", \\"SSE\\", \\"token stream\\", \\"chat UI\\", \\"real-time AI\\". NOT for batch processing, non-streaming APIs, or WebSocket bidirectional chat.","source":"@site/docs/skills/llm_streaming_response_handler/index.md","sourceDirName":"skills/llm_streaming_response_handler","slug":"/skills/llm_streaming_response_handler","permalink":"/docs/skills/llm_streaming_response_handler","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"name":"llm-streaming-response-handler","description":"Build production LLM streaming UIs with Server-Sent Events, real-time token display, cancellation, error recovery. Handles OpenAI/Anthropic/Claude streaming APIs. Use for chatbots, AI assistants, real-time text generation. Activate on \\"LLM streaming\\", \\"SSE\\", \\"token stream\\", \\"chat UI\\", \\"real-time AI\\". NOT for batch processing, non-streaming APIs, or WebSocket bidirectional chat.","allowed-tools":"Read,Write,Edit,Bash(npm:*)","slug":"/skills/llm_streaming_response_handler"}}');var s=t(74848),o=t(28453);const i={name:"llm-streaming-response-handler",description:'Build production LLM streaming UIs with Server-Sent Events, real-time token display, cancellation, error recovery. Handles OpenAI/Anthropic/Claude streaming APIs. Use for chatbots, AI assistants, real-time text generation. Activate on "LLM streaming", "SSE", "token stream", "chat UI", "real-time AI". NOT for batch processing, non-streaming APIs, or WebSocket bidirectional chat.',"allowed-tools":"Read,Write,Edit,Bash(npm:*)",slug:"/skills/llm_streaming_response_handler"},l="LLM Streaming Response Handler",a={},c=[{value:"When to Use",id:"when-to-use",level:2},{value:"Quick Decision Tree",id:"quick-decision-tree",level:2},{value:"Technology Selection",id:"technology-selection",level:2},{value:"Server-Sent Events (SSE) - Recommended",id:"server-sent-events-sse---recommended",level:3},{value:"Streaming APIs",id:"streaming-apis",level:3},{value:"Common Anti-Patterns",id:"common-anti-patterns",level:2},{value:"Anti-Pattern 1: Buffering Before Display",id:"anti-pattern-1-buffering-before-display",level:3},{value:"Anti-Pattern 2: No Stream Cancellation",id:"anti-pattern-2-no-stream-cancellation",level:3},{value:"Anti-Pattern 3: No Error Recovery",id:"anti-pattern-3-no-error-recovery",level:3},{value:"Anti-Pattern 4: Memory Leaks from Unclosed Streams",id:"anti-pattern-4-memory-leaks-from-unclosed-streams",level:3},{value:"Anti-Pattern 5: No Typing Indicator Between Tokens",id:"anti-pattern-5-no-typing-indicator-between-tokens",level:3},{value:"Implementation Patterns",id:"implementation-patterns",level:2},{value:"Pattern 1: Basic SSE Stream Handler",id:"pattern-1-basic-sse-stream-handler",level:3},{value:"Pattern 2: React Hook for Streaming",id:"pattern-2-react-hook-for-streaming",level:3},{value:"Pattern 3: Server-Side Streaming (Next.js)",id:"pattern-3-server-side-streaming-nextjs",level:3},{value:"Production Checklist",id:"production-checklist",level:2},{value:"When to Use vs Avoid",id:"when-to-use-vs-avoid",level:2},{value:"Technology Comparison",id:"technology-comparison",level:2},{value:"References",id:"references",level:2},{value:"Scripts",id:"scripts",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"llm-streaming-response-handler",children:"LLM Streaming Response Handler"})}),"\n",(0,s.jsx)(n.p,{children:"Expert in building production-grade streaming interfaces for LLM responses that feel instant and responsive."}),"\n",(0,s.jsx)(n.h2,{id:"when-to-use",children:"When to Use"}),"\n",(0,s.jsxs)(n.p,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"Use for"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Chat interfaces with typing animation"}),"\n",(0,s.jsx)(n.li,{children:"Real-time AI assistants"}),"\n",(0,s.jsx)(n.li,{children:"Code generation with live preview"}),"\n",(0,s.jsx)(n.li,{children:"Document summarization with progressive display"}),"\n",(0,s.jsx)(n.li,{children:"Any UI where users expect immediate feedback from LLMs"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["\u274c ",(0,s.jsx)(n.strong,{children:"NOT for"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Batch document processing (no user watching)"}),"\n",(0,s.jsx)(n.li,{children:"APIs that don't support streaming"}),"\n",(0,s.jsx)(n.li,{children:"WebSocket-based bidirectional chat (use Socket.IO)"}),"\n",(0,s.jsx)(n.li,{children:"Simple request/response (fetch is fine)"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"quick-decision-tree",children:"Quick Decision Tree"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Does your LLM interaction:\n\u251c\u2500\u2500 Need immediate visual feedback? \u2192 Streaming\n\u251c\u2500\u2500 Display long-form content (&gt;100 words)? \u2192 Streaming\n\u251c\u2500\u2500 User expects typewriter effect? \u2192 Streaming\n\u251c\u2500\u2500 Short response (&lt;50 words)? \u2192 Regular fetch\n\u2514\u2500\u2500 Background processing? \u2192 Regular fetch\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"technology-selection",children:"Technology Selection"}),"\n",(0,s.jsx)(n.h3,{id:"server-sent-events-sse---recommended",children:"Server-Sent Events (SSE) - Recommended"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Why SSE over WebSockets for LLM streaming"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Simplicity"}),": HTTP-based, works with existing infrastructure"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Auto-reconnect"}),": Built-in reconnection logic"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Firewall-friendly"}),": Easier than WebSockets through proxies"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"One-way perfect"}),": LLMs only stream server \u2192 client"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Timeline"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"2015-2020: WebSockets for everything"}),"\n",(0,s.jsx)(n.li,{children:"2020: SSE adoption for streaming APIs"}),"\n",(0,s.jsx)(n.li,{children:"2023+: SSE standard for LLM streaming (OpenAI, Anthropic)"}),"\n",(0,s.jsx)(n.li,{children:"2024: Vercel AI SDK popularizes SSE patterns"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"streaming-apis",children:"Streaming APIs"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Provider"}),(0,s.jsx)(n.th,{children:"Streaming Method"}),(0,s.jsx)(n.th,{children:"Response Format"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"OpenAI"}),(0,s.jsx)(n.td,{children:"SSE"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:'data: {"choices":[{"delta":{"content":"token"}}]}'})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Anthropic"}),(0,s.jsx)(n.td,{children:"SSE"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:'data: {"type":"content_block_delta","delta":{"text":"token"}}'})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Claude (API)"}),(0,s.jsx)(n.td,{children:"SSE"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:'data: {"delta":{"text":"token"}}'})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Vercel AI SDK"}),(0,s.jsx)(n.td,{children:"SSE"}),(0,s.jsx)(n.td,{children:"Normalized across providers"})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"common-anti-patterns",children:"Common Anti-Patterns"}),"\n",(0,s.jsx)(n.h3,{id:"anti-pattern-1-buffering-before-display",children:"Anti-Pattern 1: Buffering Before Display"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Novice thinking"}),': "Collect all tokens, then show complete response"']}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),": Defeats the entire purpose of streaming."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Wrong approach"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// \u274c Waits for entire response before showing anything\nconst response = await fetch('/api/chat', { method: 'POST', body: prompt });\nconst fullText = await response.text();\nsetMessage(fullText); // User sees nothing until done\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Correct approach"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// \u2705 Display tokens as they arrive\nconst response = await fetch('/api/chat', {\n  method: 'POST',\n  body: JSON.stringify({ prompt })\n});\n\nconst reader = response.body.getReader();\nconst decoder = new TextDecoder();\n\nwhile (true) {\n  const { done, value } = await reader.read();\n  if (done) break;\n\n  const chunk = decoder.decode(value);\n  const lines = chunk.split('\\n').filter(line => line.trim());\n\n  for (const line of lines) {\n    if (line.startsWith('data: ')) {\n      const data = JSON.parse(line.slice(6));\n      setMessage(prev => prev + data.content); // Update immediately\n    }\n  }\n}\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Timeline"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Pre-2023: Many apps buffered entire response"}),"\n",(0,s.jsx)(n.li,{children:"2023+: Token-by-token display expected"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"anti-pattern-2-no-stream-cancellation",children:"Anti-Pattern 2: No Stream Cancellation"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),": User can't stop generation, wasting tokens and money."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Symptom"}),": \"Stop\" button doesn't work or doesn't exist."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Correct approach"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// \u2705 AbortController for cancellation\nconst [abortController, setAbortController] = useState<AbortController | null>(null);\n\nconst streamResponse = async () => {\n  const controller = new AbortController();\n  setAbortController(controller);\n\n  try {\n    const response = await fetch('/api/chat', {\n      signal: controller.signal,\n      method: 'POST',\n      body: JSON.stringify({ prompt })\n    });\n\n    // Stream handling...\n  } catch (error) {\n    if (error.name === 'AbortError') {\n      console.log('Stream cancelled by user');\n    }\n  } finally {\n    setAbortController(null);\n  }\n};\n\nconst cancelStream = () => {\n  abortController?.abort();\n};\n\nreturn (\n  <button onClick={cancelStream} disabled={!abortController}>\n    Stop Generating\n  </button>\n);\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"anti-pattern-3-no-error-recovery",children:"Anti-Pattern 3: No Error Recovery"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),": Stream fails mid-response, user sees partial text with no indication of failure."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Correct approach"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// \u2705 Error states and recovery\nconst [streamState, setStreamState] = useState<'idle' | 'streaming' | 'error' | 'complete'>('idle');\nconst [errorMessage, setErrorMessage] = useState<string | null>(null);\n\ntry {\n  setStreamState('streaming');\n\n  // Streaming logic...\n\n  setStreamState('complete');\n} catch (error) {\n  setStreamState('error');\n\n  if (error.name === 'AbortError') {\n    setErrorMessage('Generation stopped');\n  } else if (error.message.includes('429')) {\n    setErrorMessage('Rate limit exceeded. Try again in a moment.');\n  } else {\n    setErrorMessage('Something went wrong. Please retry.');\n  }\n}\n\n// UI feedback\n{streamState === 'error' && (\n  <div className=\"error-banner\">\n    {errorMessage}\n    <button onClick={retryStream}>Retry</button>\n  </div>\n)}\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"anti-pattern-4-memory-leaks-from-unclosed-streams",children:"Anti-Pattern 4: Memory Leaks from Unclosed Streams"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),": Streams not cleaned up, causing memory leaks."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Symptom"}),": Browser slows down after multiple requests."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Correct approach"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// \u2705 Cleanup with useEffect\nuseEffect(() => {\n  let reader: ReadableStreamDefaultReader | null = null;\n\n  const streamResponse = async () => {\n    const response = await fetch('/api/chat', { ... });\n    reader = response.body.getReader();\n\n    // Streaming...\n  };\n\n  streamResponse();\n\n  // Cleanup on unmount\n  return () => {\n    reader?.cancel();\n  };\n}, [prompt]);\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"anti-pattern-5-no-typing-indicator-between-tokens",children:"Anti-Pattern 5: No Typing Indicator Between Tokens"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),": UI feels frozen between slow tokens."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Correct approach"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:'// \u2705 Animated cursor during generation\n<div className="message">\n  {content}\n  {isStreaming && <span className="typing-cursor">\u258a</span>}\n</div>\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-css",children:".typing-cursor {\n  animation: blink 1s step-end infinite;\n}\n\n@keyframes blink {\n  50% { opacity: 0; }\n}\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"implementation-patterns",children:"Implementation Patterns"}),"\n",(0,s.jsx)(n.h3,{id:"pattern-1-basic-sse-stream-handler",children:"Pattern 1: Basic SSE Stream Handler"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"async function* streamCompletion(prompt: string) {\n  const response = await fetch('/api/chat', {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({ prompt })\n  });\n\n  const reader = response.body!.getReader();\n  const decoder = new TextDecoder();\n\n  while (true) {\n    const { done, value } = await reader.read();\n    if (done) break;\n\n    const chunk = decoder.decode(value);\n    const lines = chunk.split('\\n');\n\n    for (const line of lines) {\n      if (line.startsWith('data: ')) {\n        const data = JSON.parse(line.slice(6));\n\n        if (data.content) {\n          yield data.content;\n        }\n\n        if (data.done) {\n          return;\n        }\n      }\n    }\n  }\n}\n\n// Usage\nfor await (const token of streamCompletion('Hello')) {\n  console.log(token);\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"pattern-2-react-hook-for-streaming",children:"Pattern 2: React Hook for Streaming"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"import { useState, useCallback } from 'react';\n\ninterface UseStreamingOptions {\n  onToken?: (token: string) => void;\n  onComplete?: (fullText: string) => void;\n  onError?: (error: Error) => void;\n}\n\nexport function useStreaming(options: UseStreamingOptions = {}) {\n  const [content, setContent] = useState('');\n  const [isStreaming, setIsStreaming] = useState(false);\n  const [error, setError] = useState<Error | null>(null);\n  const [abortController, setAbortController] = useState<AbortController | null>(null);\n\n  const stream = useCallback(async (prompt: string) => {\n    const controller = new AbortController();\n    setAbortController(controller);\n    setIsStreaming(true);\n    setError(null);\n    setContent('');\n\n    try {\n      const response = await fetch('/api/chat', {\n        method: 'POST',\n        signal: controller.signal,\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({ prompt })\n      });\n\n      const reader = response.body!.getReader();\n      const decoder = new TextDecoder();\n\n      let accumulated = '';\n\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n\n        const chunk = decoder.decode(value);\n        const lines = chunk.split('\\n').filter(line => line.trim());\n\n        for (const line of lines) {\n          if (line.startsWith('data: ')) {\n            const data = JSON.parse(line.slice(6));\n\n            if (data.content) {\n              accumulated += data.content;\n              setContent(accumulated);\n              options.onToken?.(data.content);\n            }\n          }\n        }\n      }\n\n      options.onComplete?.(accumulated);\n    } catch (err) {\n      if (err.name !== 'AbortError') {\n        setError(err as Error);\n        options.onError?.(err as Error);\n      }\n    } finally {\n      setIsStreaming(false);\n      setAbortController(null);\n    }\n  }, [options]);\n\n  const cancel = useCallback(() => {\n    abortController?.abort();\n  }, [abortController]);\n\n  return { content, isStreaming, error, stream, cancel };\n}\n\n// Usage in component\nfunction ChatInterface() {\n  const { content, isStreaming, stream, cancel } = useStreaming({\n    onToken: (token) => console.log('New token:', token),\n    onComplete: (text) => console.log('Done:', text)\n  });\n\n  return (\n    <div>\n      <div className=\"message\">\n        {content}\n        {isStreaming && <span className=\"cursor\">\u258a</span>}\n      </div>\n\n      <button onClick={() => stream('Tell me a story')} disabled={isStreaming}>\n        Generate\n      </button>\n\n      {isStreaming && <button onClick={cancel}>Stop</button>}\n    </div>\n  );\n}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"pattern-3-server-side-streaming-nextjs",children:"Pattern 3: Server-Side Streaming (Next.js)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// app/api/chat/route.ts\nimport { OpenAI } from 'openai';\n\nexport const runtime = 'edge'; // Required for streaming\n\nexport async function POST(req: Request) {\n  const { prompt } = await req.json();\n\n  const openai = new OpenAI({\n    apiKey: process.env.OPENAI_API_KEY\n  });\n\n  const stream = await openai.chat.completions.create({\n    model: 'gpt-4',\n    messages: [{ role: 'user', content: prompt }],\n    stream: true\n  });\n\n  // Convert OpenAI stream to SSE format\n  const encoder = new TextEncoder();\n\n  const readable = new ReadableStream({\n    async start(controller) {\n      try {\n        for await (const chunk of stream) {\n          const content = chunk.choices[0]?.delta?.content;\n\n          if (content) {\n            const sseMessage = `data: ${JSON.stringify({ content })}\\n\\n`;\n            controller.enqueue(encoder.encode(sseMessage));\n          }\n        }\n\n        // Send completion signal\n        controller.enqueue(encoder.encode('data: {\"done\":true}\\n\\n'));\n        controller.close();\n      } catch (error) {\n        controller.error(error);\n      }\n    }\n  });\n\n  return new Response(readable, {\n    headers: {\n      'Content-Type': 'text/event-stream',\n      'Cache-Control': 'no-cache',\n      'Connection': 'keep-alive'\n    }\n  });\n}\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"production-checklist",children:"Production Checklist"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u25a1 AbortController for cancellation\n\u25a1 Error states with retry capability\n\u25a1 Typing indicator during generation\n\u25a1 Cleanup on component unmount\n\u25a1 Rate limiting on API route\n\u25a1 Token usage tracking\n\u25a1 Streaming fallback (if API fails)\n\u25a1 Accessibility (screen reader announces updates)\n\u25a1 Mobile-friendly (touch targets for stop button)\n\u25a1 Network error recovery (auto-retry on disconnect)\n\u25a1 Max response length enforcement\n\u25a1 Cost estimation before generation\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"when-to-use-vs-avoid",children:"When to Use vs Avoid"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Scenario"}),(0,s.jsx)(n.th,{children:"Use Streaming?"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Chat interface"}),(0,s.jsx)(n.td,{children:"\u2705 Yes"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Long-form content generation"}),(0,s.jsx)(n.td,{children:"\u2705 Yes"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Code generation with preview"}),(0,s.jsx)(n.td,{children:"\u2705 Yes"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Short completions (<50 words)"}),(0,s.jsx)(n.td,{children:"\u274c No - regular fetch"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Background jobs"}),(0,s.jsx)(n.td,{children:"\u274c No - use job queue"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Bidirectional chat"}),(0,s.jsx)(n.td,{children:"\u26a0\ufe0f Use WebSockets instead"})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"technology-comparison",children:"Technology Comparison"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Feature"}),(0,s.jsx)(n.th,{children:"SSE"}),(0,s.jsx)(n.th,{children:"WebSockets"}),(0,s.jsx)(n.th,{children:"Long Polling"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Complexity"}),(0,s.jsx)(n.td,{children:"Low"}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"High"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Auto-reconnect"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u274c"}),(0,s.jsx)(n.td,{children:"\u274c"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Bidirectional"}),(0,s.jsx)(n.td,{children:"\u274c"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u274c"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Firewall-friendly"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u26a0\ufe0f"}),(0,s.jsx)(n.td,{children:"\u2705"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Browser support"}),(0,s.jsx)(n.td,{children:"\u2705 All modern"}),(0,s.jsx)(n.td,{children:"\u2705 All modern"}),(0,s.jsx)(n.td,{children:"\u2705 Universal"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"LLM API support"}),(0,s.jsx)(n.td,{children:"\u2705 Standard"}),(0,s.jsx)(n.td,{children:"\u274c Rare"}),(0,s.jsx)(n.td,{children:"\u274c Not used"})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"/references/sse-protocol.md"})," - Server-Sent Events specification details"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"/references/vercel-ai-sdk.md"})," - Vercel AI SDK integration patterns"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"/references/error-recovery.md"})," - Stream error handling strategies"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"scripts",children:"Scripts"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"scripts/stream_tester.ts"})," - Test SSE endpoints locally"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"scripts/token_counter.ts"})," - Estimate costs before generation"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"This skill guides"}),": LLM streaming implementation | SSE protocol | Real-time UI updates | Cancellation | Error recovery | Token-by-token display"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>l});var r=t(96540);const s={},o=r.createContext(s);function i(e){const n=r.useContext(o);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);