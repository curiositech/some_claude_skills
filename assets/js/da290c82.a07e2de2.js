"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[74132],{28453:(e,i,n)=>{n.d(i,{R:()=>l,x:()=>a});var s=n(96540);const r={},t=s.createContext(r);function l(e){const i=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function a(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),s.createElement(t.Provider,{value:i},e.children)}},33754:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"skills/clip_aware_embeddings/index","title":"\ud83e\udd16 Clip Aware Embeddings","description":"Semantic image-text matching with CLIP and alternatives. Use for image search, zero-shot classification, similarity matching. NOT for counting objects, fine-grained classification (celebrities, car models), spatial reasoning, or compositional queries. Activate on \\"CLIP\\", \\"embeddings\\", \\"image similarity\\", \\"semantic search\\", \\"zero-shot classification\\", \\"image-text matching\\".","source":"@site/docs/skills/clip_aware_embeddings/index.md","sourceDirName":"skills/clip_aware_embeddings","slug":"/skills/clip_aware_embeddings/","permalink":"/docs/skills/clip_aware_embeddings/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_label":"Clip Aware Embeddings","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Voice Audio Implementation ...","permalink":"/docs/skills/voice_audio_engineer/references/implementations"},"next":{"title":"Photo Content Recognition Curation Expert","permalink":"/docs/skills/photo_content_recognition_curation_expert/"}}');var r=n(74848),t=n(28453);const l={sidebar_label:"Clip Aware Embeddings",sidebar_position:1},a="\ud83e\udd16 Clip Aware Embeddings",o={},c=[{value:"Allowed Tools",id:"allowed-tools",level:2},{value:"Tags",id:"tags",level:2},{value:"\ud83e\udd1d Pairs Great With",id:"-pairs-great-with",level:2},{value:"MCP Integrations",id:"mcp-integrations",level:2},{value:"Quick Decision Tree",id:"quick-decision-tree",level:2},{value:"When to Use This Skill",id:"when-to-use-this-skill",level:2},{value:"Installation",id:"installation",level:2},{value:"Basic Usage",id:"basic-usage",level:2},{value:"Image Search",id:"image-search",level:3},{value:"Common Anti-Patterns",id:"common-anti-patterns",level:2},{value:"Anti-Pattern 1: &quot;CLIP for Everything&quot;",id:"anti-pattern-1-clip-for-everything",level:3},{value:"Anti-Pattern 2: Fine-Grained Classification",id:"anti-pattern-2-fine-grained-classification",level:3},{value:"Anti-Pattern 3: Spatial Understanding",id:"anti-pattern-3-spatial-understanding",level:3},{value:"Anti-Pattern 4: Attribute Binding",id:"anti-pattern-4-attribute-binding",level:3},{value:"Evolution Timeline",id:"evolution-timeline",level:2},{value:"2021: CLIP Released",id:"2021-clip-released",level:3},{value:"2022-2023: Limitations Discovered",id:"2022-2023-limitations-discovered",level:3},{value:"2024: Alternatives Emerge",id:"2024-alternatives-emerge",level:3},{value:"2025: Current Best Practices",id:"2025-current-best-practices",level:3},{value:"Validation Script",id:"validation-script",level:2},{value:"Task-Specific Guidance",id:"task-specific-guidance",level:2},{value:"Image Search (CLIP \u2713)",id:"image-search-clip-",level:3},{value:"Zero-Shot Classification (CLIP \u2713)",id:"zero-shot-classification-clip-",level:3},{value:"Object Counting (CLIP \u2717)",id:"object-counting-clip-",level:3},{value:"Fine-Grained Classification (CLIP \u2717)",id:"fine-grained-classification-clip-",level:3},{value:"Spatial Reasoning (CLIP \u2717)",id:"spatial-reasoning-clip-",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Issue: CLIP gives unexpected results",id:"issue-clip-gives-unexpected-results",level:3},{value:"Issue: Low similarity scores",id:"issue-low-similarity-scores",level:3},{value:"Model Selection Guide",id:"model-selection-guide",level:2},{value:"Performance Notes",id:"performance-notes",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const i={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(i.header,{children:(0,r.jsx)(i.h1,{id:"-clip-aware-embeddings",children:"\ud83e\udd16 Clip Aware Embeddings"})}),"\n",(0,r.jsx)(i.p,{children:'Semantic image-text matching with CLIP and alternatives. Use for image search, zero-shot classification, similarity matching. NOT for counting objects, fine-grained classification (celebrities, car models), spatial reasoning, or compositional queries. Activate on "CLIP", "embeddings", "image similarity", "semantic search", "zero-shot classification", "image-text matching".'}),"\n",(0,r.jsx)(i.hr,{}),"\n",(0,r.jsx)(i.h2,{id:"allowed-tools",children:"Allowed Tools"}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{children:"Read, Write, Edit, Bash\n"})}),"\n",(0,r.jsx)(i.h2,{id:"tags",children:"Tags"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.code,{children:"clip"})," ",(0,r.jsx)(i.code,{children:"embeddings"})," ",(0,r.jsx)(i.code,{children:"vision"})," ",(0,r.jsx)(i.code,{children:"similarity"})," ",(0,r.jsx)(i.code,{children:"zero-shot"})]}),"\n",(0,r.jsx)(i.h2,{id:"-pairs-great-with",children:"\ud83e\udd1d Pairs Great With"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:(0,r.jsx)(i.a,{href:"/docs/skills/photo_content_recognition_curation_expert",children:"Photo Content Recognition Curation Expert"})}),": Content-aware photo processing"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:(0,r.jsx)(i.a,{href:"/docs/skills/collage_layout_expert",children:"Collage Layout Expert"})}),": Semantic image matching for layouts"]}),"\n"]}),"\n",(0,r.jsx)(i.h1,{id:"clip-aware-image-embeddings",children:"CLIP-Aware Image Embeddings"}),"\n",(0,r.jsx)(i.p,{children:"Smart image-text matching that knows when CLIP works and when to use alternatives."}),"\n",(0,r.jsx)(i.h2,{id:"mcp-integrations",children:"MCP Integrations"}),"\n",(0,r.jsxs)(i.table,{children:[(0,r.jsx)(i.thead,{children:(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.th,{children:"MCP"}),(0,r.jsx)(i.th,{children:"Purpose"})]})}),(0,r.jsxs)(i.tbody,{children:[(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:(0,r.jsx)(i.strong,{children:"Firecrawl"})}),(0,r.jsx)(i.td,{children:"Research latest CLIP alternatives and benchmarks"})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsxs)(i.td,{children:[(0,r.jsx)(i.strong,{children:"Hugging Face"})," (if configured)"]}),(0,r.jsx)(i.td,{children:"Access model cards and documentation"})]})]})]}),"\n",(0,r.jsx)(i.h2,{id:"quick-decision-tree",children:"Quick Decision Tree"}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{children:'Your task:\n\u251c\u2500 Semantic search ("find beach images") \u2192 CLIP \u2713\n\u251c\u2500 Zero-shot classification (broad categories) \u2192 CLIP \u2713\n\u251c\u2500 Counting objects \u2192 DETR, Faster R-CNN \u2717\n\u251c\u2500 Fine-grained ID (celebrities, car models) \u2192 Specialized model \u2717\n\u251c\u2500 Spatial relations ("cat left of dog") \u2192 GQA, SWIG \u2717\n\u2514\u2500 Compositional ("red car AND blue truck") \u2192 DCSMs, PC-CLIP \u2717\n'})}),"\n",(0,r.jsx)(i.h2,{id:"when-to-use-this-skill",children:"When to Use This Skill"}),"\n",(0,r.jsxs)(i.p,{children:["\u2705 ",(0,r.jsx)(i.strong,{children:"Use for"}),":"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Semantic image search"}),"\n",(0,r.jsx)(i.li,{children:"Broad category classification"}),"\n",(0,r.jsx)(i.li,{children:"Image similarity matching"}),"\n",(0,r.jsx)(i.li,{children:"Zero-shot tasks on new categories"}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:["\u274c ",(0,r.jsx)(i.strong,{children:"Do NOT use for"}),":"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Counting objects in images"}),"\n",(0,r.jsx)(i.li,{children:"Fine-grained classification"}),"\n",(0,r.jsx)(i.li,{children:"Spatial understanding"}),"\n",(0,r.jsx)(i.li,{children:"Attribute binding"}),"\n",(0,r.jsx)(i.li,{children:"Negation handling"}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"installation",children:"Installation"}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-bash",children:"pip install transformers pillow torch sentence-transformers --break-system-packages\n"})}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Validation"}),": Run ",(0,r.jsx)(i.code,{children:"python scripts/validate_setup.py"})]}),"\n",(0,r.jsx)(i.h2,{id:"basic-usage",children:"Basic Usage"}),"\n",(0,r.jsx)(i.h3,{id:"image-search",children:"Image Search"}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-python",children:'from transformers import CLIPProcessor, CLIPModel\nfrom PIL import Image\n\nmodel = CLIPModel.from_pretrained("openai/clip-vit-large-patch14")\nprocessor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")\n\n# Embed images\nimages = [Image.open(f"img{i}.jpg") for i in range(10)]\ninputs = processor(images=images, return_tensors="pt")\nimage_features = model.get_image_features(**inputs)\n\n# Search with text\ntext_inputs = processor(text=["a beach at sunset"], return_tensors="pt")\ntext_features = model.get_text_features(**text_inputs)\n\n# Compute similarity\nsimilarity = (image_features @ text_features.T).softmax(dim=0)\n'})}),"\n",(0,r.jsx)(i.h2,{id:"common-anti-patterns",children:"Common Anti-Patterns"}),"\n",(0,r.jsx)(i.h3,{id:"anti-pattern-1-clip-for-everything",children:'Anti-Pattern 1: "CLIP for Everything"'}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"\u274c Wrong"}),":"]}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-python",children:'# Using CLIP to count cars in an image\nprompt = "How many cars are in this image?"\n# CLIP cannot count - it will give nonsense results\n'})}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Why wrong"}),": CLIP's architecture collapses spatial information into a single vector. It literally cannot count."]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"\u2713 Right"}),":"]}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-python",children:'from transformers import DetrImageProcessor, DetrForObjectDetection\n\nprocessor = DetrImageProcessor.from_pretrained("facebook/detr-resnet-50")\nmodel = DetrForObjectDetection.from_pretrained("facebook/detr-resnet-50")\n\n# Detect objects\nresults = model(**processor(images=image, return_tensors="pt"))\n# Filter for cars and count\ncar_detections = [d for d in results if d[\'label\'] == \'car\']\ncount = len(car_detections)\n'})}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"How to detect"}),': If query contains "how many", "count", or numeric questions \u2192 Use object detection']}),"\n",(0,r.jsx)(i.hr,{}),"\n",(0,r.jsx)(i.h3,{id:"anti-pattern-2-fine-grained-classification",children:"Anti-Pattern 2: Fine-Grained Classification"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"\u274c Wrong"}),":"]}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-python",children:'# Trying to identify specific celebrities with CLIP\nprompts = ["Tom Hanks", "Brad Pitt", "Morgan Freeman"]\n# CLIP will perform poorly - not trained for fine-grained face ID\n'})}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Why wrong"}),": CLIP trained on coarse categories. Fine-grained faces, car models, flower species require specialized models."]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"\u2713 Right"}),":"]}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-python",children:'# Use a fine-tuned face recognition model\nfrom transformers import AutoFeatureExtractor, AutoModelForImageClassification\n\nmodel = AutoModelForImageClassification.from_pretrained(\n    "microsoft/resnet-50"  # Then fine-tune on celebrity dataset\n)\n# Or use dedicated face recognition: ArcFace, CosFace\n'})}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"How to detect"}),": If query asks to distinguish between similar items in same category \u2192 Use specialized model"]}),"\n",(0,r.jsx)(i.hr,{}),"\n",(0,r.jsx)(i.h3,{id:"anti-pattern-3-spatial-understanding",children:"Anti-Pattern 3: Spatial Understanding"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"\u274c Wrong"}),":"]}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-python",children:'# CLIP cannot understand spatial relationships\nprompts = [\n    "cat to the left of dog",\n    "cat to the right of dog"\n]\n# Will give nearly identical scores\n'})}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Why wrong"}),': CLIP embeddings lose spatial topology. "Left" and "right" are treated as bag-of-words.']}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"\u2713 Right"}),":"]}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-python",children:'# Use a spatial reasoning model\n# Examples: GQA models, Visual Genome models, SWIG\nfrom swig_model import SpatialRelationModel\n\nmodel = SpatialRelationModel()\nresult = model.predict_relation(image, "cat", "dog")\n# Returns: "left", "right", "above", "below", etc.\n'})}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"How to detect"}),": If query contains directional words (left, right, above, under, next to) \u2192 Use spatial model"]}),"\n",(0,r.jsx)(i.hr,{}),"\n",(0,r.jsx)(i.h3,{id:"anti-pattern-4-attribute-binding",children:"Anti-Pattern 4: Attribute Binding"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"\u274c Wrong"}),":"]}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-python",children:'prompts = [\n    "red car and blue truck",\n    "blue car and red truck"\n]\n# CLIP often gives similar scores for both\n'})}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Why wrong"}),': CLIP cannot bind attributes to objects. It sees "red, blue, car, truck" as a bag of concepts.']}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"\u2713 Right - Use PC-CLIP or DCSMs"}),":"]}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-python",children:'# PC-CLIP: Fine-tuned for pairwise comparisons\nfrom pc_clip import PCCLIPModel\n\nmodel = PCCLIPModel.from_pretrained("pc-clip-vit-l")\n# Or use DCSMs (Dense Cosine Similarity Maps)\n'})}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"How to detect"}),": If query has multiple objects with different attributes \u2192 Use compositional model"]}),"\n",(0,r.jsx)(i.hr,{}),"\n",(0,r.jsx)(i.h2,{id:"evolution-timeline",children:"Evolution Timeline"}),"\n",(0,r.jsx)(i.h3,{id:"2021-clip-released",children:"2021: CLIP Released"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Revolutionary: zero-shot, 400M image-text pairs"}),"\n",(0,r.jsx)(i.li,{children:"Widely adopted for everything"}),"\n",(0,r.jsx)(i.li,{children:"Limitations not yet understood"}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"2022-2023-limitations-discovered",children:"2022-2023: Limitations Discovered"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Cannot count objects"}),"\n",(0,r.jsx)(i.li,{children:"Poor at fine-grained classification"}),"\n",(0,r.jsx)(i.li,{children:"Fails spatial reasoning"}),"\n",(0,r.jsx)(i.li,{children:"Can't bind attributes"}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"2024-alternatives-emerge",children:"2024: Alternatives Emerge"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"DCSMs"}),": Preserve patch/token topology"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"PC-CLIP"}),": Trained on pairwise comparisons"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.strong,{children:"SpLiCE"}),": Sparse interpretable embeddings"]}),"\n"]}),"\n",(0,r.jsx)(i.h3,{id:"2025-current-best-practices",children:"2025: Current Best Practices"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"Use CLIP for what it's good at"}),"\n",(0,r.jsx)(i.li,{children:"Task-specific models for limitations"}),"\n",(0,r.jsx)(i.li,{children:"Compositional models for complex queries"}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"LLM Mistake"}),": LLMs trained on 2021-2023 data will suggest CLIP for everything because limitations weren't widely known. This skill corrects that."]}),"\n",(0,r.jsx)(i.hr,{}),"\n",(0,r.jsx)(i.h2,{id:"validation-script",children:"Validation Script"}),"\n",(0,r.jsx)(i.p,{children:"Before using CLIP, check if it's appropriate:"}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-bash",children:'python scripts/validate_clip_usage.py \\\n    --query "your query here" \\\n    --check-all\n'})}),"\n",(0,r.jsx)(i.p,{children:"Returns:"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"\u2705 CLIP is appropriate"}),"\n",(0,r.jsx)(i.li,{children:"\u274c Use alternative (with suggestion)"}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"task-specific-guidance",children:"Task-Specific Guidance"}),"\n",(0,r.jsx)(i.h3,{id:"image-search-clip-",children:"Image Search (CLIP \u2713)"}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-python",children:'# Good use of CLIP\nqueries = ["beach", "mountain", "city skyline"]\n# Works well for broad semantic concepts\n'})}),"\n",(0,r.jsx)(i.h3,{id:"zero-shot-classification-clip-",children:"Zero-Shot Classification (CLIP \u2713)"}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-python",children:'# Good: Broad categories\ncategories = ["indoor", "outdoor", "nature", "urban"]\n# CLIP excels at this\n'})}),"\n",(0,r.jsx)(i.h3,{id:"object-counting-clip-",children:"Object Counting (CLIP \u2717)"}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-python",children:"# Use object detection instead\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\n# See /references/object_detection.md\n"})}),"\n",(0,r.jsx)(i.h3,{id:"fine-grained-classification-clip-",children:"Fine-Grained Classification (CLIP \u2717)"}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-python",children:"# Use specialized models\n# See /references/fine_grained_models.md\n"})}),"\n",(0,r.jsx)(i.h3,{id:"spatial-reasoning-clip-",children:"Spatial Reasoning (CLIP \u2717)"}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-python",children:"# Use spatial relation models\n# See /references/spatial_models.md\n"})}),"\n",(0,r.jsx)(i.hr,{}),"\n",(0,r.jsx)(i.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,r.jsx)(i.h3,{id:"issue-clip-gives-unexpected-results",children:"Issue: CLIP gives unexpected results"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Check"}),":"]}),"\n",(0,r.jsxs)(i.ol,{children:["\n",(0,r.jsx)(i.li,{children:"Is this a counting task? \u2192 Use object detection"}),"\n",(0,r.jsx)(i.li,{children:"Fine-grained classification? \u2192 Use specialized model"}),"\n",(0,r.jsx)(i.li,{children:"Spatial query? \u2192 Use spatial model"}),"\n",(0,r.jsx)(i.li,{children:"Multiple objects with attributes? \u2192 Use compositional model"}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Validation"}),":"]}),"\n",(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-bash",children:'python scripts/diagnose_clip_issue.py --image path/to/image --query "your query"\n'})}),"\n",(0,r.jsx)(i.h3,{id:"issue-low-similarity-scores",children:"Issue: Low similarity scores"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Possible causes"}),":"]}),"\n",(0,r.jsxs)(i.ol,{children:["\n",(0,r.jsx)(i.li,{children:"Query too specific (CLIP works better with broad concepts)"}),"\n",(0,r.jsx)(i.li,{children:"Fine-grained task (not CLIP's strength)"}),"\n",(0,r.jsx)(i.li,{children:"Need to adjust threshold"}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Solution"}),": Try broader query or use alternative model"]}),"\n",(0,r.jsx)(i.hr,{}),"\n",(0,r.jsx)(i.h2,{id:"model-selection-guide",children:"Model Selection Guide"}),"\n",(0,r.jsxs)(i.table,{children:[(0,r.jsx)(i.thead,{children:(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.th,{children:"Model"}),(0,r.jsx)(i.th,{children:"Best For"}),(0,r.jsx)(i.th,{children:"Avoid For"})]})}),(0,r.jsxs)(i.tbody,{children:[(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"CLIP ViT-L/14"}),(0,r.jsx)(i.td,{children:"Semantic search, broad categories"}),(0,r.jsx)(i.td,{children:"Counting, fine-grained, spatial"})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"DETR"}),(0,r.jsx)(i.td,{children:"Object detection, counting"}),(0,r.jsx)(i.td,{children:"Semantic similarity"})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"DINOv2"}),(0,r.jsx)(i.td,{children:"Fine-grained features"}),(0,r.jsx)(i.td,{children:"Text-image matching"})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"PC-CLIP"}),(0,r.jsx)(i.td,{children:"Attribute binding, comparisons"}),(0,r.jsx)(i.td,{children:"General embedding"})]}),(0,r.jsxs)(i.tr,{children:[(0,r.jsx)(i.td,{children:"DCSMs"}),(0,r.jsx)(i.td,{children:"Compositional reasoning"}),(0,r.jsx)(i.td,{children:"Simple similarity"})]})]})]}),"\n",(0,r.jsx)(i.h2,{id:"performance-notes",children:"Performance Notes"}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"CLIP models"}),":"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"ViT-B/32: Fast, lower quality"}),"\n",(0,r.jsx)(i.li,{children:"ViT-L/14: Balanced (recommended)"}),"\n",(0,r.jsx)(i.li,{children:"ViT-g-14: Highest quality, slower"}),"\n"]}),"\n",(0,r.jsxs)(i.p,{children:[(0,r.jsx)(i.strong,{children:"Inference time"})," (single image, CPU):"]}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsx)(i.li,{children:"ViT-B/32: ~100ms"}),"\n",(0,r.jsx)(i.li,{children:"ViT-L/14: ~300ms"}),"\n",(0,r.jsx)(i.li,{children:"ViT-g-14: ~1000ms"}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"/references/clip_limitations.md"})," - Detailed analysis of CLIP's failures"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"/references/alternatives.md"})," - When to use what model"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"/references/compositional_reasoning.md"})," - DCSMs and PC-CLIP deep dive"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"/scripts/validate_clip_usage.py"})," - Pre-flight validation tool"]}),"\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"/scripts/diagnose_clip_issue.py"})," - Debug unexpected results"]}),"\n"]}),"\n",(0,r.jsx)(i.hr,{}),"\n",(0,r.jsx)(i.p,{children:(0,r.jsx)(i.em,{children:"See CHANGELOG.md for version history."})})]})}function h(e={}){const{wrapper:i}={...(0,t.R)(),...e.components};return i?(0,r.jsx)(i,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);