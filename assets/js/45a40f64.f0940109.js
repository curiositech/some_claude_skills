"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[35841],{28453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>o});var i=t(96540);const r={},a=i.createContext(r);function s(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),i.createElement(a.Provider,{value:n},e.children)}},81396:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>s,metadata:()=>i,toc:()=>p});const i=JSON.parse('{"id":"skills/automatic_stateful_prompt_improver/references/ape-opro-implementation","title":"APE and OPRO Implementation Guide","description":"This document details the implementation of Automatic Prompt Engineer (APE) and Optimization by Prompting (OPRO) algorithms for prompt improvement.","source":"@site/docs/skills/automatic_stateful_prompt_improver/references/ape-opro-implementation.md","sourceDirName":"skills/automatic_stateful_prompt_improver/references","slug":"/skills/automatic_stateful_prompt_improver/references/ape-opro-implementation","permalink":"/docs/skills/automatic_stateful_prompt_improver/references/ape-opro-implementation","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"APE and OPRO Implementation Guide","sidebar_label":"APE and OPRO Implementation...","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Automatic Stateful Prompt Improver","permalink":"/docs/skills/automatic_stateful_prompt_improver/"},"next":{"title":"DSPy Optimization Patterns ...","permalink":"/docs/skills/automatic_stateful_prompt_improver/references/dspy-patterns"}}');var r=t(74848),a=t(28453);const s={title:"APE and OPRO Implementation Guide",sidebar_label:"APE and OPRO Implementation...",sidebar_position:1},o="APE and OPRO Implementation Guide",l={},p=[{value:"APE: Automatic Prompt Engineer",id:"ape-automatic-prompt-engineer",level:2},{value:"Core Idea",id:"core-idea",level:3},{value:"Algorithm",id:"algorithm",level:3},{value:"Implementation",id:"implementation",level:3},{value:"APE for Prompt Improvement",id:"ape-for-prompt-improvement",level:3},{value:"OPRO: Optimization by Prompting",id:"opro-optimization-by-prompting",level:2},{value:"Core Idea",id:"core-idea-1",level:3},{value:"Algorithm",id:"algorithm-1",level:3},{value:"Implementation",id:"implementation-1",level:3},{value:"OPRO for Prompt Improvement",id:"opro-for-prompt-improvement",level:3},{value:"Combining APE and OPRO",id:"combining-ape-and-opro",level:2},{value:"Hybrid Approach",id:"hybrid-approach",level:3},{value:"Without LLM Calls: Pattern-Based Improvement",id:"without-llm-calls-pattern-based-improvement",level:2},{value:"Convergence and Stopping Criteria",id:"convergence-and-stopping-criteria",level:2},{value:"When to Stop Optimization",id:"when-to-stop-optimization",level:3}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"ape-and-opro-implementation-guide",children:"APE and OPRO Implementation Guide"})}),"\n",(0,r.jsx)(n.p,{children:"This document details the implementation of Automatic Prompt Engineer (APE) and Optimization by Prompting (OPRO) algorithms for prompt improvement."}),"\n",(0,r.jsx)(n.h2,{id:"ape-automatic-prompt-engineer",children:"APE: Automatic Prompt Engineer"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Source"}),': Zhou et al., 2022 - "Large Language Models Are Human-Level Prompt Engineers"']}),"\n",(0,r.jsx)(n.h3,{id:"core-idea",children:"Core Idea"}),"\n",(0,r.jsx)(n.p,{children:"Use LLMs to generate instruction candidates, then select the best based on evaluation."}),"\n",(0,r.jsx)(n.h3,{id:"algorithm",children:"Algorithm"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'APE Algorithm:\n1. Given: Task description T, examples E = {(x_i, y_i)}\n2. Generate instruction candidates:\n   - Prompt LLM: "Given these examples, what instruction would produce this output?"\n   - Generate N candidates (N=20 typical)\n3. Evaluate each candidate:\n   - For each instruction I_j:\n     - Score = accuracy on held-out examples\n     - Or: Score = log_prob(y | I_j, x)\n4. Select best:\n   - I* = argmax_j Score(I_j)\n5. Optionally refine:\n   - Generate variations of I*\n   - Re-evaluate and select best variation\n'})}),"\n",(0,r.jsx)(n.h3,{id:"implementation",children:"Implementation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class APEOptimizer:\n    """\n    Automatic Prompt Engineer implementation.\n    """\n\n    def __init__(self, llm_client, num_candidates: int = 20):\n        self.llm = llm_client\n        self.num_candidates = num_candidates\n\n    async def generate_candidates(\n        self,\n        task_description: str,\n        examples: list[tuple[str, str]],\n        num_candidates: int = None\n    ) -> list[str]:\n        """\n        Generate instruction candidates from examples.\n        """\n        n = num_candidates or self.num_candidates\n\n        # Format examples\n        examples_text = "\\n".join([\n            f"Input: {x}\\nOutput: {y}"\n            for x, y in examples[:5]  # Use first 5 examples\n        ])\n\n        generation_prompt = f"""\n        I have a task where given certain inputs, I want specific outputs.\n\n        Here are some examples:\n        {examples_text}\n\n        Your task: Generate {n} different instructions that would cause an AI to produce these outputs from these inputs.\n\n        Requirements:\n        - Each instruction should be clear and complete\n        - Instructions should be diverse (different phrasings, approaches)\n        - Instructions should be concise but complete\n\n        Generate exactly {n} instructions, one per line, numbered 1-{n}:\n        """\n\n        response = await self.llm.generate(generation_prompt)\n        candidates = self._parse_numbered_list(response)\n\n        return candidates[:n]\n\n    async def evaluate_candidate(\n        self,\n        instruction: str,\n        eval_examples: list[tuple[str, str]]\n    ) -> float:\n        """\n        Evaluate an instruction on held-out examples.\n        """\n        correct = 0\n        total = len(eval_examples)\n\n        for x, expected_y in eval_examples:\n            prompt = f"{instruction}\\n\\nInput: {x}\\n\\nOutput:"\n            response = await self.llm.generate(prompt)\n\n            # Simple exact match (could use semantic similarity)\n            if self._normalize(response) == self._normalize(expected_y):\n                correct += 1\n\n        return correct / total if total > 0 else 0\n\n    async def optimize(\n        self,\n        task_description: str,\n        examples: list[tuple[str, str]],\n        refinement_iterations: int = 3\n    ) -> tuple[str, float]:\n        """\n        Full APE optimization pipeline.\n\n        Returns:\n            (best_instruction, best_score)\n        """\n        # Split examples\n        train_examples = examples[:len(examples)//2]\n        eval_examples = examples[len(examples)//2:]\n\n        # Generate initial candidates\n        candidates = await self.generate_candidates(\n            task_description,\n            train_examples\n        )\n\n        # Evaluate each\n        scores = []\n        for candidate in candidates:\n            score = await self.evaluate_candidate(candidate, eval_examples)\n            scores.append((candidate, score))\n\n        # Sort by score\n        scores.sort(key=lambda x: x[1], reverse=True)\n        best_instruction, best_score = scores[0]\n\n        # Refinement iterations\n        for i in range(refinement_iterations):\n            # Generate variations of best\n            variations = await self._generate_variations(best_instruction)\n\n            # Evaluate variations\n            for variation in variations:\n                score = await self.evaluate_candidate(variation, eval_examples)\n                if score > best_score:\n                    best_instruction = variation\n                    best_score = score\n\n        return best_instruction, best_score\n\n    async def _generate_variations(\n        self,\n        instruction: str,\n        num_variations: int = 5\n    ) -> list[str]:\n        """\n        Generate variations of an instruction.\n        """\n        variation_prompt = f"""\n        Generate {num_variations} variations of the following instruction.\n        Keep the semantic meaning but vary the phrasing, structure, or emphasis.\n\n        Original instruction:\n        {instruction}\n\n        Variations (numbered 1-{num_variations}):\n        """\n\n        response = await self.llm.generate(variation_prompt)\n        return self._parse_numbered_list(response)\n\n    def _parse_numbered_list(self, text: str) -> list[str]:\n        """Parse numbered list from LLM output."""\n        lines = text.strip().split(\'\\n\')\n        items = []\n        for line in lines:\n            # Remove numbering (1., 1), etc.)\n            cleaned = re.sub(r\'^[\\d]+[\\.\\)\\:]?\\s*\', \'\', line.strip())\n            if cleaned:\n                items.append(cleaned)\n        return items\n\n    def _normalize(self, text: str) -> str:\n        """Normalize text for comparison."""\n        return text.strip().lower()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"ape-for-prompt-improvement",children:"APE for Prompt Improvement"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'async def improve_prompt_ape_style(\n    original_prompt: str,\n    examples: list[tuple[str, str]],\n    llm_client\n) -> str:\n    """\n    Improve a prompt using APE methodology.\n    """\n    optimizer = APEOptimizer(llm_client)\n\n    # Use original prompt as task description\n    task_description = f"Original prompt: {original_prompt}"\n\n    # Optimize\n    best_instruction, score = await optimizer.optimize(\n        task_description,\n        examples,\n        refinement_iterations=3\n    )\n\n    return best_instruction\n'})}),"\n",(0,r.jsx)(n.h2,{id:"opro-optimization-by-prompting",children:"OPRO: Optimization by Prompting"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Source"}),': Yang et al., 2023 - "Large Language Models as Optimizers"']}),"\n",(0,r.jsx)(n.h3,{id:"core-idea-1",children:"Core Idea"}),"\n",(0,r.jsx)(n.p,{children:"Treat optimization itself as a prompting task. The LLM sees previous solutions and their scores, then generates new (hopefully better) solutions."}),"\n",(0,r.jsx)(n.h3,{id:"algorithm-1",children:"Algorithm"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'OPRO Algorithm:\n1. Initialize: meta_prompt = empty history\n2. For i in range(max_iterations):\n   a. Add instruction to meta_prompt:\n      "[previous solutions + scores]\n       Generate a new solution that scores higher."\n   b. LLM generates new candidate solution\n   c. Evaluate candidate on validation set\n   d. Add (candidate, score) to history\n   e. If converged: break\n3. Return best solution from history\n'})}),"\n",(0,r.jsx)(n.h3,{id:"implementation-1",children:"Implementation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class OPROOptimizer:\n    """\n    Optimization by Prompting implementation.\n    """\n\n    def __init__(\n        self,\n        llm_client,\n        max_history: int = 20,\n        temperature: float = 0.7\n    ):\n        self.llm = llm_client\n        self.max_history = max_history\n        self.temperature = temperature\n        self.history = []\n\n    def _build_meta_prompt(self, task_description: str) -> str:\n        """\n        Build the meta-prompt showing previous solutions and scores.\n        """\n        # Sort history by score (ascending) - show progression\n        sorted_history = sorted(self.history, key=lambda x: x[1])\n\n        # Take most recent/best entries\n        recent = sorted_history[-self.max_history:]\n\n        history_text = "\\n".join([\n            f"Instruction: {inst}\\nScore: {score:.3f}"\n            for inst, score in recent\n        ])\n\n        meta_prompt = f"""\n        Your task is to generate an instruction that will achieve a high score on this task:\n\n        {task_description}\n\n        Here are some previous instructions and their scores (higher is better):\n\n        {history_text}\n\n        Based on the patterns in what works well:\n        1. Analyze why high-scoring instructions perform better\n        2. Identify elements that correlate with success\n        3. Generate a NEW instruction that should score even higher\n\n        Requirements:\n        - The instruction should be different from previous ones\n        - Build on what worked, avoid what didn\'t\n        - Be specific and clear\n\n        New instruction (just the instruction, nothing else):\n        """\n\n        return meta_prompt\n\n    async def generate_candidate(self, task_description: str) -> str:\n        """\n        Generate a new candidate instruction based on history.\n        """\n        meta_prompt = self._build_meta_prompt(task_description)\n\n        response = await self.llm.generate(\n            meta_prompt,\n            temperature=self.temperature\n        )\n\n        return response.strip()\n\n    async def evaluate(\n        self,\n        instruction: str,\n        eval_fn: callable\n    ) -> float:\n        """\n        Evaluate an instruction using provided evaluation function.\n        """\n        return await eval_fn(instruction)\n\n    async def optimize(\n        self,\n        task_description: str,\n        eval_fn: callable,\n        initial_candidates: list[str] = None,\n        max_iterations: int = 20,\n        convergence_threshold: float = 0.01,\n        convergence_window: int = 5\n    ) -> tuple[str, float, list]:\n        """\n        Run OPRO optimization.\n\n        Args:\n            task_description: Description of the task\n            eval_fn: Async function that scores an instruction\n            initial_candidates: Optional starting candidates\n            max_iterations: Maximum optimization iterations\n            convergence_threshold: Stop if improvement < this\n            convergence_window: Check convergence over this many iterations\n\n        Returns:\n            (best_instruction, best_score, optimization_history)\n        """\n        # Initialize with any provided candidates\n        if initial_candidates:\n            for candidate in initial_candidates:\n                score = await self.evaluate(candidate, eval_fn)\n                self.history.append((candidate, score))\n\n        # Track scores for convergence detection\n        scores_over_time = [h[1] for h in self.history]\n\n        for i in range(max_iterations):\n            # Generate new candidate\n            candidate = await self.generate_candidate(task_description)\n\n            # Evaluate\n            score = await self.evaluate(candidate, eval_fn)\n\n            # Add to history\n            self.history.append((candidate, score))\n            scores_over_time.append(score)\n\n            # Check convergence\n            if len(scores_over_time) >= convergence_window:\n                recent_best = max(scores_over_time[-convergence_window:])\n                previous_best = max(scores_over_time[:-convergence_window])\n                if recent_best - previous_best < convergence_threshold:\n                    break\n\n        # Find best\n        best_instruction, best_score = max(self.history, key=lambda x: x[1])\n\n        return best_instruction, best_score, self.history\n'})}),"\n",(0,r.jsx)(n.h3,{id:"opro-for-prompt-improvement",children:"OPRO for Prompt Improvement"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'async def improve_prompt_opro_style(\n    original_prompt: str,\n    eval_examples: list[tuple[str, str]],\n    llm_client,\n    max_iterations: int = 15\n) -> tuple[str, float]:\n    """\n    Improve a prompt using OPRO methodology.\n    """\n    optimizer = OPROOptimizer(llm_client)\n\n    # Create evaluation function\n    async def eval_fn(instruction: str) -> float:\n        correct = 0\n        for x, expected_y in eval_examples:\n            prompt = f"{instruction}\\n\\nInput: {x}"\n            response = await llm_client.generate(prompt)\n            if expected_y.lower() in response.lower():\n                correct += 1\n        return correct / len(eval_examples)\n\n    # Task description\n    task_description = f"""\n    Original prompt: {original_prompt}\n\n    The goal is to find an instruction that produces correct outputs for various inputs.\n    The instruction should be clear, specific, and effective.\n    """\n\n    # Start with original as baseline\n    initial_candidates = [original_prompt]\n\n    # Optimize\n    best, score, history = await optimizer.optimize(\n        task_description=task_description,\n        eval_fn=eval_fn,\n        initial_candidates=initial_candidates,\n        max_iterations=max_iterations\n    )\n\n    return best, score\n'})}),"\n",(0,r.jsx)(n.h2,{id:"combining-ape-and-opro",children:"Combining APE and OPRO"}),"\n",(0,r.jsx)(n.h3,{id:"hybrid-approach",children:"Hybrid Approach"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class HybridPromptOptimizer:\n    """\n    Combines APE (generation) with OPRO (optimization) for best results.\n\n    1. APE generates diverse initial candidates\n    2. OPRO iteratively improves using history\n    """\n\n    def __init__(self, llm_client):\n        self.llm = llm_client\n        self.ape = APEOptimizer(llm_client, num_candidates=10)\n        self.opro = OPROOptimizer(llm_client, max_history=20)\n\n    async def optimize(\n        self,\n        original_prompt: str,\n        examples: list[tuple[str, str]],\n        max_iterations: int = 20\n    ) -> tuple[str, float, dict]:\n        """\n        Hybrid optimization combining APE and OPRO.\n        """\n        # Split examples\n        train_examples = examples[:len(examples)//2]\n        eval_examples = examples[len(examples)//2:]\n\n        # Phase 1: APE generates initial candidates\n        candidates = await self.ape.generate_candidates(\n            f"Task: {original_prompt}",\n            train_examples,\n            num_candidates=10\n        )\n\n        # Add original prompt as candidate\n        candidates = [original_prompt] + candidates\n\n        # Evaluate initial candidates\n        initial_scores = []\n        for candidate in candidates:\n            score = await self._evaluate(candidate, eval_examples)\n            initial_scores.append((candidate, score))\n\n        # Sort and take top 5 as OPRO seed\n        initial_scores.sort(key=lambda x: x[1], reverse=True)\n        top_candidates = [c for c, s in initial_scores[:5]]\n\n        # Phase 2: OPRO iterative optimization\n        task_description = f"""\n        Task: Generate instructions for this prompt optimization task.\n        Original prompt: {original_prompt}\n\n        The instruction should make the AI produce correct outputs for various inputs.\n        """\n\n        # Initialize OPRO with APE candidates\n        for candidate, score in initial_scores[:5]:\n            self.opro.history.append((candidate, score))\n\n        # OPRO iterations\n        async def eval_fn(instruction):\n            return await self._evaluate(instruction, eval_examples)\n\n        best, score, history = await self.opro.optimize(\n            task_description=task_description,\n            eval_fn=eval_fn,\n            max_iterations=max_iterations - 5  # Already did 5 in APE\n        )\n\n        return best, score, {\n            \'ape_candidates\': len(candidates),\n            \'opro_iterations\': len(history) - 5,\n            \'final_history\': history\n        }\n\n    async def _evaluate(\n        self,\n        instruction: str,\n        examples: list[tuple[str, str]]\n    ) -> float:\n        """Simple evaluation function."""\n        correct = 0\n        for x, expected_y in examples:\n            prompt = f"{instruction}\\n\\nInput: {x}"\n            response = await self.llm.generate(prompt)\n            if self._matches(response, expected_y):\n                correct += 1\n        return correct / len(examples)\n\n    def _matches(self, response: str, expected: str) -> bool:\n        """Check if response matches expected."""\n        return expected.lower().strip() in response.lower()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"without-llm-calls-pattern-based-improvement",children:"Without LLM Calls: Pattern-Based Improvement"}),"\n",(0,r.jsx)(n.p,{children:"When you can't make LLM calls for optimization, use pattern-based rules:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class PatternBasedOptimizer:\n    """\n    Improve prompts using known-good patterns without LLM calls.\n    """\n\n    IMPROVEMENT_PATTERNS = [\n        {\n            "name": "add_structure",\n            "check": lambda p: not any(x in p.lower() for x in ["1.", "2.", "step", "first"]),\n            "apply": lambda p: f"{p}\\n\\nProvide your response in this format:\\n1. [First point]\\n2. [Second point]\\n3. [Summary]",\n            "expected_improvement": 0.15\n        },\n        {\n            "name": "add_cot",\n            "check": lambda p: "step by step" not in p.lower() and "think" not in p.lower(),\n            "apply": lambda p: f"{p}\\n\\nThink through this step by step before providing your final answer.",\n            "expected_improvement": 0.20\n        },\n        {\n            "name": "add_constraints",\n            "check": lambda p: len(p) < 100,\n            "apply": lambda p: f"{p}\\n\\nRequirements:\\n- Be specific and precise\\n- Support claims with evidence\\n- Keep response focused",\n            "expected_improvement": 0.10\n        },\n        {\n            "name": "add_role",\n            "check": lambda p: not p.lower().startswith(("you are", "as a", "act as")),\n            "apply": lambda p: f"You are an expert in this domain. {p}",\n            "expected_improvement": 0.05\n        },\n        {\n            "name": "add_output_format",\n            "check": lambda p: "format" not in p.lower() and "json" not in p.lower(),\n            "apply": lambda p: f"{p}\\n\\nFormat your response as a clear, structured answer.",\n            "expected_improvement": 0.10\n        }\n    ]\n\n    def improve(self, prompt: str) -> tuple[str, list[str]]:\n        """\n        Apply applicable improvement patterns.\n\n        Returns:\n            (improved_prompt, list of applied patterns)\n        """\n        improved = prompt\n        applied = []\n\n        for pattern in self.IMPROVEMENT_PATTERNS:\n            if pattern["check"](improved):\n                improved = pattern["apply"](improved)\n                applied.append(pattern["name"])\n\n        return improved, applied\n\n    def estimate_improvement(self, applied_patterns: list[str]) -> float:\n        """\n        Estimate expected improvement based on applied patterns.\n        """\n        total = 0\n        for pattern in self.IMPROVEMENT_PATTERNS:\n            if pattern["name"] in applied_patterns:\n                total += pattern["expected_improvement"]\n        return min(total, 0.5)  # Cap at 50% improvement\n'})}),"\n",(0,r.jsx)(n.h2,{id:"convergence-and-stopping-criteria",children:"Convergence and Stopping Criteria"}),"\n",(0,r.jsx)(n.h3,{id:"when-to-stop-optimization",children:"When to Stop Optimization"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def check_optimization_convergence(\n    scores: list[float],\n    iteration: int,\n    config: dict = None\n) -> tuple[bool, str]:\n    """\n    Check if optimization should stop.\n\n    Config options:\n    - max_iterations: Hard limit (default: 20)\n    - min_iterations: Minimum before stopping (default: 5)\n    - plateau_window: Iterations to check for plateau (default: 5)\n    - plateau_threshold: Minimum improvement (default: 0.01)\n    - target_score: Stop if reached (default: 0.95)\n    """\n    config = config or {}\n    max_iter = config.get(\'max_iterations\', 20)\n    min_iter = config.get(\'min_iterations\', 5)\n    window = config.get(\'plateau_window\', 5)\n    threshold = config.get(\'plateau_threshold\', 0.01)\n    target = config.get(\'target_score\', 0.95)\n\n    # Not enough iterations\n    if iteration < min_iter:\n        return False, "below_minimum_iterations"\n\n    # Max iterations reached\n    if iteration >= max_iter:\n        return True, "max_iterations_reached"\n\n    # Target reached\n    if scores[-1] >= target:\n        return True, f"target_reached ({scores[-1]:.3f} >= {target})"\n\n    # Plateau detection\n    if len(scores) >= window:\n        recent_improvement = max(scores[-window:]) - scores[-window]\n        if recent_improvement < threshold:\n            return True, f"plateau_detected (improvement: {recent_improvement:.4f})"\n\n    # Decreasing performance (possible overfitting)\n    if len(scores) >= 3 and all(scores[-i-1] < scores[-i-2] for i in range(2)):\n        return True, "performance_decreasing"\n\n    return False, "continue"\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.p,{children:"These implementations provide the foundation for systematic prompt optimization using research-backed algorithms."})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}}}]);