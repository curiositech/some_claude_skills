"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[29915],{28453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>c});var r=i(96540);const s={},t=r.createContext(s);function o(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),r.createElement(t.Provider,{value:n},e.children)}},46713:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"skills/computer_vision_pipeline/index","title":"\ud83d\udcca Computer Vision Pipeline","description":"Build production computer vision pipelines for object detection, tracking, and video analysis. Handles drone footage, wildlife monitoring, and real-time detection. Supports YOLO, Detectron2, TensorFlow, PyTorch. Use for archaeological surveys, conservation, security. Activate on \\"object detection\\", \\"video analysis\\", \\"YOLO\\", \\"tracking\\", \\"drone footage\\". NOT for simple image filters, photo editing, or face recognition APIs.","source":"@site/docs/skills/computer_vision_pipeline/index.md","sourceDirName":"skills/computer_vision_pipeline","slug":"/skills/computer_vision_pipeline/","permalink":"/docs/skills/computer_vision_pipeline/","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_label":"Computer Vision Pipeline","sidebar_position":1}}');var s=i(74848),t=i(28453);const o={sidebar_label:"Computer Vision Pipeline",sidebar_position:1},c="\ud83d\udcca Computer Vision Pipeline",l={},d=[{value:"Allowed Tools",id:"allowed-tools",level:2},{value:"References",id:"references",level:2},{value:"When to Use",id:"when-to-use",level:2},{value:"Technology Selection",id:"technology-selection",level:2},{value:"Object Detection Models",id:"object-detection-models",level:3},{value:"Common Anti-Patterns",id:"common-anti-patterns",level:2},{value:"Anti-Pattern 1: Not Preprocessing Frames Before Detection",id:"anti-pattern-1-not-preprocessing-frames-before-detection",level:3},{value:"Anti-Pattern 2: Processing Every Frame in Video",id:"anti-pattern-2-processing-every-frame-in-video",level:3},{value:"Anti-Pattern 3: Not Using Batch Inference",id:"anti-pattern-3-not-using-batch-inference",level:3},{value:"Anti-Pattern 4: Ignoring Non-Maximum Suppression (NMS) Tuning",id:"anti-pattern-4-ignoring-non-maximum-suppression-nms-tuning",level:3},{value:"Anti-Pattern 5: No Tracking Between Frames",id:"anti-pattern-5-no-tracking-between-frames",level:3},{value:"Production Checklist",id:"production-checklist",level:2},{value:"When to Use vs Avoid",id:"when-to-use-vs-avoid",level:2},{value:"References",id:"references-1",level:2},{value:"Scripts",id:"scripts",level:2}];function a(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"-computer-vision-pipeline",children:"\ud83d\udcca Computer Vision Pipeline"})}),"\n",(0,s.jsx)(n.p,{children:'Build production computer vision pipelines for object detection, tracking, and video analysis. Handles drone footage, wildlife monitoring, and real-time detection. Supports YOLO, Detectron2, TensorFlow, PyTorch. Use for archaeological surveys, conservation, security. Activate on "object detection", "video analysis", "YOLO", "tracking", "drone footage". NOT for simple image filters, photo editing, or face recognition APIs.'}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"allowed-tools",children:"Allowed Tools"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Read, Write, Edit, Bash(python*, pip*, ffmpeg*)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"./references/tracking-algorithms",children:"Multi-Object Tracking Algorithms"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"./references/video-processing",children:"Video Processing for Computer Vision"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"./references/yolo-guide",children:"YOLOv8 Guide"})}),"\n"]}),"\n",(0,s.jsx)(n.h1,{id:"computer-vision-pipeline",children:"Computer Vision Pipeline"}),"\n",(0,s.jsx)(n.p,{children:"Expert in building production-ready computer vision systems for object detection, tracking, and video analysis."}),"\n",(0,s.jsx)(n.h2,{id:"when-to-use",children:"When to Use"}),"\n",(0,s.jsxs)(n.p,{children:["\u2705 ",(0,s.jsx)(n.strong,{children:"Use for"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Drone footage analysis (archaeological surveys, conservation)"}),"\n",(0,s.jsx)(n.li,{children:"Wildlife monitoring and tracking"}),"\n",(0,s.jsx)(n.li,{children:"Real-time object detection systems"}),"\n",(0,s.jsx)(n.li,{children:"Video preprocessing and analysis"}),"\n",(0,s.jsx)(n.li,{children:"Custom model training and inference"}),"\n",(0,s.jsx)(n.li,{children:"Multi-object tracking (MOT)"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["\u274c ",(0,s.jsx)(n.strong,{children:"NOT for"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Simple image filters (use Pillow/PIL)"}),"\n",(0,s.jsx)(n.li,{children:"Photo editing (use Photoshop/GIMP)"}),"\n",(0,s.jsx)(n.li,{children:"Face recognition APIs (use AWS Rekognition)"}),"\n",(0,s.jsx)(n.li,{children:"Basic OCR (use Tesseract)"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"technology-selection",children:"Technology Selection"}),"\n",(0,s.jsx)(n.h3,{id:"object-detection-models",children:"Object Detection Models"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{children:"Speed (FPS)"}),(0,s.jsx)(n.th,{children:"Accuracy (mAP)"}),(0,s.jsx)(n.th,{children:"Use Case"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"YOLOv8"}),(0,s.jsx)(n.td,{children:"140"}),(0,s.jsx)(n.td,{children:"53.9%"}),(0,s.jsx)(n.td,{children:"Real-time detection"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Detectron2"}),(0,s.jsx)(n.td,{children:"25"}),(0,s.jsx)(n.td,{children:"58.7%"}),(0,s.jsx)(n.td,{children:"High accuracy, research"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"EfficientDet"}),(0,s.jsx)(n.td,{children:"35"}),(0,s.jsx)(n.td,{children:"55.1%"}),(0,s.jsx)(n.td,{children:"Mobile deployment"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Faster R-CNN"}),(0,s.jsx)(n.td,{children:"10"}),(0,s.jsx)(n.td,{children:"42.0%"}),(0,s.jsx)(n.td,{children:"Legacy systems"})]})]})]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Timeline"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"2015: Faster R-CNN (two-stage detection)"}),"\n",(0,s.jsx)(n.li,{children:"2016: YOLO v1 (one-stage, real-time)"}),"\n",(0,s.jsx)(n.li,{children:"2020: YOLOv5 (PyTorch, production-ready)"}),"\n",(0,s.jsx)(n.li,{children:"2023: YOLOv8 (state-of-the-art)"}),"\n",(0,s.jsx)(n.li,{children:"2024: YOLOv8 is industry standard for real-time"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Decision tree"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Need real-time (&gt;30 FPS)? \u2192 YOLOv8\nNeed highest accuracy? \u2192 Detectron2 Mask R-CNN\nNeed mobile deployment? \u2192 YOLOv8-nano or EfficientDet\nNeed instance segmentation? \u2192 Detectron2 or YOLOv8-seg\nNeed custom objects? \u2192 Fine-tune YOLOv8\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"common-anti-patterns",children:"Common Anti-Patterns"}),"\n",(0,s.jsx)(n.h3,{id:"anti-pattern-1-not-preprocessing-frames-before-detection",children:"Anti-Pattern 1: Not Preprocessing Frames Before Detection"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Novice thinking"}),': "Just run detection on raw video frames"']}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),": Poor detection accuracy, wasted GPU cycles."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Wrong approach"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# \u274c No preprocessing - poor results\nimport cv2\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolov8n.pt')\nvideo = cv2.VideoCapture('drone_footage.mp4')\n\nwhile True:\n    ret, frame = video.read()\n    if not ret:\n        break\n\n    # Raw frame detection - no normalization, no resizing\n    results = model(frame)\n    # Poor accuracy, slow inference\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Why wrong"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Video resolution too high (4K = 8.3 megapixels per frame)"}),"\n",(0,s.jsx)(n.li,{children:"No normalization (pixel values 0-255 instead of 0-1)"}),"\n",(0,s.jsx)(n.li,{children:"Aspect ratio not maintained"}),"\n",(0,s.jsx)(n.li,{children:"GPU memory overflow on high-res frames"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Correct approach"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# \u2705 Proper preprocessing pipeline\nimport cv2\nimport numpy as np\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolov8n.pt')\nvideo = cv2.VideoCapture('drone_footage.mp4')\n\n# Model expects 640x640 input\nTARGET_SIZE = 640\n\ndef preprocess_frame(frame):\n    # Resize while maintaining aspect ratio\n    h, w = frame.shape[:2]\n    scale = TARGET_SIZE / max(h, w)\n    new_w, new_h = int(w * scale), int(h * scale)\n\n    resized = cv2.resize(frame, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n\n    # Pad to square\n    pad_w = (TARGET_SIZE - new_w) // 2\n    pad_h = (TARGET_SIZE - new_h) // 2\n\n    padded = cv2.copyMakeBorder(\n        resized,\n        pad_h, TARGET_SIZE - new_h - pad_h,\n        pad_w, TARGET_SIZE - new_w - pad_w,\n        cv2.BORDER_CONSTANT,\n        value=(114, 114, 114)  # Gray padding\n    )\n\n    # Normalize to 0-1 (if model expects it)\n    # normalized = padded.astype(np.float32) / 255.0\n\n    return padded, scale\n\nwhile True:\n    ret, frame = video.read()\n    if not ret:\n        break\n\n    preprocessed, scale = preprocess_frame(frame)\n    results = model(preprocessed)\n\n    # Scale bounding boxes back to original coordinates\n    for box in results[0].boxes:\n        x1, y1, x2, y2 = box.xyxy[0]\n        x1, y1, x2, y2 = x1/scale, y1/scale, x2/scale, y2/scale\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Performance comparison"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Raw 4K frames: 5 FPS, 72% mAP"}),"\n",(0,s.jsx)(n.li,{children:"Preprocessed 640x640: 45 FPS, 89% mAP"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Timeline context"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"2015: Manual preprocessing required"}),"\n",(0,s.jsx)(n.li,{children:"2020: YOLOv5 added auto-resize"}),"\n",(0,s.jsx)(n.li,{children:"2023: YOLOv8 has smart preprocessing but explicit control is better"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"anti-pattern-2-processing-every-frame-in-video",children:"Anti-Pattern 2: Processing Every Frame in Video"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Novice thinking"}),': "Run detection on every single frame"']}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),": 99% of frames are redundant, wasting compute."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Wrong approach"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# \u274c Process every frame (30 FPS video = 1800 frames/min)\nimport cv2\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolov8n.pt')\nvideo = cv2.VideoCapture('drone_footage.mp4')\n\ndetections = []\n\nwhile True:\n    ret, frame = video.read()\n    if not ret:\n        break\n\n    # Run detection on EVERY frame\n    results = model(frame)\n    detections.append(results)\n\n# 10-minute video = 18,000 inferences (15 minutes on GPU)\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Why wrong"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Adjacent frames are nearly identical"}),"\n",(0,s.jsx)(n.li,{children:"Wasting 95% of compute on duplicate work"}),"\n",(0,s.jsx)(n.li,{children:"Slow processing time"}),"\n",(0,s.jsx)(n.li,{children:"Massive storage for results"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Correct approach 1"}),": Frame sampling"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# \u2705 Sample every Nth frame\nimport cv2\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolov8n.pt')\nvideo = cv2.VideoCapture('drone_footage.mp4')\n\nSAMPLE_RATE = 30  # Process 1 frame per second (if 30 FPS video)\n\nframe_count = 0\ndetections = []\n\nwhile True:\n    ret, frame = video.read()\n    if not ret:\n        break\n\n    frame_count += 1\n\n    # Only process every 30th frame\n    if frame_count % SAMPLE_RATE == 0:\n        results = model(frame)\n        detections.append({\n            'frame': frame_count,\n            'timestamp': frame_count / 30.0,\n            'results': results\n        })\n\n# 10-minute video = 600 inferences (30 seconds on GPU)\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Correct approach 2"}),": Adaptive sampling with scene change detection"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# \u2705 Only process when scene changes significantly\nimport cv2\nimport numpy as np\nfrom ultralytics import YOLO\n\nmodel = YOLO(\'yolov8n.pt\')\nvideo = cv2.VideoCapture(\'drone_footage.mp4\')\n\ndef scene_changed(prev_frame, curr_frame, threshold=0.3):\n    """Detect scene change using histogram comparison"""\n    if prev_frame is None:\n        return True\n\n    # Convert to grayscale\n    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n    curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n\n    # Calculate histograms\n    prev_hist = cv2.calcHist([prev_gray], [0], None, [256], [0, 256])\n    curr_hist = cv2.calcHist([curr_gray], [0], None, [256], [0, 256])\n\n    # Compare histograms\n    correlation = cv2.compareHist(prev_hist, curr_hist, cv2.HISTCMP_CORREL)\n\n    return correlation < (1 - threshold)\n\nprev_frame = None\ndetections = []\n\nwhile True:\n    ret, frame = video.read()\n    if not ret:\n        break\n\n    # Only run detection if scene changed\n    if scene_changed(prev_frame, frame):\n        results = model(frame)\n        detections.append(results)\n\n    prev_frame = frame.copy()\n\n# Adapts to video content - static shots skip frames, action scenes process more\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Savings"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Every frame: 18,000 inferences"}),"\n",(0,s.jsx)(n.li,{children:"Sample 1 FPS: 600 inferences (97% reduction)"}),"\n",(0,s.jsx)(n.li,{children:"Adaptive: ~1,200 inferences (93% reduction)"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"anti-pattern-3-not-using-batch-inference",children:"Anti-Pattern 3: Not Using Batch Inference"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Novice thinking"}),': "Process one image at a time"']}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),": GPU sits idle 80% of the time waiting for data."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Wrong approach"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# \u274c Sequential processing - GPU underutilized\nimport cv2\nfrom ultralytics import YOLO\nimport time\n\nmodel = YOLO('yolov8n.pt')\n\n# 100 images to process\nimage_paths = [f'frame_{i:04d}.jpg' for i in range(100)]\n\nstart = time.time()\n\nfor path in image_paths:\n    frame = cv2.imread(path)\n    results = model(frame)  # Process one at a time\n    # GPU utilization: ~20%\n\nelapsed = time.time() - start\nprint(f\"Processed {len(image_paths)} images in {elapsed:.2f}s\")\n# Output: 45 seconds\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Why wrong"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"GPU has to wait for CPU to load each image"}),"\n",(0,s.jsx)(n.li,{children:"No parallelization"}),"\n",(0,s.jsx)(n.li,{children:"GPU utilization ~20%"}),"\n",(0,s.jsx)(n.li,{children:"Slow throughput"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Correct approach"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# \u2705 Batch inference - GPU fully utilized\nimport cv2\nfrom ultralytics import YOLO\nimport time\n\nmodel = YOLO('yolov8n.pt')\n\nimage_paths = [f'frame_{i:04d}.jpg' for i in range(100)]\n\nBATCH_SIZE = 16  # Process 16 images at once\n\nstart = time.time()\n\nfor i in range(0, len(image_paths), BATCH_SIZE):\n    batch_paths = image_paths[i:i+BATCH_SIZE]\n\n    # Load batch\n    frames = [cv2.imread(path) for path in batch_paths]\n\n    # Batch inference (single GPU call)\n    results = model(frames)  # Pass list of images\n    # GPU utilization: ~85%\n\nelapsed = time.time() - start\nprint(f\"Processed {len(image_paths)} images in {elapsed:.2f}s\")\n# Output: 8 seconds (5.6x faster!)\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Performance comparison"}),":"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Method"}),(0,s.jsx)(n.th,{children:"Time (100 images)"}),(0,s.jsx)(n.th,{children:"GPU Util"}),(0,s.jsx)(n.th,{children:"Throughput"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Sequential"}),(0,s.jsx)(n.td,{children:"45s"}),(0,s.jsx)(n.td,{children:"20%"}),(0,s.jsx)(n.td,{children:"2.2 img/s"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Batch (16)"}),(0,s.jsx)(n.td,{children:"8s"}),(0,s.jsx)(n.td,{children:"85%"}),(0,s.jsx)(n.td,{children:"12.5 img/s"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Batch (32)"}),(0,s.jsx)(n.td,{children:"6s"}),(0,s.jsx)(n.td,{children:"92%"}),(0,s.jsx)(n.td,{children:"16.7 img/s"})]})]})]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Batch size tuning"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Find optimal batch size for your GPU\nimport torch\n\ndef find_optimal_batch_size(model, image_size=(640, 640)):\n    for batch_size in [1, 2, 4, 8, 16, 32, 64]:\n        try:\n            dummy_input = torch.randn(batch_size, 3, *image_size).cuda()\n\n            start = time.time()\n            with torch.no_grad():\n                _ = model(dummy_input)\n            elapsed = time.time() - start\n\n            throughput = batch_size / elapsed\n            print(f"Batch {batch_size}: {throughput:.1f} img/s")\n        except RuntimeError as e:\n            print(f"Batch {batch_size}: OOM (out of memory)")\n            break\n\n# Find optimal batch size before production\nfind_optimal_batch_size(model)\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"anti-pattern-4-ignoring-non-maximum-suppression-nms-tuning",children:"Anti-Pattern 4: Ignoring Non-Maximum Suppression (NMS) Tuning"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),": Duplicate detections, missed objects, slow post-processing."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Wrong approach"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# \u274c Use default NMS settings for everything\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolov8n.pt')\n\n# Default settings (iou_threshold=0.45, conf_threshold=0.25)\nresults = model('crowded_scene.jpg')\n\n# Result: 50 bounding boxes, 30 are duplicates!\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Why wrong"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Default IoU=0.45 is too permissive for dense objects"}),"\n",(0,s.jsx)(n.li,{children:"Default conf=0.25 includes low-quality detections"}),"\n",(0,s.jsx)(n.li,{children:"No adaptation to use case"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Correct approach"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# \u2705 Tune NMS for your use case\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolov8n.pt')\n\n# Sparse objects (dolphins in ocean)\nsparse_results = model(\n    'ocean_footage.jpg',\n    iou=0.5,    # Higher IoU = allow closer boxes\n    conf=0.4    # Higher confidence = fewer false positives\n)\n\n# Dense objects (crowd, flock of birds)\ndense_results = model(\n    'crowded_scene.jpg',\n    iou=0.3,    # Lower IoU = suppress more duplicates\n    conf=0.5    # Higher confidence = filter noise\n)\n\n# High precision needed (legal evidence)\nprecise_results = model(\n    'evidence.jpg',\n    iou=0.5,\n    conf=0.7,   # Very high confidence\n    max_det=50  # Limit max detections\n)\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"NMS parameter guide"}),":"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Use Case"}),(0,s.jsx)(n.th,{children:"IoU"}),(0,s.jsx)(n.th,{children:"Conf"}),(0,s.jsx)(n.th,{children:"Max Det"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Sparse objects (wildlife)"}),(0,s.jsx)(n.td,{children:"0.5"}),(0,s.jsx)(n.td,{children:"0.4"}),(0,s.jsx)(n.td,{children:"100"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Dense objects (crowd)"}),(0,s.jsx)(n.td,{children:"0.3"}),(0,s.jsx)(n.td,{children:"0.5"}),(0,s.jsx)(n.td,{children:"300"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"High precision (evidence)"}),(0,s.jsx)(n.td,{children:"0.5"}),(0,s.jsx)(n.td,{children:"0.7"}),(0,s.jsx)(n.td,{children:"50"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Real-time (speed priority)"}),(0,s.jsx)(n.td,{children:"0.45"}),(0,s.jsx)(n.td,{children:"0.3"}),(0,s.jsx)(n.td,{children:"100"})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"anti-pattern-5-no-tracking-between-frames",children:"Anti-Pattern 5: No Tracking Between Frames"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Novice thinking"}),': "Run detection on each frame independently"']}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),": Can't count unique objects, track movement, or build trajectories."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Wrong approach"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# \u274c Independent frame detection - no object identity\nfrom ultralytics import YOLO\nimport cv2\n\nmodel = YOLO('yolov8n.pt')\nvideo = cv2.VideoCapture('dolphins.mp4')\n\ndetections = []\n\nwhile True:\n    ret, frame = video.read()\n    if not ret:\n        break\n\n    results = model(frame)\n    detections.append(results)\n\n# Result: Can't tell if frame 10 dolphin is same as frame 20 dolphin\n# Can't count unique dolphins\n# Can't track trajectories\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Why wrong"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"No object identity across frames"}),"\n",(0,s.jsx)(n.li,{children:"Can't count unique objects"}),"\n",(0,s.jsx)(n.li,{children:"Can't analyze movement patterns"}),"\n",(0,s.jsx)(n.li,{children:"Can't build trajectories"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Correct approach"}),": Use tracking (ByteTrack)"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# \u2705 Multi-object tracking with ByteTrack\nfrom ultralytics import YOLO\nimport cv2\n\n# YOLO with tracking\nmodel = YOLO('yolov8n.pt')\nvideo = cv2.VideoCapture('dolphins.mp4')\n\n# Track objects across frames\ntracks = {}\n\nwhile True:\n    ret, frame = video.read()\n    if not ret:\n        break\n\n    # Run detection + tracking\n    results = model.track(\n        frame,\n        persist=True,     # Maintain IDs across frames\n        tracker='bytetrack.yaml'  # ByteTrack algorithm\n    )\n\n    # Each detection now has persistent ID\n    for box in results[0].boxes:\n        track_id = int(box.id[0])  # Unique ID across frames\n        x1, y1, x2, y2 = box.xyxy[0]\n\n        # Store trajectory\n        if track_id not in tracks:\n            tracks[track_id] = []\n\n        tracks[track_id].append({\n            'frame': len(tracks[track_id]),\n            'bbox': (x1, y1, x2, y2),\n            'conf': box.conf[0]\n        })\n\n# Now we can analyze:\nprint(f\"Unique dolphins detected: {len(tracks)}\")\n\n# Trajectory analysis\nfor track_id, trajectory in tracks.items():\n    if len(trajectory) > 30:  # Only long tracks\n        print(f\"Dolphin {track_id} appeared in {len(trajectory)} frames\")\n        # Calculate movement, speed, etc.\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Tracking benefits"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Count unique objects (not just detections per frame)"}),"\n",(0,s.jsx)(n.li,{children:"Build trajectories and movement patterns"}),"\n",(0,s.jsx)(n.li,{children:"Analyze behavior over time"}),"\n",(0,s.jsx)(n.li,{children:"Filter out brief false positives"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Tracking algorithms"}),":"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Algorithm"}),(0,s.jsx)(n.th,{children:"Speed"}),(0,s.jsx)(n.th,{children:"Robustness"}),(0,s.jsx)(n.th,{children:"Occlusion Handling"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"ByteTrack"}),(0,s.jsx)(n.td,{children:"Fast"}),(0,s.jsx)(n.td,{children:"Good"}),(0,s.jsx)(n.td,{children:"Excellent"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"SORT"}),(0,s.jsx)(n.td,{children:"Very Fast"}),(0,s.jsx)(n.td,{children:"Fair"}),(0,s.jsx)(n.td,{children:"Fair"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"DeepSORT"}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"Excellent"}),(0,s.jsx)(n.td,{children:"Good"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"BotSORT"}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"Excellent"}),(0,s.jsx)(n.td,{children:"Excellent"})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"production-checklist",children:"Production Checklist"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u25a1 Preprocess frames (resize, pad, normalize)\n\u25a1 Sample frames intelligently (1 FPS or scene change detection)\n\u25a1 Use batch inference (16-32 images per batch)\n\u25a1 Tune NMS thresholds for your use case\n\u25a1 Implement tracking if analyzing video\n\u25a1 Log inference time and GPU utilization\n\u25a1 Handle edge cases (empty frames, corrupted video)\n\u25a1 Save results in structured format (JSON, CSV)\n\u25a1 Visualize detections for debugging\n\u25a1 Benchmark on representative data\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"when-to-use-vs-avoid",children:"When to Use vs Avoid"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Scenario"}),(0,s.jsx)(n.th,{children:"Appropriate?"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Analyze drone footage for archaeology"}),(0,s.jsx)(n.td,{children:"\u2705 Yes - custom object detection"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Track wildlife in video"}),(0,s.jsx)(n.td,{children:"\u2705 Yes - detection + tracking"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Count people in crowd"}),(0,s.jsx)(n.td,{children:"\u2705 Yes - dense object detection"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Real-time security camera"}),(0,s.jsx)(n.td,{children:"\u2705 Yes - YOLOv8 real-time"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Filter vacation photos"}),(0,s.jsx)(n.td,{children:"\u274c No - use photo management apps"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Face recognition login"}),(0,s.jsx)(n.td,{children:"\u274c No - use AWS Rekognition API"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Read license plates"}),(0,s.jsx)(n.td,{children:"\u274c No - use specialized OCR"})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"references-1",children:"References"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"/references/yolo-guide.md"})," - YOLOv8 setup, training, inference patterns"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"/references/video-processing.md"})," - Frame extraction, scene detection, optimization"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"/references/tracking-algorithms.md"})," - ByteTrack, SORT, DeepSORT comparison"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"scripts",children:"Scripts"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"scripts/video_analyzer.py"})," - Extract frames, run detection, generate timeline"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"scripts/model_trainer.py"})," - Fine-tune YOLO on custom dataset, export weights"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"This skill guides"}),": Computer vision | Object detection | Video analysis | YOLO | Tracking | Drone footage | Wildlife monitoring"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(a,{...e})}):a(e)}}}]);