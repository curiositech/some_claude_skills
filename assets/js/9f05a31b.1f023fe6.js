"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[91574],{19200:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>f,frontMatter:()=>i,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"skills/drone_cv_expert/references/object-detection-tracking","title":"Object Detection & Tracking Reference","description":"Real-Time Aerial Object Detection","source":"@site/docs/skills/drone_cv_expert/references/object-detection-tracking.md","sourceDirName":"skills/drone_cv_expert/references","slug":"/skills/drone_cv_expert/references/object-detection-tracking","permalink":"/docs/skills/drone_cv_expert/references/object-detection-tracking","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Object Detection & Tracking Reference","sidebar_label":"Object Detection & Tracking...","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Navigation Algorithms Refer...","permalink":"/docs/skills/drone_cv_expert/references/navigation-algorithms"},"next":{"title":"Sensor Fusion & State Estim...","permalink":"/docs/skills/drone_cv_expert/references/sensor-fusion-ekf"}}');var r=t(74848),o=t(28453);const i={title:"Object Detection & Tracking Reference",sidebar_label:"Object Detection & Tracking...",sidebar_position:2},a="Object Detection & Tracking Reference",l={},c=[{value:"Real-Time Aerial Object Detection",id:"real-time-aerial-object-detection",level:2},{value:"Multi-Object Tracking",id:"multi-object-tracking",level:2},{value:"ByteTrack Implementation",id:"bytetrack-implementation",level:3},{value:"Optical Flow for Motion Estimation",id:"optical-flow-for-motion-estimation",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",pre:"pre",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"object-detection--tracking-reference",children:"Object Detection & Tracking Reference"})}),"\n",(0,r.jsx)(n.h2,{id:"real-time-aerial-object-detection",children:"Real-Time Aerial Object Detection"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import torch\nimport numpy as np\nfrom ultralytics import YOLO\nfrom typing import List, Dict, Tuple, Optional\n\nclass AerialObjectDetector:\n    """\n    YOLOv8-based object detection optimized for drone platforms.\n    Handles altitude-dependent scaling and aerial-specific classes.\n    """\n    def __init__(self, model_path: str = \'yolov8n.pt\', device: str = \'cuda\'):\n        self.model = YOLO(model_path)\n        self.model.to(device)\n        self.device = device\n\n        # Confidence thresholds by altitude (lower when higher)\n        self.altitude_conf_map = {\n            0: 0.5,    # Ground level\n            50: 0.4,   # 50m\n            100: 0.35, # 100m\n            200: 0.3   # 200m+\n        }\n\n    def detect(self, frame: np.ndarray, altitude: float = 0,\n               classes: Optional[List[int]] = None) -> List[Dict]:\n        """\n        Detect objects in frame with altitude-adaptive confidence.\n\n        Args:\n            frame: BGR image from drone camera\n            altitude: Current drone altitude in meters\n            classes: Filter for specific class IDs (None for all)\n\n        Returns:\n            List of detections with bbox, class, confidence\n        """\n        # Get altitude-appropriate confidence threshold\n        conf = self._get_confidence_threshold(altitude)\n\n        # Run detection\n        results = self.model(\n            frame,\n            conf=conf,\n            classes=classes,\n            verbose=False\n        )[0]\n\n        # Parse results\n        detections = []\n        for box in results.boxes:\n            det = {\n                \'bbox\': box.xyxy[0].cpu().numpy().tolist(),\n                \'class_id\': int(box.cls[0].item()),\n                \'class_name\': results.names[int(box.cls[0].item())],\n                \'confidence\': float(box.conf[0].item()),\n                \'center\': self._get_center(box.xyxy[0].cpu().numpy())\n            }\n            detections.append(det)\n\n        return detections\n\n    def _get_confidence_threshold(self, altitude: float) -> float:\n        """Interpolate confidence threshold based on altitude"""\n        sorted_alts = sorted(self.altitude_conf_map.keys())\n        for i, alt in enumerate(sorted_alts[:-1]):\n            if altitude < sorted_alts[i + 1]:\n                # Linear interpolation\n                t = (altitude - alt) / (sorted_alts[i + 1] - alt)\n                return self.altitude_conf_map[alt] * (1 - t) + self.altitude_conf_map[sorted_alts[i + 1]] * t\n        return self.altitude_conf_map[sorted_alts[-1]]\n\n    def _get_center(self, bbox: np.ndarray) -> Tuple[float, float]:\n        return ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)\n\n\nclass TensorRTDetector:\n    """\n    TensorRT-optimized detector for edge deployment.\n    3-5x faster than PyTorch inference on Jetson.\n    """\n    def __init__(self, engine_path: str, input_size: Tuple[int, int] = (640, 640)):\n        import tensorrt as trt\n\n        self.input_size = input_size\n        self.logger = trt.Logger(trt.Logger.WARNING)\n\n        # Load engine\n        with open(engine_path, \'rb\') as f:\n            self.engine = trt.Runtime(self.logger).deserialize_cuda_engine(f.read())\n\n        self.context = self.engine.create_execution_context()\n\n        # Allocate buffers\n        self._allocate_buffers()\n\n    def _allocate_buffers(self):\n        import pycuda.driver as cuda\n\n        self.inputs = []\n        self.outputs = []\n        self.bindings = []\n\n        for binding in self.engine:\n            size = trt.volume(self.engine.get_binding_shape(binding))\n            dtype = trt.nptype(self.engine.get_binding_dtype(binding))\n\n            # Allocate host and device buffers\n            host_mem = cuda.pagelocked_empty(size, dtype)\n            device_mem = cuda.mem_alloc(host_mem.nbytes)\n\n            self.bindings.append(int(device_mem))\n\n            if self.engine.binding_is_input(binding):\n                self.inputs.append({\'host\': host_mem, \'device\': device_mem})\n            else:\n                self.outputs.append({\'host\': host_mem, \'device\': device_mem})\n\n    def detect(self, frame: np.ndarray) -> List[Dict]:\n        """Run TensorRT inference"""\n        import pycuda.driver as cuda\n\n        # Preprocess\n        input_tensor = self._preprocess(frame)\n        np.copyto(self.inputs[0][\'host\'], input_tensor.ravel())\n\n        # Transfer to GPU\n        cuda.memcpy_htod(self.inputs[0][\'device\'], self.inputs[0][\'host\'])\n\n        # Execute\n        self.context.execute_v2(bindings=self.bindings)\n\n        # Transfer results back\n        cuda.memcpy_dtoh(self.outputs[0][\'host\'], self.outputs[0][\'device\'])\n\n        # Postprocess\n        return self._postprocess(self.outputs[0][\'host\'], frame.shape)\n\n    def _preprocess(self, frame: np.ndarray) -> np.ndarray:\n        """Preprocess frame for inference"""\n        import cv2\n        img = cv2.resize(frame, self.input_size)\n        img = img.astype(np.float32) / 255.0\n        img = img.transpose(2, 0, 1)  # HWC to CHW\n        return np.expand_dims(img, axis=0)\n\n    def _postprocess(self, output: np.ndarray, orig_shape: Tuple) -> List[Dict]:\n        """Parse YOLO output format"""\n        # Implementation depends on YOLO version export format\n        detections = []\n        # ... parse output tensor ...\n        return detections\n'})}),"\n",(0,r.jsx)(n.h2,{id:"multi-object-tracking",children:"Multi-Object Tracking"}),"\n",(0,r.jsx)(n.h3,{id:"bytetrack-implementation",children:"ByteTrack Implementation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom typing import List, Dict, Tuple\nfrom scipy.optimize import linear_sum_assignment\n\nclass KalmanBoxTracker:\n    """Kalman filter for tracking bounding box state"""\n    count = 0\n\n    def __init__(self, bbox: np.ndarray):\n        from filterpy.kalman import KalmanFilter\n\n        self.kf = KalmanFilter(dim_x=7, dim_z=4)\n\n        # State: [x, y, s, r, dx, dy, ds]\n        # x, y = center, s = area, r = aspect ratio\n        self.kf.F = np.array([\n            [1, 0, 0, 0, 1, 0, 0],\n            [0, 1, 0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0, 0, 1],\n            [0, 0, 0, 1, 0, 0, 0],\n            [0, 0, 0, 0, 1, 0, 0],\n            [0, 0, 0, 0, 0, 1, 0],\n            [0, 0, 0, 0, 0, 0, 1]\n        ])\n\n        self.kf.H = np.array([\n            [1, 0, 0, 0, 0, 0, 0],\n            [0, 1, 0, 0, 0, 0, 0],\n            [0, 0, 1, 0, 0, 0, 0],\n            [0, 0, 0, 1, 0, 0, 0]\n        ])\n\n        self.kf.R[2:, 2:] *= 10.0\n        self.kf.P[4:, 4:] *= 1000.0\n        self.kf.P *= 10.0\n        self.kf.Q[-1, -1] *= 0.01\n        self.kf.Q[4:, 4:] *= 0.01\n\n        self.kf.x[:4] = self._bbox_to_z(bbox)\n\n        self.time_since_update = 0\n        self.id = KalmanBoxTracker.count\n        KalmanBoxTracker.count += 1\n        self.history = []\n        self.hits = 0\n        self.hit_streak = 0\n        self.age = 0\n\n    def update(self, bbox: np.ndarray):\n        """Update with matched detection"""\n        self.time_since_update = 0\n        self.history = []\n        self.hits += 1\n        self.hit_streak += 1\n        self.kf.update(self._bbox_to_z(bbox))\n\n    def predict(self) -> np.ndarray:\n        """Predict next state"""\n        if (self.kf.x[6] + self.kf.x[2]) <= 0:\n            self.kf.x[6] *= 0.0\n\n        self.kf.predict()\n        self.age += 1\n\n        if self.time_since_update > 0:\n            self.hit_streak = 0\n        self.time_since_update += 1\n\n        self.history.append(self._z_to_bbox(self.kf.x))\n        return self.history[-1]\n\n    def get_state(self) -> np.ndarray:\n        return self._z_to_bbox(self.kf.x)\n\n    def _bbox_to_z(self, bbox: np.ndarray) -> np.ndarray:\n        """Convert [x1, y1, x2, y2] to [cx, cy, s, r]"""\n        w = bbox[2] - bbox[0]\n        h = bbox[3] - bbox[1]\n        x = bbox[0] + w / 2\n        y = bbox[1] + h / 2\n        s = w * h\n        r = w / float(h) if h > 0 else 1\n        return np.array([x, y, s, r]).reshape((4, 1))\n\n    def _z_to_bbox(self, z: np.ndarray) -> np.ndarray:\n        """Convert [cx, cy, s, r] to [x1, y1, x2, y2]"""\n        w = np.sqrt(z[2] * z[3])\n        h = z[2] / w if w > 0 else z[2]\n        return np.array([\n            z[0] - w / 2,\n            z[1] - h / 2,\n            z[0] + w / 2,\n            z[1] + h / 2\n        ]).flatten()\n\n\nclass ByteTracker:\n    """\n    ByteTrack multi-object tracker.\n    Associates detections across frames using IoU and appearance.\n    """\n    def __init__(self, max_age: int = 30, min_hits: int = 3, iou_threshold: float = 0.3):\n        self.max_age = max_age\n        self.min_hits = min_hits\n        self.iou_threshold = iou_threshold\n        self.trackers: List[KalmanBoxTracker] = []\n        self.frame_count = 0\n\n    def update(self, detections: List[Dict]) -> List[Dict]:\n        """\n        Update tracks with new detections.\n\n        Args:\n            detections: List of detections with \'bbox\' and \'confidence\'\n\n        Returns:\n            List of tracks with track_id, bbox, and status\n        """\n        self.frame_count += 1\n\n        # Get predicted locations from existing trackers\n        trks = np.zeros((len(self.trackers), 5))\n        to_del = []\n        for t, trk in enumerate(self.trackers):\n            pos = trk.predict()\n            trks[t, :] = [pos[0], pos[1], pos[2], pos[3], 0]\n            if np.any(np.isnan(pos)):\n                to_del.append(t)\n\n        trks = np.ma.compress_rows(np.ma.masked_invalid(trks))\n        for t in reversed(to_del):\n            self.trackers.pop(t)\n\n        # Split detections by confidence\n        dets = np.array([d[\'bbox\'] for d in detections])\n        confs = np.array([d[\'confidence\'] for d in detections])\n\n        high_conf_mask = confs >= 0.5\n        low_conf_mask = ~high_conf_mask\n\n        high_dets = dets[high_conf_mask] if len(dets) > 0 else np.empty((0, 4))\n        low_dets = dets[low_conf_mask] if len(dets) > 0 else np.empty((0, 4))\n\n        # First association with high confidence detections\n        matched, unmatched_dets, unmatched_trks = self._associate(\n            high_dets, trks, self.iou_threshold\n        )\n\n        # Update matched trackers\n        for t, d in matched:\n            self.trackers[t].update(high_dets[d])\n\n        # Second association with low confidence detections\n        if len(low_dets) > 0 and len(unmatched_trks) > 0:\n            left_trks = trks[unmatched_trks]\n            matched2, _, _ = self._associate(low_dets, left_trks, 0.5)\n\n            for t_idx, d in matched2:\n                t = unmatched_trks[t_idx]\n                self.trackers[t].update(low_dets[d])\n                unmatched_trks = unmatched_trks[unmatched_trks != t]\n\n        # Create new trackers for unmatched high-confidence detections\n        for i in unmatched_dets:\n            trk = KalmanBoxTracker(high_dets[i])\n            self.trackers.append(trk)\n\n        # Return active tracks\n        ret = []\n        for trk in reversed(self.trackers):\n            d = trk.get_state()\n            if (trk.time_since_update < 1) and \\\n               (trk.hit_streak >= self.min_hits or self.frame_count <= self.min_hits):\n                ret.append({\n                    \'track_id\': trk.id,\n                    \'bbox\': d.tolist(),\n                    \'status\': \'tracked\'\n                })\n\n            # Remove dead tracks\n            if trk.time_since_update > self.max_age:\n                self.trackers.remove(trk)\n\n        return ret\n\n    def _associate(self, detections: np.ndarray, trackers: np.ndarray,\n                   iou_threshold: float) -> Tuple[List, np.ndarray, np.ndarray]:\n        """Associate detections with trackers using Hungarian algorithm"""\n        if len(trackers) == 0:\n            return [], np.arange(len(detections)), np.empty((0,), dtype=int)\n\n        if len(detections) == 0:\n            return [], np.empty((0,), dtype=int), np.arange(len(trackers))\n\n        # Compute IoU matrix\n        iou_matrix = self._iou_batch(detections, trackers[:, :4])\n        cost_matrix = 1 - iou_matrix\n\n        # Hungarian algorithm\n        row_indices, col_indices = linear_sum_assignment(cost_matrix)\n\n        # Filter low IoU matches\n        matched = []\n        unmatched_dets = list(range(len(detections)))\n        unmatched_trks = list(range(len(trackers)))\n\n        for r, c in zip(row_indices, col_indices):\n            if iou_matrix[r, c] >= iou_threshold:\n                matched.append((c, r))  # (tracker_idx, detection_idx)\n                unmatched_dets.remove(r)\n                unmatched_trks.remove(c)\n\n        return matched, np.array(unmatched_dets), np.array(unmatched_trks)\n\n    def _iou_batch(self, bb_test: np.ndarray, bb_gt: np.ndarray) -> np.ndarray:\n        """Compute IoU between all pairs of boxes"""\n        bb_gt = np.expand_dims(bb_gt, 0)\n        bb_test = np.expand_dims(bb_test, 1)\n\n        xx1 = np.maximum(bb_test[..., 0], bb_gt[..., 0])\n        yy1 = np.maximum(bb_test[..., 1], bb_gt[..., 1])\n        xx2 = np.minimum(bb_test[..., 2], bb_gt[..., 2])\n        yy2 = np.minimum(bb_test[..., 3], bb_gt[..., 3])\n        w = np.maximum(0., xx2 - xx1)\n        h = np.maximum(0., yy2 - yy1)\n        wh = w * h\n        o = wh / ((bb_test[..., 2] - bb_test[..., 0]) * (bb_test[..., 3] - bb_test[..., 1])\n                  + (bb_gt[..., 2] - bb_gt[..., 0]) * (bb_gt[..., 3] - bb_gt[..., 1]) - wh)\n        return o\n\n\nclass TargetTracker:\n    """\n    Single-target tracker with prediction for fast-moving objects.\n    Uses Kalman filter + optical flow for robust tracking.\n    """\n    def __init__(self):\n        self.kalman = KalmanBoxTracker(np.array([0, 0, 100, 100]))\n        self.last_bbox = None\n        self.lost_frames = 0\n        self.max_lost = 30\n\n    def update(self, detection: Optional[Dict] = None) -> Dict:\n        """Update tracker with detection (or None if lost)"""\n        predicted = self.kalman.predict()\n\n        if detection is not None:\n            self.kalman.update(np.array(detection[\'bbox\']))\n            self.last_bbox = detection[\'bbox\']\n            self.lost_frames = 0\n            status = \'tracking\'\n        else:\n            self.lost_frames += 1\n            self.last_bbox = predicted.tolist()\n            status = \'predicting\' if self.lost_frames < self.max_lost else \'lost\'\n\n        return {\n            \'bbox\': self.last_bbox,\n            \'predicted_bbox\': predicted.tolist(),\n            \'status\': status,\n            \'lost_frames\': self.lost_frames\n        }\n'})}),"\n",(0,r.jsx)(n.h2,{id:"optical-flow-for-motion-estimation",children:"Optical Flow for Motion Estimation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import cv2\nimport numpy as np\nfrom typing import Tuple, Optional\n\nclass OpticalFlowTracker:\n    """\n    Optical flow for visual motion estimation.\n    Useful for velocity estimation and feature tracking.\n    """\n    def __init__(self, feature_params: Optional[dict] = None, lk_params: Optional[dict] = None):\n        self.feature_params = feature_params or {\n            \'maxCorners\': 100,\n            \'qualityLevel\': 0.3,\n            \'minDistance\': 7,\n            \'blockSize\': 7\n        }\n\n        self.lk_params = lk_params or {\n            \'winSize\': (15, 15),\n            \'maxLevel\': 2,\n            \'criteria\': (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)\n        }\n\n        self.prev_gray = None\n        self.prev_points = None\n\n    def compute_flow(self, frame: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n        """\n        Compute sparse optical flow between frames.\n\n        Returns:\n            flow_vectors: Motion vectors for each tracked point\n            good_points: Points successfully tracked\n        """\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) if len(frame.shape) == 3 else frame\n\n        if self.prev_gray is None:\n            self.prev_gray = gray\n            self.prev_points = cv2.goodFeaturesToTrack(gray, mask=None, **self.feature_params)\n            return np.array([]), np.array([])\n\n        if self.prev_points is None or len(self.prev_points) < 10:\n            self.prev_points = cv2.goodFeaturesToTrack(gray, mask=None, **self.feature_params)\n\n        # Calculate optical flow\n        next_points, status, _ = cv2.calcOpticalFlowPyrLK(\n            self.prev_gray, gray, self.prev_points, None, **self.lk_params\n        )\n\n        # Select good points\n        if next_points is not None:\n            good_old = self.prev_points[status == 1]\n            good_new = next_points[status == 1]\n\n            # Compute flow vectors\n            flow_vectors = good_new - good_old\n\n            # Update for next frame\n            self.prev_gray = gray.copy()\n            self.prev_points = good_new.reshape(-1, 1, 2)\n\n            return flow_vectors, good_new\n\n        return np.array([]), np.array([])\n\n    def estimate_motion(self, frame: np.ndarray) -> Tuple[float, float, float]:\n        """\n        Estimate camera motion from optical flow.\n\n        Returns:\n            dx, dy: Translation in pixels\n            rotation: Rotation in radians\n        """\n        flow_vectors, points = self.compute_flow(frame)\n\n        if len(flow_vectors) < 4:\n            return 0.0, 0.0, 0.0\n\n        # Average translation\n        dx = np.median(flow_vectors[:, 0])\n        dy = np.median(flow_vectors[:, 1])\n\n        # Estimate rotation from flow field\n        # Points moving clockwise = positive rotation\n        center = np.mean(points, axis=0)\n        relative_pos = points - center\n\n        # Cross product for rotation direction\n        rotation_components = []\n        for i, (pos, flow) in enumerate(zip(relative_pos, flow_vectors)):\n            cross = pos[0] * flow[1] - pos[1] * flow[0]\n            dist = np.linalg.norm(pos)\n            if dist > 10:  # Avoid center points\n                rotation_components.append(cross / dist)\n\n        rotation = np.median(rotation_components) if rotation_components else 0.0\n\n        return float(dx), float(dy), float(rotation)\n'})})]})}function f(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>a});var s=t(96540);const r={},o=s.createContext(r);function i(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);