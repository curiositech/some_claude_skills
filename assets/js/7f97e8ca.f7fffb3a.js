"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[45839],{28453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var s=i(96540);const t={},r=s.createContext(t);function a(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(r.Provider,{value:n},e.children)}},99397:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>g,frontMatter:()=>a,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"skills/collage_layout_expert/references/edge-assembly","title":"Edge-Based Assembly Strategy","description":"Core Concept: \\"Edge-First\\" Composition","source":"@site/docs/skills/collage_layout_expert/references/edge-assembly.md","sourceDirName":"skills/collage_layout_expert/references","slug":"/skills/collage_layout_expert/references/edge-assembly","permalink":"/docs/skills/collage_layout_expert/references/edge-assembly","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"title":"Edge-Based Assembly Strategy","sidebar_label":"Edge-Based Assembly Strategy","sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Collage Types & Techniques","permalink":"/docs/skills/collage_layout_expert/references/collage-types"},"next":{"title":"David Hockney\'s Joiners Tec...","permalink":"/docs/skills/collage_layout_expert/references/hockney-technique"}}');var t=i(74848),r=i(28453);const a={title:"Edge-Based Assembly Strategy",sidebar_label:"Edge-Based Assembly Strategy",sidebar_position:5},o="Edge-Based Assembly Strategy",c={},l=[{value:"Core Concept: &quot;Edge-First&quot; Composition",id:"core-concept-edge-first-composition",level:2},{value:"Edge Descriptor",id:"edge-descriptor",level:2},{value:"Edge Compatibility Scoring",id:"edge-compatibility-scoring",level:2},{value:"Angle Alignment",id:"angle-alignment",level:2},{value:"Position Alignment",id:"position-alignment",level:2},{value:"Assembly Algorithm: Greedy Edge Growth",id:"assembly-algorithm-greedy-edge-growth",level:2},{value:"Edge Urgency Heuristic",id:"edge-urgency-heuristic",level:2},{value:"Practical Optimizations",id:"practical-optimizations",level:2},{value:"1. Hierarchical Clustering",id:"1-hierarchical-clustering",level:3},{value:"2. Multi-Scale Matching",id:"2-multi-scale-matching",level:3},{value:"3. Caching Good Pairs",id:"3-caching-good-pairs",level:3},{value:"4. Pruning Generic Edges",id:"4-pruning-generic-edges",level:3},{value:"5. Backtracking",id:"5-backtracking",level:3},{value:"Performance Impact",id:"performance-impact",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"edge-based-assembly-strategy",children:"Edge-Based Assembly Strategy"})}),"\n",(0,t.jsx)(n.h2,{id:"core-concept-edge-first-composition",children:'Core Concept: "Edge-First" Composition'}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"The Insight"}),": Photos connect at their edges, not by timestamp or random placement."]}),"\n",(0,t.jsx)(n.h2,{id:"edge-descriptor",children:"Edge Descriptor"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"@dataclass\nclass EdgeDescriptor:\n    photo_id: UUID\n    side: str  # 'top', 'bottom', 'left', 'right'\n    region: np.ndarray  # 10% strip along edge\n\n    # Geometric features\n    lines: List[Line]              # Lines intersecting this edge\n    curves: List[Curve]            # Curves at edge\n    dominant_angle: float          # -90\xb0 to 90\xb0\n    complexity: float              # 0-1 (busy vs. clean)\n\n    # Color features\n    colors: ColorPalette           # 3-5 dominant colors in LAB\n    gradient_direction: str        # 'lighter', 'darker', 'neutral'\n    temperature: str               # 'warm', 'cool', 'neutral'\n\n    # Semantic features\n    clip_embedding: np.ndarray     # 512-dim CLIP of edge region\n    detected_objects: List[str]    # ['sky', 'water', 'person_partial']\n\n    # Match preferences\n    blendability: float            # 0-1 (how well can this edge blend?)\n    wants_continuation: bool       # Is something cut off?\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"edge-compatibility-scoring",children:"Edge Compatibility Scoring"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def edge_compatibility(edge1, edge2):\n    \"\"\"\n    Score how well two edges can connect (0-1, higher = better).\n    \"\"\"\n    scores = {}\n\n    # GEOMETRIC: Lines/curves flow across boundary\n    scores['line_continuation'] = (\n        angle_alignment(edge1.lines, edge2.lines) * 0.4 +\n        position_alignment(edge1.lines, edge2.lines) * 0.3 +\n        multiple_line_bonus(edge1.lines, edge2.lines) * 0.3\n    )\n\n    scores['curve_flow'] = (\n        tangent_match(edge1.curves, edge2.curves) * 0.5 +\n        curvature_naturalness(edge1.curves, edge2.curves) * 0.5\n    )\n\n    # COLOR: Harmonious or complementary\n    scores['color_harmony'] = compute_color_harmony(\n        edge1.colors, edge2.colors, mode='edge_regions'\n    )\n\n    # SEMANTIC: Related content (CLIP similarity)\n    scores['semantic_coherence'] = cosine_similarity(\n        edge1.clip_embedding, edge2.clip_embedding\n    )\n\n    # BALANCE: Complexity contrast\n    complexity_diff = abs(edge1.complexity - edge2.complexity)\n    scores['complexity_balance'] = 1.0 - min(1.0, complexity_diff / 0.5)\n\n    # Weighted combination\n    return (\n        0.30 * scores['line_continuation'] +\n        0.15 * scores['curve_flow'] +\n        0.25 * scores['color_harmony'] +\n        0.20 * scores['semantic_coherence'] +\n        0.10 * scores['complexity_balance']\n    )\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"angle-alignment",children:"Angle Alignment"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def angle_alignment(lines1, lines2, tolerance=15.0):\n    """\n    Check if dominant angles of two edge regions align.\n    tolerance: degrees (15\xb0 is forgiving, 5\xb0 is strict)\n    """\n    if not lines1 or not lines2:\n        return 0.0\n\n    # Weighted average by line length and strength\n    angle1 = weighted_average_angle(lines1)\n    angle2 = weighted_average_angle(lines2)\n\n    # Angular difference (accounting for \xb1180\xb0 equivalence)\n    diff = abs(angle1 - angle2)\n    diff = min(diff, 180 - diff)  # Handle wraparound\n\n    # Score: 1.0 if perfect, 0.0 if > tolerance\n    return max(0.0, 1.0 - diff / tolerance)\n\ndef weighted_average_angle(lines):\n    """Calculate dominant angle weighted by line properties."""\n    weights = [line.length * line.strength for line in lines]\n    angles = [line.angle for line in lines]\n    return np.average(angles, weights=weights)\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"position-alignment",children:"Position Alignment"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def position_alignment(lines1, lines2, edge_pair):\n    \"\"\"\n    Check if lines align positionally across boundary.\n\n    Example: For right edge of photo A and left edge of photo B,\n             do horizontal lines have matching y-coordinates?\n    \"\"\"\n    edge_type = edge_pair  # ('right', 'left') or ('top', 'bottom')\n\n    if edge_type in [('right', 'left'), ('left', 'right')]:\n        coord_dim = 'y'\n    else:\n        coord_dim = 'x'\n\n    relevant_lines1 = filter_lines_by_orientation(lines1, edge_type[0])\n    relevant_lines2 = filter_lines_by_orientation(lines2, edge_type[1])\n\n    if not relevant_lines1 or not relevant_lines2:\n        return 0.0\n\n    coords1 = [get_boundary_coord(line, edge_type[0], coord_dim) for line in relevant_lines1]\n    coords2 = [get_boundary_coord(line, edge_type[1], coord_dim) for line in relevant_lines2]\n\n    # Find closest pairs and compute alignment score\n    min_distances = []\n    for c1 in coords1:\n        min_dist = min(abs(c1 - c2) for c2 in coords2)\n        min_distances.append(min_dist)\n\n    avg_misalignment = np.mean(min_distances)\n\n    # Score: 1.0 if perfect (&lt;5px), 0.0 if terrible (&gt;50px)\n    return max(0.0, 1.0 - avg_misalignment / 50.0)\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"assembly-algorithm-greedy-edge-growth",children:"Assembly Algorithm: Greedy Edge Growth"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def assemble_collage_greedy(seed_photo, photo_database, target_size=(10, 10)):\n    """\n    Build collage by iteratively adding photos to best-matching edges.\n    """\n    # 1. SEED SELECTION\n    canvas = Canvas(target_size)\n    canvas.place_photo(seed_photo, position=\'center\', locked=True)\n\n    # Priority queue of open edges (scored by "urgency")\n    open_edges = PriorityQueue()\n    for edge in seed_photo.edges:\n        urgency = compute_edge_urgency(edge)\n        open_edges.push(edge, priority=urgency)\n\n    # 2. ITERATIVE GROWTH\n    while canvas.coverage < 0.8 and not open_edges.empty():\n        current_edge = open_edges.pop()\n\n        # Query k best matches from database\n        candidates = photo_database.find_compatible_edges(\n            query_edge=current_edge,\n            k=20,\n            filters={\n                \'aspect_ratio\': current_edge.compatible_aspect_ratios,\n                \'min_compatibility\': 0.4\n            }\n        )\n\n        # Try candidates in order of compatibility\n        placed = False\n        for candidate_photo in candidates:\n            if canvas.would_overlap(candidate_photo):\n                continue\n\n            local_fit = edge_compatibility(current_edge, candidate_photo.opposite_edge)\n            global_aesthetics = canvas.score_global_aesthetics_with(candidate_photo)\n\n            if local_fit > 0.5 and global_aesthetics > 0.6:\n                canvas.place_photo(candidate_photo, adjacent_to=current_edge)\n\n                for new_edge in candidate_photo.new_open_edges:\n                    urgency = compute_edge_urgency(new_edge)\n                    open_edges.push(new_edge, priority=urgency)\n\n                placed = True\n                break\n\n        if not placed:\n            current_edge.relaxed = True\n            open_edges.push(current_edge, priority=0.5)\n\n    # 3. BOUNDARY REFINEMENT\n    canvas.refine_boundaries(\n        crop_for_alignment=True,\n        blend_overlaps=True,\n        inpaint_gaps=True,\n        color_grade_globally=True\n    )\n\n    return canvas.render()\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"edge-urgency-heuristic",children:"Edge Urgency Heuristic"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def compute_edge_urgency(edge):\n    """\n    Determine which edges should be filled first.\n    Higher urgency = fill sooner\n    """\n    urgency = 0.0\n\n    # Strong lines \u2192 high urgency (want to continue them)\n    if edge.has_strong_lines():\n        urgency += 0.5\n\n    # Cut-off objects \u2192 very high urgency (want completion)\n    if edge.wants_continuation:\n        urgency += 0.7\n\n    # High aesthetic quality \u2192 high urgency\n    urgency += edge.photo.aesthetic_score * 0.3\n\n    # Central position \u2192 higher urgency (build from center out)\n    distance_from_center = edge.distance_to_canvas_center()\n    urgency += (1.0 - distance_from_center) * 0.2\n\n    return urgency\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"practical-optimizations",children:"Practical Optimizations"}),"\n",(0,t.jsx)(n.h3,{id:"1-hierarchical-clustering",children:"1. Hierarchical Clustering"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Concept"}),": Group similar photos into clusters, search within clusters first."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class PhotoDatabase:\n    def __init__(self, photos):\n        self.clusters = self._cluster_photos_hierarchically(photos)\n\n    def _cluster_photos_hierarchically(self, photos):\n        """\n        Group photos into ~50-100 clusters using CLIP embeddings.\n        Benefits: 50x speedup in matching\n        """\n        embeddings = np.array([p.clip_embedding for p in photos])\n\n        from sklearn.cluster import AgglomerativeClustering\n        clustering = AgglomerativeClustering(\n            n_clusters=min(100, len(photos) // 100),\n            metric=\'cosine\',\n            linkage=\'average\'\n        )\n        labels = clustering.fit_predict(embeddings)\n\n        clusters = {}\n        for photo, label in zip(photos, labels):\n            clusters.setdefault(label, []).append(photo)\n\n        return clusters\n'})}),"\n",(0,t.jsx)(n.h3,{id:"2-multi-scale-matching",children:"2. Multi-Scale Matching"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def find_matches_multiscale(query_edge, database):\n    """\n    Progressive refinement: fast coarse search, slow precise refinement.\n\n    Total: 50ms instead of 500ms for all-full-res\n    """\n    # Stage 1: Coarse search on thumbnails\n    candidates_coarse = database.search_thumbnails(\n        query_edge.thumbnail_embedding, k=100\n    )\n\n    # Stage 2: Geometric filtering\n    candidates_filtered = [\n        c for c in candidates_coarse\n        if abs(c.dominant_angle - query_edge.dominant_angle) < 30\n    ]\n\n    # Stage 3: Full-resolution scoring (top 20 only)\n    candidates_scored = []\n    for c in candidates_filtered[:20]:\n        score = edge_compatibility_fullres(query_edge, c)\n        candidates_scored.append((score, c))\n\n    candidates_scored.sort(reverse=True, key=lambda x: x[0])\n    return [c for score, c in candidates_scored[:10]]\n'})}),"\n",(0,t.jsx)(n.h3,{id:"3-caching-good-pairs",children:"3. Caching Good Pairs"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class PairCache:\n    """Learn from experience: which edges work well together?"""\n    def __init__(self):\n        self.successful_pairs = {}\n        self.usage_counts = {}\n\n    def record_success(self, edge1, edge2, score):\n        pair_key = (edge1.id, edge2.id)\n        self.successful_pairs[pair_key] = score\n        self.usage_counts[pair_key] = self.usage_counts.get(pair_key, 0) + 1\n\n    def boost_known_pairs(self, candidates, query_edge):\n        for c in candidates:\n            pair_key = (query_edge.id, c.edge_id)\n            if pair_key in self.successful_pairs:\n                boost = self.successful_pairs[pair_key] * 0.2\n                boost += np.log1p(self.usage_counts[pair_key]) * 0.1\n                c.score += boost\n        return sorted(candidates, key=lambda c: c.score, reverse=True)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"4-pruning-generic-edges",children:"4. Pruning Generic Edges"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def is_edge_generic(edge):\n    """\n    Generic edges (plain sky, solid colors) don\'t need expensive matching.\n    """\n    if edge.complexity < 0.2 and edge.blendability > 0.8:\n        if len(edge.lines) < 2 and len(edge.colors.colors) <= 2:\n            return True\n    return False\n'})}),"\n",(0,t.jsx)(n.h3,{id:"5-backtracking",children:"5. Backtracking"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def assemble_with_backtracking(seed, database, target_size):\n    """Greedy growth with backtracking for difficult cases."""\n    canvas = Canvas(target_size)\n    canvas.place_photo(seed, position=\'center\')\n\n    history = []\n    max_backtracks = 5\n\n    while canvas.coverage < 0.8:\n        edge = canvas.best_open_edge()\n        candidates = database.find_compatible_edges(edge, k=20)\n\n        placed = False\n        for candidate in candidates:\n            if canvas.can_place(candidate):\n                canvas.place_photo(candidate, adjacent_to=edge)\n                history.append((candidate, edge))\n                placed = True\n                break\n\n        if not placed and len(history) > 0 and max_backtracks > 0:\n            canvas.undo(history.pop())\n            canvas.undo(history.pop())\n            max_backtracks -= 1\n            continue\n\n        if not placed:\n            edge.mark_skipped()\n\n    return canvas\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"performance-impact",children:"Performance Impact"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Optimization"}),(0,t.jsx)(n.th,{children:"Speedup"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Hierarchical clustering"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"50x"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Multi-scale matching"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"10x"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Caching"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"1.5x"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Pruning"}),(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"2-3x"})})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Backtracking"}),(0,t.jsx)(n.td,{children:"Quality improvement"})]})]})]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Combined"}),": 10-photo collage in ",(0,t.jsx)(n.strong,{children:"0.5-2 seconds"})," instead of 50-200 seconds."]})]})}function g(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);