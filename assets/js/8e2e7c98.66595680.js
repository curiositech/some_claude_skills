"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[36188],{22870:(e,s,i)=>{i.r(s),i.d(s,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"skills/photo_composition_critic/references/ml-models","title":"Computational Aesthetics Models","description":"ML models and datasets for image quality assessment.","source":"@site/docs/skills/photo_composition_critic/references/ml-models.md","sourceDirName":"skills/photo_composition_critic/references","slug":"/skills/photo_composition_critic/references/ml-models","permalink":"/docs/skills/photo_composition_critic/references/ml-models","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Computational Aesthetics Models","sidebar_label":"Computational Aesthetics Mo...","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Composition Theory","permalink":"/docs/skills/photo_composition_critic/references/composition-theory"},"next":{"title":"Color Theory Palette Harmony Expert","permalink":"/docs/skills/color_theory_palette_harmony_expert/"}}');var n=i(74848),a=i(28453);const o={title:"Computational Aesthetics Models",sidebar_label:"Computational Aesthetics Mo...",sidebar_position:4},r="Computational Aesthetics Models",l={},c=[{value:"AVA Dataset (Aesthetic Visual Analysis)",id:"ava-dataset-aesthetic-visual-analysis",level:2},{value:"NIMA (Neural Image Assessment)",id:"nima-neural-image-assessment",level:2},{value:"LAION-Aesthetics",id:"laion-aesthetics",level:2},{value:"VisualQuality-R1 (2024)",id:"visualquality-r1-2024",level:2},{value:"Key Papers",id:"key-papers",level:2}];function d(e){const s={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(s.header,{children:(0,n.jsx)(s.h1,{id:"computational-aesthetics-models",children:"Computational Aesthetics Models"})}),"\n",(0,n.jsx)(s.p,{children:"ML models and datasets for image quality assessment."}),"\n",(0,n.jsx)(s.h2,{id:"ava-dataset-aesthetic-visual-analysis",children:"AVA Dataset (Aesthetic Visual Analysis)"}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{children:"250,000+ images from dpchallenge.com\n\u251c\u2500\u2500 Mean scores from 78-549 votes each\n\u251c\u2500\u2500 Semantic tags (landscape, portrait, etc.)\n\u251c\u2500\u2500 Style tags (HDR, vintage, etc.)\n\u2514\u2500\u2500 Ground truth for training aesthetics models\n\nSCORE DISTRIBUTION INSIGHT\n\u251c\u2500\u2500 Most images: 5.0-5.5 (mediocre)\n\u251c\u2500\u2500 Great images: 6.5+ (top ~5%)\n\u251c\u2500\u2500 Exceptional: 7.0+ (top ~1%)\n\u2514\u2500\u2500 Bimodal: Some images polarize voters\n"})}),"\n",(0,n.jsx)(s.h2,{id:"nima-neural-image-assessment",children:"NIMA (Neural Image Assessment)"}),"\n",(0,n.jsxs)(s.p,{children:["Google's 2017 model predicting AVA scores. Key innovation: predicts ",(0,n.jsx)(s.strong,{children:"distribution"}),", not just mean score."]}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-python",children:"# Architecture: MobileNet/VGG16/Inception + custom head\n# Output: 10-class probability distribution (scores 1-10)\n# Loss: Earth Mover's Distance (EMD)\n\ndef get_nima_score(image_path):\n    img = preprocess(load_image(image_path))\n    distribution = model.predict(img)\n    mean_score = sum(i * distribution[i] for i in range(10))\n    return mean_score, distribution\n\n# INTERPRETATION\n# Mean: Overall quality prediction\n# Std Dev: How polarizing/consistent\n# Distribution shape: Technical vs aesthetic issues\n"})}),"\n",(0,n.jsx)(s.h2,{id:"laion-aesthetics",children:"LAION-Aesthetics"}),"\n",(0,n.jsxs)(s.p,{children:["LAION-5B filtered by aesthetic predictor. ",(0,n.jsx)(s.strong,{children:"Used to train Stable Diffusion."})]}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{children:"SUBSETS\n\u251c\u2500\u2500 aesthetics_6plus: ~600M images, score \u22656\n\u251c\u2500\u2500 aesthetics_5plus: ~1.2B images, score \u22655\n\nTHE AESTHETIC PREDICTOR\n\u251c\u2500\u2500 CLIP ViT-L/14 embeddings\n\u251c\u2500\u2500 Simple MLP regression head\n\u251c\u2500\u2500 Trained on SAC (Simulacra Aesthetic Captions)\n\u2514\u2500\u2500 Fast inference, reasonable accuracy\n"})}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{className:"language-python",children:"def laion_aesthetic_score(image):\n    clip_embedding = clip_model.encode_image(image)\n    score = aesthetic_mlp(clip_embedding)\n    return score  # 1-10 scale\n"})}),"\n",(0,n.jsx)(s.h2,{id:"visualquality-r1-2024",children:"VisualQuality-R1 (2024)"}),"\n",(0,n.jsx)(s.p,{children:"Recent reasoning-augmented quality assessment."}),"\n",(0,n.jsx)(s.pre,{children:(0,n.jsx)(s.code,{children:"KEY INNOVATION\n\u251c\u2500\u2500 Chain-of-thought reasoning about quality\n\u251c\u2500\u2500 Explains WHY an image scores high/low\n\u251c\u2500\u2500 Trained on quality rationales, not just scores\n\u2514\u2500\u2500 Better generalization than pure regression\n\nEVALUATION DIMENSIONS\n\u251c\u2500\u2500 Technical: Sharpness, noise, exposure, color\n\u251c\u2500\u2500 Aesthetic: Composition, lighting, subject\n\u251c\u2500\u2500 Semantic: Meaning, story, emotional impact\n\u2514\u2500\u2500 Contextual: Genre-appropriate quality\n"})}),"\n",(0,n.jsx)(s.h2,{id:"key-papers",children:"Key Papers"}),"\n",(0,n.jsxs)(s.ul,{children:["\n",(0,n.jsx)(s.li,{children:'Murray, N. et al. (2012). "AVA: A Large-Scale Database for Aesthetic Visual Analysis"'}),"\n",(0,n.jsx)(s.li,{children:'Talebi, H. & Milanfar, P. (2018). "NIMA: Neural Image Assessment"'}),"\n",(0,n.jsx)(s.li,{children:'Schuhmann, C. et al. (2022). "LAION-5B: An open large-scale dataset"'}),"\n",(0,n.jsx)(s.li,{children:'Wu, Q. et al. (2024). "Q-Instruct: Improving Low-level Visual Abilities"'}),"\n"]})]})}function h(e={}){const{wrapper:s}={...(0,a.R)(),...e.components};return s?(0,n.jsx)(s,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},28453:(e,s,i)=>{i.d(s,{R:()=>o,x:()=>r});var t=i(96540);const n={},a=t.createContext(n);function o(e){const s=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(s):{...s,...e}},[s,e])}function r(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:o(e.components),t.createElement(a.Provider,{value:s},e.children)}}}]);