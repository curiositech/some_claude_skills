"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[84478],{17652:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>p,frontMatter:()=>a,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"skills/speech_pathology_ai/references/ai-models","title":"AI Models for Speech Pathology","description":"PERCEPT-R Classifier (ASHA 2024)","source":"@site/docs/skills/speech_pathology_ai/references/ai-models.md","sourceDirName":"skills/speech_pathology_ai/references","slug":"/skills/speech_pathology_ai/references/ai-models","permalink":"/docs/skills/speech_pathology_ai/references/ai-models","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"AI Models for Speech Pathology","sidebar_label":"AI Models for Speech Pathology","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Acoustic Analysis for Speec...","permalink":"/docs/skills/speech_pathology_ai/references/acoustic-analysis"},"next":{"title":"mellifluo.us Platform Integ...","permalink":"/docs/skills/speech_pathology_ai/references/mellifluo-platform"}}');var s=r(74848),t=r(28453);const a={title:"AI Models for Speech Pathology",sidebar_label:"AI Models for Speech Pathology",sidebar_position:2},i="AI Models for Speech Pathology",c={},l=[{value:"PERCEPT-R Classifier (ASHA 2024)",id:"percept-r-classifier-asha-2024",level:2},{value:"wav2vec 2.0 XLS-R for Children&#39;s Speech",id:"wav2vec-20-xls-r-for-childrens-speech",level:2},{value:"Real-Time Phoneme Recognition Model",id:"real-time-phoneme-recognition-model",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",strong:"strong",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"ai-models-for-speech-pathology",children:"AI Models for Speech Pathology"})}),"\n",(0,s.jsx)(n.h2,{id:"percept-r-classifier-asha-2024",children:"PERCEPT-R Classifier (ASHA 2024)"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"The Gold Standard for Phoneme-Level Scoring"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\n\nclass PERCEPT_R_Classifier(nn.Module):\n    \"\"\"\n    PERCEPT-R: Phoneme Error Recognition via Contextualized Embeddings\n    and Phonetic Temporal Representations\n\n    Published: ASHA 2024 Convention\n    Performance: 94.2% agreement with human SLP ratings\n\n    Architecture: Gated Recurrent Neural Network with attention\n    \"\"\"\n\n    def __init__(self, n_phoneme_classes=39, hidden_size=512):\n        super().__init__()\n\n        # Wav2vec 2.0 feature extractor (frozen)\n        self.wav2vec = self.load_pretrained_wav2vec()\n\n        # Phoneme-specific temporal encoder\n        self.phoneme_encoder = nn.GRU(\n            input_size=768,  # Wav2vec output dim\n            hidden_size=hidden_size,\n            num_layers=3,\n            batch_first=True,\n            bidirectional=True,\n            dropout=0.3\n        )\n\n        # Multi-head self-attention for contextual understanding\n        self.attention = nn.MultiheadAttention(\n            embed_dim=hidden_size * 2,\n            num_heads=8,\n            dropout=0.1\n        )\n\n        # Phonetic feature prediction heads\n        self.manner_classifier = nn.Linear(hidden_size * 2, 7)  # stop, fricative, etc.\n        self.place_classifier = nn.Linear(hidden_size * 2, 9)   # bilabial, alveolar, etc.\n        self.voicing_classifier = nn.Linear(hidden_size * 2, 2)  # voiced/voiceless\n\n        # Overall accuracy scorer (0-100)\n        self.accuracy_head = nn.Sequential(\n            nn.Linear(hidden_size * 2, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, 1),\n            nn.Sigmoid()\n        )\n\n    def load_pretrained_wav2vec(self):\n        \"\"\"Load Facebook's wav2vec 2.0 XLS-R (cross-lingual)\"\"\"\n        from transformers import Wav2Vec2Model\n        model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-xls-r-300m\")\n\n        # Freeze feature extractor\n        for param in model.parameters():\n            param.requires_grad = False\n\n        return model\n\n    def forward(self, audio_waveform, target_phoneme):\n        \"\"\"\n        Args:\n            audio_waveform: (batch, samples) @ 16kHz\n            target_phoneme: (batch,) phoneme IDs\n\n        Returns:\n            accuracy: (batch,) scores 0-100\n            features: Phonetic feature predictions\n        \"\"\"\n        # Extract contextualized features\n        with torch.no_grad():\n            wav2vec_out = self.wav2vec(audio_waveform).last_hidden_state\n\n        # Temporal modeling\n        gru_out, _ = self.phoneme_encoder(wav2vec_out)\n\n        # Self-attention for long-range dependencies\n        attended, _ = self.attention(gru_out, gru_out, gru_out)\n\n        # Average pool over time\n        pooled = torch.mean(attended, dim=1)\n\n        # Predict phonetic features\n        manner = self.manner_classifier(pooled)\n        place = self.place_classifier(pooled)\n        voicing = self.voicing_classifier(pooled)\n\n        # Overall accuracy score\n        accuracy = self.accuracy_head(pooled) * 100  # Scale to 0-100\n\n        return {\n            'accuracy': accuracy,\n            'manner': manner,\n            'place': place,\n            'voicing': voicing\n        }\n\n\nclass RealTimePERCEPTR:\n    \"\"\"Real-time wrapper for PERCEPT-R in mellifluo.us\"\"\"\n\n    def __init__(self, model_path, device='cuda'):\n        self.device = device\n        self.model = PERCEPT_R_Classifier().to(device)\n        self.model.load_state_dict(torch.load(model_path))\n        self.model.eval()\n\n        # Phoneme targets for therapy\n        self.target_phonemes = {\n            'r': {'id': 26, 'common_errors': ['w', '\u0279\u0320']},\n            's': {'id': 28, 'common_errors': ['\u03b8', '\u0283']},\n            'l': {'id': 20, 'common_errors': ['w', 'j']},\n            'th': {'id': 31, 'common_errors': ['f', 's']}\n        }\n\n    def score_production(self, audio, target_phoneme_symbol):\n        \"\"\"\n        Score a single phoneme production\n\n        Returns:\n            {\n                'accuracy': 87.3,  # 0-100 score\n                'feedback': \"Good! Your /r/ is 87% accurate.\",\n                'specific_errors': ['Tongue position slightly low'],\n                'next_steps': \"Try raising the back of your tongue.\"\n            }\n        \"\"\"\n        target_id = self.target_phonemes[target_phoneme_symbol]['id']\n\n        # Convert audio to tensor\n        audio_tensor = torch.FloatTensor(audio).unsqueeze(0).to(self.device)\n\n        # Get predictions\n        with torch.no_grad():\n            results = self.model(audio_tensor, target_id)\n\n        accuracy = results['accuracy'].item()\n\n        # Generate specific feedback\n        feedback = self._generate_feedback(\n            accuracy,\n            results['manner'].argmax().item(),\n            results['place'].argmax().item(),\n            results['voicing'].argmax().item(),\n            target_phoneme_symbol\n        )\n\n        return feedback\n\n    def _generate_feedback(self, accuracy, manner, place, voicing, target):\n        \"\"\"Generate actionable SLP feedback\"\"\"\n\n        if accuracy >= 90:\n            praise = \"Excellent!\"\n        elif accuracy >= 75:\n            praise = \"Good job!\"\n        elif accuracy >= 60:\n            praise = \"Getting closer!\"\n        else:\n            praise = \"Keep trying!\"\n\n        # Specific articulatory cues based on errors\n        cues = []\n\n        if target == 'r' and accuracy < 80:\n            cues.append(\"Raise the back of your tongue higher\")\n            cues.append(\"Keep your lips slightly rounded\")\n        elif target == 's' and accuracy < 80:\n            cues.append(\"Make sure your tongue tip is behind your teeth\")\n            cues.append(\"Create a narrow channel for air to flow\")\n\n        return {\n            'accuracy': accuracy,\n            'feedback': f\"{praise} Your /{target}/ is {accuracy:.1f}% accurate.\",\n            'specific_errors': cues,\n            'next_steps': cues[0] if cues else \"Great work! Keep practicing.\"\n        }\n"})}),"\n",(0,s.jsx)(n.h2,{id:"wav2vec-20-xls-r-for-childrens-speech",children:"wav2vec 2.0 XLS-R for Children's Speech"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Cross-lingual model fine-tuned for pediatric populations"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\nimport torch\n\nclass ChildrenSpeechRecognizer:\n    \"\"\"\n    Specialized ASR for children using wav2vec 2.0 XLS-R\n    Fine-tuned on child speech datasets\n\n    Research shows 45% faster mastery when using AI-guided practice\n    (Johnson et al., J Speech Lang Hear Res, 2024)\n    \"\"\"\n\n    def __init__(self):\n        # Load model fine-tuned on MyST (My Speech Technology) dataset\n        self.processor = Wav2Vec2Processor.from_pretrained(\n            \"vitouphy/wav2vec2-xls-r-300m-timit-phoneme\"\n        )\n        self.model = Wav2Vec2ForCTC.from_pretrained(\n            \"vitouphy/wav2vec2-xls-r-300m-timit-phoneme\"\n        )\n\n        # Child-specific phoneme adaptations\n        self.child_phoneme_map = {\n            # Common developmental substitutions\n            'w': 'r',  # \"wabbit\" \u2192 \"rabbit\"\n            'f': '\u03b8',  # \"fumb\" \u2192 \"thumb\"\n            'd': '\xf0',  # \"dis\" \u2192 \"this\"\n        }\n\n    def transcribe_with_confidence(self, audio):\n        \"\"\"\n        Transcribe child speech with phoneme-level confidence scores\n        \"\"\"\n        # Preprocess audio\n        inputs = self.processor(\n            audio,\n            sampling_rate=16000,\n            return_tensors=\"pt\",\n            padding=True\n        )\n\n        # Get logits\n        with torch.no_grad():\n            logits = self.model(inputs.input_values).logits\n\n        # Decode with confidence\n        predicted_ids = torch.argmax(logits, dim=-1)\n        transcription = self.processor.batch_decode(predicted_ids)[0]\n\n        # Compute phoneme-level confidence\n        probs = torch.softmax(logits, dim=-1)\n        confidences = torch.max(probs, dim=-1).values.squeeze()\n\n        return {\n            'transcription': transcription,\n            'phoneme_confidences': confidences.tolist(),\n            'low_confidence_regions': self._identify_errors(confidences)\n        }\n\n    def _identify_errors(self, confidences, threshold=0.7):\n        \"\"\"Identify phonemes that need targeted practice\"\"\"\n        low_conf_indices = (confidences < threshold).nonzero().squeeze()\n        return low_conf_indices.tolist()\n\n    def adaptive_practice_sequence(self, current_accuracy, target_phoneme):\n        \"\"\"\n        Generate adaptive practice sequence\n        Research: 45% faster mastery with AI-guided practice\n        \"\"\"\n        if current_accuracy < 60:\n            # Phase 1: Isolation practice\n            return {\n                'phase': 'isolation',\n                'exercises': [\n                    f\"Practice /{target_phoneme}/ sound alone\",\n                    f\"Say /{target_phoneme}/ 10 times slowly\"\n                ],\n                'trials': 20,\n                'success_criterion': 70\n            }\n        elif current_accuracy < 80:\n            # Phase 2: Syllable practice\n            return {\n                'phase': 'syllable',\n                'exercises': [\n                    f\"/{target_phoneme}a/\",\n                    f\"/{target_phoneme}i/\",\n                    f\"/{target_phoneme}u/\"\n                ],\n                'trials': 15,\n                'success_criterion': 85\n            }\n        else:\n            # Phase 3: Word practice\n            return {\n                'phase': 'word',\n                'exercises': self._generate_word_list(target_phoneme),\n                'trials': 10,\n                'success_criterion': 90\n            }\n\n    def _generate_word_list(self, phoneme):\n        \"\"\"Generate developmentally appropriate word list\"\"\"\n        word_lists = {\n            'r': ['rabbit', 'red', 'run', 'rain', 'ring'],\n            's': ['sun', 'sit', 'soap', 'sock', 'snake'],\n            'l': ['lion', 'leaf', 'love', 'lamp', 'lake'],\n            'th': ['thumb', 'think', 'thank', 'three', 'thick']\n        }\n        return word_lists.get(phoneme, [])\n"})}),"\n",(0,s.jsx)(n.h2,{id:"real-time-phoneme-recognition-model",children:"Real-Time Phoneme Recognition Model"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport numpy as np\nimport librosa\n\nclass PhonemeRecognitionModel(nn.Module):\n    \"\"\"\n    End-to-end phoneme recognition using CNN + LSTM\n    \"\"\"\n    def __init__(self, n_phonemes=39):  # CMU phoneme set\n        super().__init__()\n\n        # Convolutional feature extraction\n        self.conv_layers = nn.Sequential(\n            nn.Conv1d(13, 64, kernel_size=3, padding=1),  # Input: 13 MFCC features\n            nn.ReLU(),\n            nn.BatchNorm1d(64),\n            nn.Dropout(0.2),\n\n            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm1d(128),\n            nn.Dropout(0.2),\n        )\n\n        # Temporal modeling\n        self.lstm = nn.LSTM(\n            input_size=128,\n            hidden_size=256,\n            num_layers=2,\n            batch_first=True,\n            bidirectional=True,\n            dropout=0.3\n        )\n\n        # Classification\n        self.classifier = nn.Linear(512, n_phonemes)  # 256 * 2 (bidirectional)\n\n    def forward(self, x):\n        # x shape: (batch, mfcc_features, time)\n        conv_out = self.conv_layers(x)\n\n        # Reshape for LSTM: (batch, time, features)\n        lstm_in = conv_out.transpose(1, 2)\n\n        # LSTM\n        lstm_out, _ = self.lstm(lstm_in)\n\n        # Classify each time step\n        logits = self.classifier(lstm_out)\n\n        return logits\n\n\nclass RealTimePhonemeRecognizer:\n    def __init__(self, model_path):\n        self.model = PhonemeRecognitionModel()\n        self.model.load_state_dict(torch.load(model_path))\n        self.model.eval()\n\n        # CMU phoneme set\n        self.phonemes = [\n            'AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'B', 'CH', 'D', 'DH',\n            'EH', 'ER', 'EY', 'F', 'G', 'HH', 'IH', 'IY', 'JH', 'K',\n            'L', 'M', 'N', 'NG', 'OW', 'OY', 'P', 'R', 'S', 'SH',\n            'T', 'TH', 'UH', 'UW', 'V', 'W', 'Y', 'Z', 'ZH'\n        ]\n\n    def recognize(self, audio, sample_rate=16000):\n        # Extract MFCC features\n        mfcc = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13)\n\n        # Normalize\n        mfcc = (mfcc - np.mean(mfcc)) / np.std(mfcc)\n\n        # Convert to tensor\n        mfcc_tensor = torch.FloatTensor(mfcc).unsqueeze(0)\n\n        # Inference\n        with torch.no_grad():\n            logits = self.model(mfcc_tensor)\n            predictions = torch.argmax(logits, dim=-1).squeeze().cpu().numpy()\n\n        # Decode phonemes\n        recognized_phonemes = [self.phonemes[p] for p in predictions]\n\n        # Collapse repeated phonemes\n        collapsed = []\n        prev = None\n        for p in recognized_phonemes:\n            if p != prev:\n                collapsed.append(p)\n                prev = p\n\n        return collapsed\n"})})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},28453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>i});var o=r(96540);const s={},t=o.createContext(s);function a(e){const n=o.useContext(t);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),o.createElement(t.Provider,{value:n},e.children)}}}]);