"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[67332],{28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var i=t(96540);const s={},a=i.createContext(s);function r(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),i.createElement(a.Provider,{value:n},e.children)}},71176:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"skills/wedding_immortalist/references/face-clustering-aesthetics","title":"Face Clustering & Aesthetic Photo Selection","description":"Overview","source":"@site/docs/skills/wedding_immortalist/references/face-clustering-aesthetics.md","sourceDirName":"skills/wedding_immortalist/references","slug":"/skills/wedding_immortalist/references/face-clustering-aesthetics","permalink":"/docs/skills/wedding_immortalist/references/face-clustering-aesthetics","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Face Clustering & Aesthetic Photo Selection","sidebar_label":"Face Clustering & Aesthetic...","sidebar_position":1}}');var s=t(74848),a=t(28453);const r={title:"Face Clustering & Aesthetic Photo Selection",sidebar_label:"Face Clustering & Aesthetic...",sidebar_position:1},o="Face Clustering & Aesthetic Photo Selection",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Face Detection Pipeline",id:"face-detection-pipeline",level:2},{value:"Detection Models Comparison",id:"detection-models-comparison",level:3},{value:"RetinaFace Implementation",id:"retinaface-implementation",level:3},{value:"Face Embedding &amp; Clustering",id:"face-embedding--clustering",level:2},{value:"Embedding Models",id:"embedding-models",level:3},{value:"ArcFace Embedding",id:"arcface-embedding",level:3},{value:"HDBSCAN Clustering",id:"hdbscan-clustering",level:3},{value:"Aesthetic Quality Scoring",id:"aesthetic-quality-scoring",level:2},{value:"Multi-Factor Scoring Model",id:"multi-factor-scoring-model",level:3},{value:"Diversity-Aware Selection",id:"diversity-aware-selection",level:3},{value:"Identity Linking Workflow",id:"identity-linking-workflow",level:2},{value:"Priority-Based Naming",id:"priority-based-naming",level:3},{value:"Output Format",id:"output-format",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"face-clustering--aesthetic-photo-selection",children:"Face Clustering & Aesthetic Photo Selection"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"Every wedding guest deserves great photos of themselves. This system automatically:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Detects all faces across thousands of photos"}),"\n",(0,s.jsx)(n.li,{children:"Clusters them by identity"}),"\n",(0,s.jsx)(n.li,{children:"Scores each photo for aesthetic quality"}),"\n",(0,s.jsx)(n.li,{children:"Selects the best N photos per person"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"face-detection-pipeline",children:"Face Detection Pipeline"}),"\n",(0,s.jsx)(n.h3,{id:"detection-models-comparison",children:"Detection Models Comparison"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{children:"Speed"}),(0,s.jsx)(n.th,{children:"Accuracy"}),(0,s.jsx)(n.th,{children:"Best For"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"RetinaFace"})}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"Highest"}),(0,s.jsx)(n.td,{children:"Production quality"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"MTCNN"})}),(0,s.jsx)(n.td,{children:"Slow"}),(0,s.jsx)(n.td,{children:"High"}),(0,s.jsx)(n.td,{children:"Fallback for hard cases"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"YOLOv8-face"})}),(0,s.jsx)(n.td,{children:"Fast"}),(0,s.jsx)(n.td,{children:"Good"}),(0,s.jsx)(n.td,{children:"Quick preview"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"MediaPipe"})}),(0,s.jsx)(n.td,{children:"Very Fast"}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"Real-time applications"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"retinaface-implementation",children:"RetinaFace Implementation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from retinaface import RetinaFace\nimport cv2\nimport numpy as np\n\ndef detect_faces(image_path: str, threshold: float = 0.9):\n    \"\"\"\n    Detect all faces in an image with landmarks.\n\n    Returns list of face dictionaries with:\n    - bbox: [x1, y1, x2, y2]\n    - landmarks: 5-point facial landmarks\n    - confidence: detection confidence\n    \"\"\"\n    faces = RetinaFace.detect_faces(image_path, threshold=threshold)\n\n    if not isinstance(faces, dict):\n        return []\n\n    results = []\n    for face_id, face_data in faces.items():\n        results.append({\n            'bbox': face_data['facial_area'],  # [x1, y1, x2, y2]\n            'landmarks': face_data['landmarks'],  # 5 points\n            'confidence': face_data['score']\n        })\n\n    return results\n\ndef extract_aligned_face(\n    image: np.ndarray,\n    landmarks: dict,\n    output_size: tuple = (112, 112)\n) -> np.ndarray:\n    \"\"\"\n    Align face using 5-point landmarks for consistent embeddings.\n\n    Standard alignment targets (for 112x112):\n    - Left eye center: (38.29, 51.69)\n    - Right eye center: (73.53, 51.69)\n    - Nose tip: (56.02, 71.73)\n    - Left mouth: (41.54, 92.36)\n    - Right mouth: (70.72, 92.36)\n    \"\"\"\n    # Standard reference points\n    ref_pts = np.array([\n        [38.29, 51.69],   # left eye\n        [73.53, 51.69],   # right eye\n        [56.02, 71.73],   # nose\n        [41.54, 92.36],   # left mouth\n        [70.72, 92.36]    # right mouth\n    ], dtype=np.float32)\n\n    # Source points from detection\n    src_pts = np.array([\n        landmarks['left_eye'],\n        landmarks['right_eye'],\n        landmarks['nose'],\n        landmarks['mouth_left'],\n        landmarks['mouth_right']\n    ], dtype=np.float32)\n\n    # Compute similarity transform\n    transform = cv2.estimateAffinePartial2D(src_pts, ref_pts)[0]\n\n    # Apply transformation\n    aligned = cv2.warpAffine(\n        image, transform, output_size,\n        borderMode=cv2.BORDER_REPLICATE\n    )\n\n    return aligned\n"})}),"\n",(0,s.jsx)(n.h2,{id:"face-embedding--clustering",children:"Face Embedding & Clustering"}),"\n",(0,s.jsx)(n.h3,{id:"embedding-models",children:"Embedding Models"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{children:"Dimensions"}),(0,s.jsx)(n.th,{children:"Accuracy (LFW)"}),(0,s.jsx)(n.th,{children:"Speed"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"ArcFace"})}),(0,s.jsx)(n.td,{children:"512"}),(0,s.jsx)(n.td,{children:"99.83%"}),(0,s.jsx)(n.td,{children:"Fast"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"AdaFace"})}),(0,s.jsx)(n.td,{children:"512"}),(0,s.jsx)(n.td,{children:"99.82%"}),(0,s.jsx)(n.td,{children:"Fast"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"CosFace"})}),(0,s.jsx)(n.td,{children:"512"}),(0,s.jsx)(n.td,{children:"99.73%"}),(0,s.jsx)(n.td,{children:"Fast"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"FaceNet"})}),(0,s.jsx)(n.td,{children:"128/512"}),(0,s.jsx)(n.td,{children:"99.65%"}),(0,s.jsx)(n.td,{children:"Medium"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"arcface-embedding",children:"ArcFace Embedding"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\nfrom insightface.app import FaceAnalysis\n\nclass FaceEmbedder:\n    def __init__(self, model_name: str = 'buffalo_l'):\n        \"\"\"\n        Initialize face embedding model.\n\n        buffalo_l: ArcFace with ResNet100 backbone\n        buffalo_s: Lighter version for faster processing\n        \"\"\"\n        self.app = FaceAnalysis(\n            name=model_name,\n            providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n        )\n        self.app.prepare(ctx_id=0, det_size=(640, 640))\n\n    def get_embedding(self, image: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Get 512-dimensional face embedding.\n        \"\"\"\n        faces = self.app.get(image)\n        if len(faces) == 0:\n            return None\n\n        # Return embedding of largest face\n        largest_face = max(faces, key=lambda x: (x.bbox[2]-x.bbox[0]) * (x.bbox[3]-x.bbox[1]))\n        return largest_face.embedding\n\n    def get_all_embeddings(self, image: np.ndarray) -> list:\n        \"\"\"\n        Get embeddings for all faces in image.\n        \"\"\"\n        faces = self.app.get(image)\n        return [\n            {\n                'embedding': face.embedding,\n                'bbox': face.bbox.tolist(),\n                'landmarks': face.landmark_2d_106.tolist() if hasattr(face, 'landmark_2d_106') else None,\n                'age': face.age if hasattr(face, 'age') else None,\n                'gender': face.gender if hasattr(face, 'gender') else None\n            }\n            for face in faces\n        ]\n"})}),"\n",(0,s.jsx)(n.h3,{id:"hdbscan-clustering",children:"HDBSCAN Clustering"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import hdbscan\nfrom sklearn.preprocessing import normalize\nimport numpy as np\n\ndef cluster_faces(\n    embeddings: np.ndarray,\n    min_cluster_size: int = 3,\n    min_samples: int = 2,\n    cluster_selection_epsilon: float = 0.3\n):\n    """\n    Cluster face embeddings using HDBSCAN.\n\n    Why HDBSCAN over K-means?\n    - Doesn\'t require knowing number of guests in advance\n    - Handles noise (non-face detections, strangers)\n    - Works with varying cluster densities\n\n    Parameters tuned for wedding photos:\n    - min_cluster_size=3: At least 3 photos to be considered a "person"\n    - min_samples=2: Robust to outliers\n    - cluster_selection_epsilon=0.3: Allow some variation in embeddings\n    """\n    # Normalize embeddings to unit sphere (cosine similarity)\n    embeddings_norm = normalize(embeddings)\n\n    # Cluster\n    clusterer = hdbscan.HDBSCAN(\n        min_cluster_size=min_cluster_size,\n        min_samples=min_samples,\n        metric=\'euclidean\',  # On normalized vectors = cosine\n        cluster_selection_epsilon=cluster_selection_epsilon,\n        cluster_selection_method=\'eom\',  # Excess of mass\n        prediction_data=True  # For adding new faces later\n    )\n\n    labels = clusterer.fit_predict(embeddings_norm)\n\n    # Get cluster centers for each identity\n    unique_labels = set(labels) - {-1}  # -1 is noise\n    centers = {}\n    for label in unique_labels:\n        mask = labels == label\n        centers[label] = embeddings_norm[mask].mean(axis=0)\n\n    return labels, centers, clusterer\n\ndef assign_new_face(\n    embedding: np.ndarray,\n    clusterer: hdbscan.HDBSCAN,\n    threshold: float = 0.6\n):\n    """\n    Assign a new face to existing clusters.\n    Returns cluster label or -1 if no match.\n    """\n    embedding_norm = normalize(embedding.reshape(1, -1))\n\n    # Use approximate_predict for new points\n    label, strength = hdbscan.approximate_predict(clusterer, embedding_norm)\n\n    if strength[0] > threshold:\n        return label[0]\n    return -1\n'})}),"\n",(0,s.jsx)(n.h2,{id:"aesthetic-quality-scoring",children:"Aesthetic Quality Scoring"}),"\n",(0,s.jsx)(n.h3,{id:"multi-factor-scoring-model",children:"Multi-Factor Scoring Model"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import cv2\nimport numpy as np\nfrom dataclasses import dataclass\n\n@dataclass\nclass AestheticScore:\n    technical: float  # Sharpness, exposure, noise\n    composition: float  # Rule of thirds, framing\n    expression: float  # Smile, eyes open, genuine emotion\n    context: float  # Group inclusion, moment importance\n    overall: float  # Weighted combination\n\ndef calculate_aesthetic_score(\n    image: np.ndarray,\n    face_bbox: list,\n    face_landmarks: dict,\n    is_candid: bool = True\n) -> AestheticScore:\n    """\n    Calculate comprehensive aesthetic score for a face in a photo.\n    """\n\n    # 1. Technical Quality (25%)\n    technical = calculate_technical_score(image, face_bbox)\n\n    # 2. Composition (20%)\n    composition = calculate_composition_score(image, face_bbox)\n\n    # 3. Expression (35%)\n    expression = calculate_expression_score(image, face_landmarks)\n\n    # 4. Context (20%)\n    context = calculate_context_score(image, face_bbox, is_candid)\n\n    # Weighted combination\n    overall = (\n        0.25 * technical +\n        0.20 * composition +\n        0.35 * expression +\n        0.20 * context\n    )\n\n    return AestheticScore(\n        technical=technical,\n        composition=composition,\n        expression=expression,\n        context=context,\n        overall=overall\n    )\n\ndef calculate_technical_score(image: np.ndarray, bbox: list) -> float:\n    """\n    Score technical quality: sharpness, exposure, noise.\n    """\n    x1, y1, x2, y2 = [int(v) for v in bbox]\n    face_region = image[y1:y2, x1:x2]\n\n    # Sharpness via Laplacian variance\n    gray = cv2.cvtColor(face_region, cv2.COLOR_BGR2GRAY)\n    sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()\n    sharpness_score = min(1.0, sharpness / 500)  # Normalize\n\n    # Exposure via histogram analysis\n    hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n    hist = hist.flatten() / hist.sum()\n\n    # Penalize if too much in shadows (0-50) or highlights (200-255)\n    shadow_ratio = hist[:50].sum()\n    highlight_ratio = hist[200:].sum()\n    exposure_score = 1.0 - (shadow_ratio + highlight_ratio) * 0.5\n\n    # Noise estimation via median filter difference\n    denoised = cv2.medianBlur(gray, 3)\n    noise = np.abs(gray.astype(float) - denoised.astype(float)).mean()\n    noise_score = max(0, 1.0 - noise / 20)\n\n    return (sharpness_score + exposure_score + noise_score) / 3\n\ndef calculate_expression_score(image: np.ndarray, landmarks: dict) -> float:\n    """\n    Score facial expression quality.\n\n    Factors:\n    - Eye openness (blink detection)\n    - Smile detection (Duchenne marker)\n    - Gaze direction\n    - Overall expression quality\n    """\n    scores = []\n\n    # Eye openness\n    # Calculate eye aspect ratio (EAR)\n    left_eye = landmarks.get(\'left_eye\')\n    right_eye = landmarks.get(\'right_eye\')\n\n    if left_eye and right_eye:\n        # Simple EAR approximation\n        # Real implementation would use 6 points per eye\n        eye_openness = 0.8  # Placeholder\n        blink_penalty = 0.0 if eye_openness > 0.2 else 0.5\n        scores.append(1.0 - blink_penalty)\n\n    # Smile detection\n    mouth_left = landmarks.get(\'mouth_left\')\n    mouth_right = landmarks.get(\'mouth_right\')\n\n    if mouth_left and mouth_right:\n        # Mouth width relative to face width\n        mouth_width = np.linalg.norm(\n            np.array(mouth_right) - np.array(mouth_left)\n        )\n        # Wider smile = higher score (to a point)\n        smile_score = min(1.0, mouth_width / 50)\n        scores.append(smile_score)\n\n    # Gaze direction (looking at camera vs. away)\n    # For candids, looking away can be good\n    # For portraits, looking at camera is preferred\n    gaze_score = 0.7  # Placeholder\n    scores.append(gaze_score)\n\n    return np.mean(scores) if scores else 0.5\n\ndef calculate_composition_score(image: np.ndarray, bbox: list) -> float:\n    """\n    Score composition quality.\n    """\n    h, w = image.shape[:2]\n    x1, y1, x2, y2 = bbox\n    face_center_x = (x1 + x2) / 2 / w\n    face_center_y = (y1 + y2) / 2 / h\n\n    # Rule of thirds scoring\n    thirds_x = [1/3, 1/2, 2/3]\n    thirds_y = [1/3, 2/3]\n\n    min_dist_x = min(abs(face_center_x - t) for t in thirds_x)\n    min_dist_y = min(abs(face_center_y - t) for t in thirds_y)\n\n    thirds_score = 1.0 - (min_dist_x + min_dist_y)\n\n    # Face size (not too small, not too cropped)\n    face_area = (x2 - x1) * (y2 - y1)\n    image_area = w * h\n    face_ratio = face_area / image_area\n\n    # Optimal face ratio: 5-25% of image\n    if 0.05 <= face_ratio <= 0.25:\n        size_score = 1.0\n    elif face_ratio < 0.05:\n        size_score = face_ratio / 0.05\n    else:\n        size_score = max(0, 1.0 - (face_ratio - 0.25) * 2)\n\n    return (thirds_score + size_score) / 2\n'})}),"\n",(0,s.jsx)(n.h3,{id:"diversity-aware-selection",children:"Diversity-Aware Selection"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def select_best_photos_diverse(\n    cluster_photos: list,\n    n: int = 5,\n    diversity_threshold: float = 0.7\n) -> list:\n    \"\"\"\n    Select top N photos for a person with diversity constraint.\n\n    Avoids selecting N nearly-identical shots from the same moment.\n    Instead, picks best photo from each distinct moment/pose.\n    \"\"\"\n\n    # Score all photos\n    scored = []\n    for photo in cluster_photos:\n        score = calculate_aesthetic_score(\n            photo['image'],\n            photo['bbox'],\n            photo['landmarks']\n        )\n        scored.append({\n            **photo,\n            'aesthetic_score': score\n        })\n\n    # Sort by overall score\n    scored.sort(key=lambda x: x['aesthetic_score'].overall, reverse=True)\n\n    # Select with diversity constraint\n    selected = []\n    for candidate in scored:\n        if len(selected) >= n:\n            break\n\n        # Check diversity against already selected\n        is_diverse = True\n        for existing in selected:\n            similarity = compute_photo_similarity(\n                candidate['embedding'],\n                existing['embedding']\n            )\n            if similarity > diversity_threshold:\n                is_diverse = False\n                break\n\n        if is_diverse:\n            selected.append(candidate)\n\n    # If we couldn't get N diverse photos, fill with best remaining\n    if len(selected) < n:\n        for candidate in scored:\n            if candidate not in selected:\n                selected.append(candidate)\n            if len(selected) >= n:\n                break\n\n    return selected\n\ndef compute_photo_similarity(emb1: np.ndarray, emb2: np.ndarray) -> float:\n    \"\"\"\n    Compute similarity between two photo embeddings.\n    Uses face embedding + pose + timestamp proximity.\n    \"\"\"\n    # Cosine similarity of face embeddings\n    face_sim = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n\n    return face_sim\n"})}),"\n",(0,s.jsx)(n.h2,{id:"identity-linking-workflow",children:"Identity Linking Workflow"}),"\n",(0,s.jsx)(n.h3,{id:"priority-based-naming",children:"Priority-Based Naming"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"IDENTITY_PRIORITY = [\n    ('couple', ['bride', 'groom', 'spouse_1', 'spouse_2']),\n    ('wedding_party', ['best_man', 'maid_of_honor', 'bridesmaid', 'groomsman']),\n    ('parents', ['mother_bride', 'father_bride', 'mother_groom', 'father_groom']),\n    ('grandparents', ['grandmother', 'grandfather']),\n    ('siblings', ['sister', 'brother']),\n    ('extended_family', ['aunt', 'uncle', 'cousin']),\n    ('friends', []),\n    ('vendors', ['photographer', 'dj', 'coordinator']),\n]\n\ndef link_identities(\n    clusters: dict,\n    seed_identities: dict,  # User-provided: {cluster_id: \"Aunt Martha\"}\n    guest_list: list = None  # Optional: [\"Aunt Martha\", \"Uncle Bob\", ...]\n) -> dict:\n    \"\"\"\n    Link cluster IDs to human-readable names.\n\n    Workflow:\n    1. User tags couple in 2-3 photos \u2192 seeds those clusters\n    2. User optionally tags wedding party\n    3. System propagates through all photos\n    4. Remaining clusters get generic names or guest list matching\n    \"\"\"\n\n    identity_map = {}\n\n    # Start with user-provided seeds\n    for cluster_id, name in seed_identities.items():\n        identity_map[cluster_id] = {\n            'name': name,\n            'confidence': 1.0,\n            'source': 'user_tagged'\n        }\n\n    # Remaining clusters\n    unnamed_clusters = set(clusters.keys()) - set(identity_map.keys())\n\n    for i, cluster_id in enumerate(unnamed_clusters):\n        cluster_data = clusters[cluster_id]\n\n        # Try to match with guest list using any available signals\n        if guest_list:\n            # Could use location proximity to tagged people, etc.\n            pass\n\n        # Fallback to generic naming\n        identity_map[cluster_id] = {\n            'name': f\"Guest {i + 1}\",\n            'confidence': 0.5,\n            'source': 'auto_assigned'\n        }\n\n    return identity_map\n"})}),"\n",(0,s.jsx)(n.h2,{id:"output-format",children:"Output Format"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n  "wedding_id": "smith-jones-2024",\n  "processed_date": "2024-12-15T10:30:00Z",\n  "total_photos": 3847,\n  "total_faces_detected": 12453,\n  "unique_identities": 127,\n\n  "identities": [\n    {\n      "cluster_id": 0,\n      "name": "Alex Smith",\n      "role": "spouse_1",\n      "photo_count": 487,\n      "best_photos": [\n        {\n          "photo_id": "IMG_2847.jpg",\n          "score": 0.94,\n          "scores": {\n            "technical": 0.91,\n            "composition": 0.88,\n            "expression": 0.98,\n            "context": 0.95\n          },\n          "moment": "first_dance",\n          "timestamp": "2024-11-15T20:45:00Z"\n        }\n      ],\n      "thumbnail": "clusters/0/thumbnail.jpg"\n    }\n  ]\n}\n'})})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);