"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[26321],{28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>s});var r=t(96540);const a={},i=r.createContext(a);function o(e){const n=r.useContext(i);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),r.createElement(i.Provider,{value:n},e.children)}},29059:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>o,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"skills/drone_inspection_specialist/references/fire-detection","title":"Forest Fire Detection Reference","description":"Multi-Modal Fire Detection Pipeline","source":"@site/docs/skills/drone_inspection_specialist/references/fire-detection.md","sourceDirName":"skills/drone_inspection_specialist/references","slug":"/skills/drone_inspection_specialist/references/fire-detection","permalink":"/docs/skills/drone_inspection_specialist/references/fire-detection","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Forest Fire Detection Reference","sidebar_label":"Forest Fire Detection Refer...","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Drone Inspection Specialist","permalink":"/docs/skills/drone_inspection_specialist/"},"next":{"title":"Gaussian Splatting 3D Recon...","permalink":"/docs/skills/drone_inspection_specialist/references/gaussian-splatting-3d"}}');var a=t(74848),i=t(28453);const o={title:"Forest Fire Detection Reference",sidebar_label:"Forest Fire Detection Refer...",sidebar_position:1},s="Forest Fire Detection Reference",l={},d=[{value:"Multi-Modal Fire Detection Pipeline",id:"multi-modal-fire-detection-pipeline",level:2},{value:"Thermal Camera Integration",id:"thermal-camera-integration",level:2},{value:"Fire Progression Tracking",id:"fire-progression-tracking",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",pre:"pre",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"forest-fire-detection-reference",children:"Forest Fire Detection Reference"})}),"\n",(0,a.jsx)(n.h2,{id:"multi-modal-fire-detection-pipeline",children:"Multi-Modal Fire Detection Pipeline"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import cv2\nimport numpy as np\nimport torch\nfrom ultralytics import YOLO\nfrom typing import List, Dict, Tuple, Optional\nfrom dataclasses import dataclass\nimport time\n\n@dataclass\nclass FireAlert:\n    alert_type: str  # 'CONFIRMED_FIRE', 'HOTSPOT', 'SMOKE'\n    confidence: float\n    bbox: Tuple[int, int, int, int]\n    gps_coords: Optional[Tuple[float, float]] = None\n    temperature: Optional[float] = None\n    timestamp: float = 0.0\n    priority: str = 'MEDIUM'\n\n\nclass ForestFireDetector:\n    \"\"\"\n    Multi-modal fire detection combining RGB and thermal imagery.\n    Designed for real-time drone deployment.\n    \"\"\"\n    def __init__(self,\n                 thermal_model_path: str = 'fire_thermal_yolov8.pt',\n                 smoke_model_path: str = 'smoke_detection_yolov8.pt',\n                 device: str = 'cuda'):\n        # Thermal camera fire detection\n        self.thermal_model = YOLO(thermal_model_path)\n        self.thermal_model.to(device)\n\n        # RGB smoke detection\n        self.smoke_model = YOLO(smoke_model_path)\n        self.smoke_model.to(device)\n\n        # Temperature thresholds\n        self.temp_threshold = 60  # Celsius - potential hotspot\n        self.fire_threshold = 150  # Celsius - confirmed fire\n        self.alert_threshold = 0.7\n\n        # Alert cooldown to prevent spam\n        self.last_alert_time = {}\n        self.alert_cooldown = 5.0  # seconds\n\n    def process_frame(self, rgb_frame: np.ndarray,\n                      thermal_frame: np.ndarray) -> List[FireAlert]:\n        \"\"\"\n        Process both RGB and thermal frames simultaneously.\n\n        Args:\n            rgb_frame: BGR image from visible camera\n            thermal_frame: Temperature array from thermal camera (Celsius)\n\n        Returns:\n            List of fire alerts with location and confidence\n        \"\"\"\n        alerts = []\n\n        # Smoke detection in RGB\n        smoke_dets = self._detect_smoke(rgb_frame)\n\n        # Hotspot detection in thermal\n        hotspots = self._detect_hotspots(thermal_frame)\n\n        # Fire detection using thermal model\n        fire_dets = self._detect_thermal_fire(thermal_frame)\n\n        # Multi-modal fusion\n        alerts = self._fuse_detections(smoke_dets, fire_dets, hotspots)\n\n        return alerts\n\n    def _detect_smoke(self, rgb_frame: np.ndarray) -> List[Dict]:\n        \"\"\"Detect smoke plumes in RGB image\"\"\"\n        results = self.smoke_model(rgb_frame, conf=0.5, verbose=False)[0]\n\n        detections = []\n        for box in results.boxes:\n            det = {\n                'bbox': tuple(map(int, box.xyxy[0].cpu().numpy())),\n                'confidence': float(box.conf[0].item()),\n                'class': 'smoke'\n            }\n            detections.append(det)\n\n        return detections\n\n    def _detect_hotspots(self, thermal_frame: np.ndarray) -> List[Dict]:\n        \"\"\"Detect temperature anomalies in thermal image\"\"\"\n        # Create binary mask for hot regions\n        hot_mask = (thermal_frame > self.temp_threshold).astype(np.uint8)\n\n        # Find contours\n        contours, _ = cv2.findContours(\n            hot_mask,\n            cv2.RETR_EXTERNAL,\n            cv2.CHAIN_APPROX_SIMPLE\n        )\n\n        hotspots = []\n        for contour in contours:\n            area = cv2.contourArea(contour)\n            if area < 100:  # Skip tiny regions (noise)\n                continue\n\n            x, y, w, h = cv2.boundingRect(contour)\n            region = thermal_frame[y:y+h, x:x+w]\n\n            hotspots.append({\n                'bbox': (x, y, x+w, y+h),\n                'max_temp': float(np.max(region)),\n                'mean_temp': float(np.mean(region)),\n                'area': area,\n                'class': 'hotspot'\n            })\n\n        return hotspots\n\n    def _detect_thermal_fire(self, thermal_frame: np.ndarray) -> List[Dict]:\n        \"\"\"Run fire detection model on thermal image\"\"\"\n        # Normalize for model input\n        thermal_normalized = self._normalize_thermal(thermal_frame)\n\n        results = self.thermal_model(thermal_normalized, conf=0.6, verbose=False)[0]\n\n        detections = []\n        for box in results.boxes:\n            x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n\n            # Get actual temperature from thermal data\n            region = thermal_frame[y1:y2, x1:x2]\n            max_temp = float(np.max(region)) if region.size > 0 else 0\n\n            det = {\n                'bbox': (x1, y1, x2, y2),\n                'confidence': float(box.conf[0].item()),\n                'max_temp': max_temp,\n                'class': 'fire'\n            }\n            detections.append(det)\n\n        return detections\n\n    def _normalize_thermal(self, thermal: np.ndarray) -> np.ndarray:\n        \"\"\"Normalize thermal data to 0-255 for model input\"\"\"\n        # Clip to expected range\n        thermal_clipped = np.clip(thermal, -20, 300)\n        # Normalize\n        normalized = ((thermal_clipped + 20) / 320 * 255).astype(np.uint8)\n        # Convert to 3-channel for YOLO\n        return cv2.cvtColor(normalized, cv2.COLOR_GRAY2BGR)\n\n    def _fuse_detections(self, smoke: List[Dict], fire: List[Dict],\n                         hotspots: List[Dict]) -> List[FireAlert]:\n        \"\"\"\n        Multi-modal fusion for high-confidence fire alerts.\n        Priority: Fire+Smoke > Fire-only > Hotspot\n        \"\"\"\n        alerts = []\n        current_time = time.time()\n\n        # High priority: Thermal fire detection + RGB smoke\n        for fire_det in fire:\n            for smoke_det in smoke:\n                iou = self._calculate_iou(fire_det['bbox'], smoke_det['bbox'])\n                if iou > 0.2:  # Overlapping detections\n                    combined_conf = (fire_det['confidence'] + smoke_det['confidence']) / 2\n                    alert = FireAlert(\n                        alert_type='CONFIRMED_FIRE',\n                        confidence=min(0.98, combined_conf + 0.2),  # Boost for multi-modal\n                        bbox=fire_det['bbox'],\n                        temperature=fire_det.get('max_temp'),\n                        timestamp=current_time,\n                        priority='CRITICAL'\n                    )\n                    alerts.append(alert)\n\n        # Medium priority: Thermal fire without smoke\n        for fire_det in fire:\n            # Skip if already in confirmed fire\n            already_confirmed = any(\n                self._calculate_iou(fire_det['bbox'], a.bbox) > 0.5\n                for a in alerts if a.alert_type == 'CONFIRMED_FIRE'\n            )\n            if not already_confirmed:\n                alert = FireAlert(\n                    alert_type='FIRE_THERMAL',\n                    confidence=fire_det['confidence'],\n                    bbox=fire_det['bbox'],\n                    temperature=fire_det.get('max_temp'),\n                    timestamp=current_time,\n                    priority='HIGH'\n                )\n                alerts.append(alert)\n\n        # Lower priority: Hotspots above fire threshold\n        for hotspot in hotspots:\n            if hotspot['max_temp'] > self.fire_threshold:\n                alert = FireAlert(\n                    alert_type='HOTSPOT_CRITICAL',\n                    confidence=0.8,\n                    bbox=hotspot['bbox'],\n                    temperature=hotspot['max_temp'],\n                    timestamp=current_time,\n                    priority='HIGH'\n                )\n                alerts.append(alert)\n            elif hotspot['max_temp'] > 80:  # Significant but not fire\n                alert = FireAlert(\n                    alert_type='HOTSPOT',\n                    confidence=0.6,\n                    bbox=hotspot['bbox'],\n                    temperature=hotspot['max_temp'],\n                    timestamp=current_time,\n                    priority='MEDIUM'\n                )\n                alerts.append(alert)\n\n        # Smoke-only alerts (could be early fire)\n        for smoke_det in smoke:\n            already_reported = any(\n                self._calculate_iou(smoke_det['bbox'], a.bbox) > 0.3\n                for a in alerts\n            )\n            if not already_reported:\n                alert = FireAlert(\n                    alert_type='SMOKE',\n                    confidence=smoke_det['confidence'],\n                    bbox=smoke_det['bbox'],\n                    timestamp=current_time,\n                    priority='MEDIUM'\n                )\n                alerts.append(alert)\n\n        return alerts\n\n    def _calculate_iou(self, box1: Tuple, box2: Tuple) -> float:\n        \"\"\"Calculate Intersection over Union\"\"\"\n        x1 = max(box1[0], box2[0])\n        y1 = max(box1[1], box2[1])\n        x2 = min(box1[2], box2[2])\n        y2 = min(box1[3], box2[3])\n\n        intersection = max(0, x2 - x1) * max(0, y2 - y1)\n        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n        union = area1 + area2 - intersection\n\n        return intersection / union if union > 0 else 0\n\n\nclass FireAlertSystem:\n    \"\"\"\n    Georeferencing and dispatch system for fire alerts.\n    \"\"\"\n    def __init__(self, camera_fov: Tuple[float, float] = (84, 58)):\n        self.fov_h, self.fov_v = camera_fov  # degrees\n        self.alert_history = []\n        self.dispatch_endpoint = None\n\n    def generate_georeferenced_alert(self, alert: FireAlert,\n                                     drone_lat: float, drone_lon: float,\n                                     drone_alt: float, gimbal_pitch: float,\n                                     drone_heading: float,\n                                     frame_size: Tuple[int, int] = (1920, 1080)) -> FireAlert:\n        \"\"\"\n        Convert image-space detection to GPS coordinates.\n\n        Args:\n            alert: Fire alert with bbox in image coordinates\n            drone_lat, drone_lon: Drone GPS position\n            drone_alt: Altitude AGL in meters\n            gimbal_pitch: Camera pitch angle (negative = looking down)\n            drone_heading: Drone heading in degrees (0 = North)\n            frame_size: Image dimensions (width, height)\n\n        Returns:\n            Alert with GPS coordinates added\n        \"\"\"\n        img_w, img_h = frame_size\n        x1, y1, x2, y2 = alert.bbox\n\n        # Center of detection\n        center_x = (x1 + x2) / 2\n        center_y = (y1 + y2) / 2\n\n        # Angle from image center\n        angle_x = (center_x - img_w/2) / img_w * self.fov_h\n        angle_y = (center_y - img_h/2) / img_h * self.fov_v\n\n        # Adjust for gimbal pitch\n        effective_pitch = gimbal_pitch + angle_y\n\n        # Ground distance calculation\n        if effective_pitch >= -5:  # Looking too horizontal\n            return alert  # Can't estimate ground position\n\n        ground_dist = drone_alt / np.tan(np.radians(-effective_pitch))\n\n        # Horizontal offset from drone nadir\n        ground_offset_x = ground_dist * np.tan(np.radians(angle_x))\n\n        # Convert to North-East offsets (accounting for heading)\n        heading_rad = np.radians(drone_heading)\n        north_offset = ground_dist * np.cos(heading_rad) - ground_offset_x * np.sin(heading_rad)\n        east_offset = ground_dist * np.sin(heading_rad) + ground_offset_x * np.cos(heading_rad)\n\n        # Convert to GPS (rough approximation)\n        # 1 degree latitude \u2248 111km\n        # 1 degree longitude \u2248 111km * cos(latitude)\n        dlat = north_offset / 111000\n        dlon = east_offset / (111000 * np.cos(np.radians(drone_lat)))\n\n        alert.gps_coords = (drone_lat + dlat, drone_lon + dlon)\n        return alert\n\n    def send_to_dispatch(self, alert: FireAlert):\n        \"\"\"Send alert to fire dispatch system\"\"\"\n        payload = {\n            'type': alert.alert_type,\n            'priority': alert.priority,\n            'confidence': alert.confidence,\n            'location': {\n                'lat': alert.gps_coords[0] if alert.gps_coords else None,\n                'lon': alert.gps_coords[1] if alert.gps_coords else None\n            },\n            'temperature': alert.temperature,\n            'timestamp': alert.timestamp\n        }\n\n        # Log for later\n        self.alert_history.append(payload)\n\n        # Send to dispatch (implementation depends on system)\n        if self.dispatch_endpoint:\n            import requests\n            requests.post(self.dispatch_endpoint, json=payload)\n\n        return payload\n"})}),"\n",(0,a.jsx)(n.h2,{id:"thermal-camera-integration",children:"Thermal Camera Integration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom typing import Tuple, Optional\nimport struct\n\nclass ThermalCameraInterface:\n    """\n    Interface for FLIR thermal cameras commonly used on drones.\n    Supports radiometric temperature extraction.\n    """\n    def __init__(self, emissivity: float = 0.95,\n                 reflected_temp: float = 25.0,\n                 atmospheric_temp: float = 25.0,\n                 distance: float = 50.0):\n        self.emissivity = emissivity\n        self.reflected_temp = reflected_temp\n        self.atmospheric_temp = atmospheric_temp\n        self.distance = distance\n\n        # Planck constants for FLIR cameras (typical values)\n        self.planck_r1 = 21106.77\n        self.planck_r2 = 0.012545258\n        self.planck_b = 1501.0\n        self.planck_f = 1.0\n        self.planck_o = -7340\n\n    def raw_to_temperature(self, raw_value: int) -> float:\n        """\n        Convert raw radiometric value to temperature in Celsius.\n        Uses FLIR\'s planck equation.\n        """\n        # Atmospheric correction\n        tau = self._atmospheric_transmission()\n\n        # Object signal\n        raw_obj = (raw_value - self.planck_o) / self.emissivity / tau\n\n        # Correct for reflected temperature\n        raw_refl = self.planck_r1 / (self.planck_r2 * (np.exp(self.planck_b / (self.reflected_temp + 273.15)) - self.planck_f)) - self.planck_o\n        raw_obj -= (1 - self.emissivity) * raw_refl / self.emissivity\n\n        # Convert to temperature\n        temp_k = self.planck_b / np.log(self.planck_r1 / (self.planck_r2 * (raw_obj + self.planck_o)) + self.planck_f)\n        return temp_k - 273.15\n\n    def _atmospheric_transmission(self) -> float:\n        """Calculate atmospheric transmission based on distance"""\n        # Simplified model - real implementation uses humidity, etc.\n        alpha = 0.006\n        return np.exp(-alpha * self.distance)\n\n    def process_thermal_frame(self, raw_frame: np.ndarray) -> np.ndarray:\n        """Convert entire raw frame to temperature array"""\n        # Vectorized conversion\n        tau = self._atmospheric_transmission()\n        raw_obj = (raw_frame.astype(np.float64) - self.planck_o) / self.emissivity / tau\n\n        raw_refl = self.planck_r1 / (self.planck_r2 * (np.exp(self.planck_b / (self.reflected_temp + 273.15)) - self.planck_f)) - self.planck_o\n        raw_obj -= (1 - self.emissivity) * raw_refl / self.emissivity\n\n        temp_k = self.planck_b / np.log(self.planck_r1 / (self.planck_r2 * (raw_obj + self.planck_o)) + self.planck_f)\n        return temp_k - 273.15\n\n\nclass FLIRBosonInterface(ThermalCameraInterface):\n    """Specific interface for FLIR Boson cameras"""\n    def __init__(self, serial_port: str = \'/dev/ttyUSB0\'):\n        super().__init__()\n        self.serial_port = serial_port\n        self.frame_width = 640\n        self.frame_height = 512\n\n    def capture_frame(self) -> Tuple[np.ndarray, np.ndarray]:\n        """\n        Capture both radiometric and visual thermal frame.\n\n        Returns:\n            temperature_frame: Float array of temperatures in Celsius\n            visual_frame: 8-bit colorized frame for display\n        """\n        # Implementation depends on specific camera SDK\n        # This is a template for the interface\n        raw_frame = self._read_raw_frame()\n        temp_frame = self.process_thermal_frame(raw_frame)\n        visual_frame = self._colorize(temp_frame)\n        return temp_frame, visual_frame\n\n    def _colorize(self, temp_frame: np.ndarray,\n                  vmin: float = -20, vmax: float = 150) -> np.ndarray:\n        """Apply colormap to temperature data"""\n        import cv2\n        normalized = np.clip((temp_frame - vmin) / (vmax - vmin), 0, 1)\n        normalized = (normalized * 255).astype(np.uint8)\n        return cv2.applyColorMap(normalized, cv2.COLORMAP_INFERNO)\n\n    def _read_raw_frame(self) -> np.ndarray:\n        """Read raw frame from camera - implementation specific"""\n        # Placeholder - actual implementation uses camera SDK\n        return np.zeros((self.frame_height, self.frame_width), dtype=np.uint16)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"fire-progression-tracking",children:"Fire Progression Tracking"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import numpy as np\nfrom typing import List, Dict, Tuple\nfrom collections import deque\nimport time\n\nclass FireProgressionTracker:\n    \"\"\"\n    Track fire spread over time using consecutive detections.\n    Estimates spread rate and direction.\n    \"\"\"\n    def __init__(self, history_length: int = 100):\n        self.detection_history: deque = deque(maxlen=history_length)\n        self.fire_zones: Dict[str, List[Dict]] = {}\n\n    def update(self, alerts: List, timestamp: float = None) -> Dict:\n        \"\"\"\n        Update tracker with new alerts.\n\n        Returns:\n            Analysis dict with spread rate, direction, and predictions\n        \"\"\"\n        timestamp = timestamp or time.time()\n\n        # Record current detections\n        current_state = {\n            'timestamp': timestamp,\n            'alerts': alerts,\n            'total_area': sum(self._bbox_area(a.bbox) for a in alerts)\n        }\n        self.detection_history.append(current_state)\n\n        if len(self.detection_history) < 2:\n            return {'status': 'insufficient_data'}\n\n        # Analyze progression\n        analysis = self._analyze_progression()\n        return analysis\n\n    def _analyze_progression(self) -> Dict:\n        \"\"\"Analyze fire spread from history\"\"\"\n        recent = list(self.detection_history)[-10:]\n\n        if len(recent) < 2:\n            return {'status': 'insufficient_data'}\n\n        # Area change rate\n        areas = [s['total_area'] for s in recent]\n        timestamps = [s['timestamp'] for s in recent]\n\n        dt = timestamps[-1] - timestamps[0]\n        if dt > 0:\n            area_rate = (areas[-1] - areas[0]) / dt  # pixels^2/second\n        else:\n            area_rate = 0\n\n        # Centroid movement\n        centroids = []\n        for state in recent:\n            if state['alerts']:\n                cx = np.mean([self._bbox_center(a.bbox)[0] for a in state['alerts']])\n                cy = np.mean([self._bbox_center(a.bbox)[1] for a in state['alerts']])\n                centroids.append((cx, cy, state['timestamp']))\n\n        spread_direction = None\n        spread_speed = 0\n\n        if len(centroids) >= 2:\n            dx = centroids[-1][0] - centroids[0][0]\n            dy = centroids[-1][1] - centroids[0][1]\n            dt = centroids[-1][2] - centroids[0][2]\n\n            if dt > 0:\n                spread_speed = np.sqrt(dx**2 + dy**2) / dt\n                spread_direction = np.degrees(np.arctan2(dy, dx))\n\n        return {\n            'status': 'tracking',\n            'area_change_rate': area_rate,\n            'spread_speed_pixels': spread_speed,\n            'spread_direction_degrees': spread_direction,\n            'trend': 'growing' if area_rate > 100 else 'stable' if area_rate > -100 else 'shrinking',\n            'num_observations': len(recent)\n        }\n\n    def _bbox_area(self, bbox: Tuple) -> float:\n        return (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\n\n    def _bbox_center(self, bbox: Tuple) -> Tuple[float, float]:\n        return ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)\n\n    def predict_spread(self, time_horizon: float = 300) -> Dict:\n        \"\"\"\n        Predict fire position in `time_horizon` seconds.\n\n        Returns:\n            Predicted fire extent and confidence\n        \"\"\"\n        analysis = self._analyze_progression()\n\n        if analysis['status'] != 'tracking':\n            return {'status': 'cannot_predict'}\n\n        if self.detection_history:\n            last_alerts = self.detection_history[-1]['alerts']\n            current_centroids = [self._bbox_center(a.bbox) for a in last_alerts]\n\n            if current_centroids and analysis['spread_direction_degrees'] is not None:\n                # Simple linear extrapolation\n                speed = analysis['spread_speed_pixels']\n                direction = np.radians(analysis['spread_direction_degrees'])\n\n                predicted_dx = speed * time_horizon * np.cos(direction)\n                predicted_dy = speed * time_horizon * np.sin(direction)\n\n                avg_centroid = np.mean(current_centroids, axis=0)\n                predicted_centroid = (\n                    avg_centroid[0] + predicted_dx,\n                    avg_centroid[1] + predicted_dy\n                )\n\n                return {\n                    'status': 'predicted',\n                    'current_centroid': tuple(avg_centroid),\n                    'predicted_centroid': predicted_centroid,\n                    'time_horizon_seconds': time_horizon,\n                    'confidence': 0.7 if speed > 0 else 0.3\n                }\n\n        return {'status': 'cannot_predict'}\n"})})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}}}]);