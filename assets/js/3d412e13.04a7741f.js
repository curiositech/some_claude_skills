"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[72660],{28453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>h});var i=s(96540);const t={},a=i.createContext(t);function r(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function h(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(a.Provider,{value:n},e.children)}},32457:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>h,default:()=>c,frontMatter:()=>r,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"skills/photo_content_recognition_curation_expert/references/perceptual-hashing","title":"Perceptual Hashing Implementation Reference","description":"Overview","source":"@site/docs/skills/photo_content_recognition_curation_expert/references/perceptual-hashing.md","sourceDirName":"skills/photo_content_recognition_curation_expert/references","slug":"/skills/photo_content_recognition_curation_expert/references/perceptual-hashing","permalink":"/docs/skills/photo_content_recognition_curation_expert/references/perceptual-hashing","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Perceptual Hashing Implementation Reference","sidebar_label":"Perceptual Hashing Implemen...","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Face Recognition & Clusteri...","permalink":"/docs/skills/photo_content_recognition_curation_expert/references/face-clustering"},"next":{"title":"Photo Indexing Pipeline Ref...","permalink":"/docs/skills/photo_content_recognition_curation_expert/references/photo-indexing"}}');var t=s(74848),a=s(28453);const r={title:"Perceptual Hashing Implementation Reference",sidebar_label:"Perceptual Hashing Implemen...",sidebar_position:3},h="Perceptual Hashing Implementation Reference",o={},d=[{value:"Overview",id:"overview",level:2},{value:"DINOHash (2025 State-of-the-Art)",id:"dinohash-2025-state-of-the-art",level:2},{value:"Classical Perceptual Hashing",id:"classical-perceptual-hashing",level:2},{value:"dHash (Difference Hash) - Fastest",id:"dhash-difference-hash---fastest",level:3},{value:"pHash (Perceptual Hash) - More Robust",id:"phash-perceptual-hash---more-robust",level:3},{value:"Hybrid Duplicate Detection Pipeline",id:"hybrid-duplicate-detection-pipeline",level:2},{value:"BK-Tree for Efficient Search (100K+ Photos)",id:"bk-tree-for-efficient-search-100k-photos",level:2},{value:"Performance",id:"performance",level:2},{value:"Method Comparison",id:"method-comparison",level:2},{value:"References",id:"references",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"perceptual-hashing-implementation-reference",children:"Perceptual Hashing Implementation Reference"})}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"Perceptual hashing generates similar hash values for visually similar images, enabling near-duplicate detection."}),"\n",(0,t.jsx)(n.h2,{id:"dinohash-2025-state-of-the-art",children:"DINOHash (2025 State-of-the-Art)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Breakthrough:"})," Adversarially fine-tuned self-supervised DINOv2 features."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Advantages:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Higher bit-accuracy under heavy crops"}),"\n",(0,t.jsx)(n.li,{children:"Robust to compression artifacts"}),"\n",(0,t.jsx)(n.li,{children:"Resilient to adversarial attacks"}),"\n",(0,t.jsx)(n.li,{children:"Outperforms classical DCT-DWT schemes and NeuralHash"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import torch\nfrom transformers import AutoModel, AutoImageProcessor\n\nclass DINOHasher:\n    """\n    DINOHash: State-of-the-art perceptual hashing using DINOv2.\n    Based on: "DINOHash: Adversarially Fine-Tuned DINOv2 Features" (2025)\n    """\n\n    def __init__(self):\n        self.model = AutoModel.from_pretrained(\'facebook/dinov2-base\')\n        self.processor = AutoImageProcessor.from_pretrained(\'facebook/dinov2-base\')\n        self.model.eval()\n        self.hash_bits = 128\n\n    def compute_hash(self, image):\n        """Compute perceptual hash from image."""\n        inputs = self.processor(images=image, return_tensors="pt")\n\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            features = outputs.last_hidden_state[:, 0]  # CLS token\n\n        features_reduced = self.project_to_hash_space(features)\n        hash_binary = (features_reduced > 0).cpu().numpy().astype(np.uint8)\n        return hash_binary.flatten()\n\n    def project_to_hash_space(self, features):\n        """Project high-dimensional features to hash space."""\n        if not hasattr(self, \'projection_matrix\'):\n            self.projection_matrix = torch.randn(\n                features.shape[-1], self.hash_bits\n            ) / np.sqrt(features.shape[-1])\n        return features @ self.projection_matrix\n\n    def hamming_distance(self, hash1, hash2):\n        """Compute Hamming distance between two hashes."""\n        return np.sum(hash1 != hash2)\n\n    def are_duplicates(self, hash1, hash2, threshold=5):\n        """Check if two hashes represent near-duplicates."""\n        return self.hamming_distance(hash1, hash2) <= threshold\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"classical-perceptual-hashing",children:"Classical Perceptual Hashing"}),"\n",(0,t.jsx)(n.h3,{id:"dhash-difference-hash---fastest",children:"dHash (Difference Hash) - Fastest"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from PIL import Image\nimport numpy as np\n\ndef compute_dhash(image, hash_size=8):\n    """\n    Compute dHash (Difference Hash).\n    Fast, good for exact duplicates and minor edits.\n    """\n    image = image.convert(\'L\')\n    image = image.resize((hash_size + 1, hash_size), Image.LANCZOS)\n    pixels = np.array(image)\n\n    diff = pixels[:, 1:] > pixels[:, :-1]\n\n    hash_value = 0\n    for bit in diff.flatten():\n        hash_value = (hash_value << 1) | int(bit)\n    return hash_value\n\n\ndef dhash_hamming_distance(hash1, hash2):\n    """Hamming distance between two dHashes."""\n    return bin(hash1 ^ hash2).count(\'1\')\n'})}),"\n",(0,t.jsx)(n.h3,{id:"phash-perceptual-hash---more-robust",children:"pHash (Perceptual Hash) - More Robust"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import cv2\nfrom scipy.fftpack import dct\n\ndef compute_phash(image, hash_size=8):\n    """\n    Compute pHash using DCT.\n    Better for near-duplicates with brightness/contrast changes.\n    """\n    image = image.convert(\'L\')\n    image = image.resize((hash_size * 4, hash_size * 4), Image.LANCZOS)\n    pixels = np.array(image, dtype=np.float32)\n\n    dct_coeff = dct(dct(pixels.T).T)\n    dct_low = dct_coeff[:hash_size, :hash_size]\n    median = np.median(dct_low)\n    hash_binary = dct_low > median\n\n    hash_value = 0\n    for bit in hash_binary.flatten():\n        hash_value = (hash_value << 1) | int(bit)\n    return hash_value\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"hybrid-duplicate-detection-pipeline",children:"Hybrid Duplicate Detection Pipeline"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Strategy:"})," Use fast classical hashing for filtering, deep learning for refinement."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class HybridDuplicateDetector:\n    """\n    Hybrid near-duplicate detection pipeline.\n    Stage 1: Fast pHash filtering (eliminates obvious non-duplicates)\n    Stage 2: DINOHash refinement (accurate near-duplicate detection)\n    Stage 3: Siamese ViT verification (final confirmation)\n    """\n\n    def __init__(self):\n        self.phash_index = {}\n        self.dinohash_index = {}\n        self.dino_hasher = DINOHasher()\n\n    def add_photo(self, photo_id, image):\n        """Add photo to index."""\n        self.phash_index[photo_id] = compute_phash(image)\n        self.dinohash_index[photo_id] = self.dino_hasher.compute_hash(image)\n\n    def find_duplicates(self, aggressive=False):\n        """Find all near-duplicate groups."""\n        # Stage 1: Fast pHash pre-filtering\n        phash_candidates = []\n        photo_ids = list(self.phash_index.keys())\n\n        for i in range(len(photo_ids)):\n            for j in range(i + 1, len(photo_ids)):\n                id1, id2 = photo_ids[i], photo_ids[j]\n                distance = bin(self.phash_index[id1] ^ self.phash_index[id2]).count(\'1\')\n                if distance <= (10 if aggressive else 5):\n                    phash_candidates.append((id1, id2, distance))\n\n        # Stage 2: DINOHash refinement\n        dino_duplicates = []\n        for id1, id2, phash_dist in phash_candidates:\n            dino_distance = self.dino_hasher.hamming_distance(\n                self.dinohash_index[id1], self.dinohash_index[id2]\n            )\n            if dino_distance <= (10 if aggressive else 5):\n                dino_duplicates.append((id1, id2, dino_distance))\n\n        return self.cluster_duplicates(dino_duplicates)\n\n    def cluster_duplicates(self, duplicate_pairs):\n        """Cluster duplicate pairs into groups using union-find."""\n        parent = {}\n\n        def find(x):\n            if x not in parent:\n                parent[x] = x\n            if parent[x] != x:\n                parent[x] = find(parent[x])\n            return parent[x]\n\n        def union(x, y):\n            root_x, root_y = find(x), find(y)\n            if root_x != root_y:\n                parent[root_x] = root_y\n\n        for id1, id2, _ in duplicate_pairs:\n            union(id1, id2)\n\n        groups = {}\n        for photo_id in set(id for pair in duplicate_pairs for id in pair[:2]):\n            root = find(photo_id)\n            groups.setdefault(root, []).append(photo_id)\n\n        return list(groups.values())\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"bk-tree-for-efficient-search-100k-photos",children:"BK-Tree for Efficient Search (100K+ Photos)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class BKTree:\n    """\n    Burkhard-Keller tree for efficient Hamming distance search.\n    Enables O(log N) average-case search for perceptual hashes.\n    """\n\n    class Node:\n        def __init__(self, hash_value, photo_id):\n            self.hash = hash_value\n            self.photo_id = photo_id\n            self.children = {}\n\n    def __init__(self):\n        self.root = None\n\n    def insert(self, photo_id, hash_value):\n        """Insert photo hash into tree."""\n        if self.root is None:\n            self.root = self.Node(hash_value, photo_id)\n        else:\n            self._insert_recursive(self.root, photo_id, hash_value)\n\n    def _insert_recursive(self, node, photo_id, hash_value):\n        distance = self.hamming_distance(node.hash, hash_value)\n        if distance in node.children:\n            self._insert_recursive(node.children[distance], photo_id, hash_value)\n        else:\n            node.children[distance] = self.Node(hash_value, photo_id)\n\n    def search(self, query_hash, threshold):\n        """Find all photos within Hamming distance threshold."""\n        if self.root is None:\n            return []\n        return self._search_recursive(self.root, query_hash, threshold)\n\n    def _search_recursive(self, node, query_hash, threshold):\n        results = []\n        distance = self.hamming_distance(node.hash, query_hash)\n\n        if distance <= threshold:\n            results.append((node.photo_id, distance))\n\n        for child_dist in range(max(0, distance - threshold),\n                                distance + threshold + 1):\n            if child_dist in node.children:\n                results.extend(\n                    self._search_recursive(node.children[child_dist],\n                                         query_hash, threshold)\n                )\n        return results\n\n    @staticmethod\n    def hamming_distance(hash1, hash2):\n        if isinstance(hash1, np.ndarray):\n            return np.sum(hash1 != hash2)\n        return bin(hash1 ^ hash2).count(\'1\')\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"performance",children:"Performance"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"O(N\xb2) for pHash comparison, but with early termination. For 10K photos:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Stage 1 (pHash): ~5 seconds"}),"\n",(0,t.jsx)(n.li,{children:"Stage 2 (DINOHash on candidates): ~2 seconds"}),"\n",(0,t.jsx)(n.li,{children:"Total: ~7 seconds for full duplicate detection"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"method-comparison",children:"Method Comparison"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Method"}),(0,t.jsx)(n.th,{children:"Speed"}),(0,t.jsx)(n.th,{children:"Robustness"}),(0,t.jsx)(n.th,{children:"Use Case"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"dHash"}),(0,t.jsx)(n.td,{children:"Fastest"}),(0,t.jsx)(n.td,{children:"Low"}),(0,t.jsx)(n.td,{children:"Exact duplicates"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"pHash"}),(0,t.jsx)(n.td,{children:"Fast"}),(0,t.jsx)(n.td,{children:"Medium"}),(0,t.jsx)(n.td,{children:"Brightness/contrast changes"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"DINOHash"}),(0,t.jsx)(n.td,{children:"Slower"}),(0,t.jsx)(n.td,{children:"High"}),(0,t.jsx)(n.td,{children:"Heavy crops, compression"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Hybrid"}),(0,t.jsx)(n.td,{children:"Medium"}),(0,t.jsx)(n.td,{children:"Very High"}),(0,t.jsx)(n.td,{children:"Production systems"})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:'"DINOHash: Adversarially Fine-Tuned DINOv2 Features" (2025)'}),"\n",(0,t.jsx)(n.li,{children:"Neal Krawetz: dHash development"}),"\n",(0,t.jsx)(n.li,{children:"DCT-based pHash algorithms"}),"\n"]})]})}function c(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}}}]);