"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[11202,88821],{88821:n=>{n.exports=JSON.parse('{"name":"video-processing-editing","type":"folder","path":"video-processing-editing","children":[{"name":"demo","type":"folder","path":"video-processing-editing/demo","children":[{"name":"index.html","type":"file","path":"video-processing-editing/demo/index.html","size":37079}]},{"name":"references","type":"folder","path":"video-processing-editing/references","children":[{"name":"export-optimization.md","type":"file","path":"video-processing-editing/references/export-optimization.md","size":10689,"content":"# Export Optimization Reference\\n\\nPlatform-specific export settings and optimization techniques.\\n\\n## Platform Specifications (2024/2025)\\n\\n### YouTube\\n\\n| Setting | Recommended | Maximum |\\n|---------|-------------|---------|\\n| Resolution | 1920x1080 (1080p) | 7680x4320 (8K) |\\n| Frame Rate | 30 fps | 60 fps |\\n| Bitrate | Auto (CRF 18) | Unlimited |\\n| Max File Size | 256 GB | 256 GB |\\n| Max Duration | 12 hours | 12 hours |\\n| Container | MP4 | MP4, MOV, MKV |\\n| Video Codec | H.264 | H.264, H.265, VP9, AV1 |\\n| Audio | AAC 192kbps | AAC 384kbps |\\n| Color | BT.709 SDR | BT.2020 HDR |\\n\\n```bash\\n# YouTube 1080p (standard)\\nffmpeg -i input.mp4 \\\\\\n  -c:v libx264 -preset slow -crf 18 \\\\\\n  -s 1920x1080 -r 30 \\\\\\n  -pix_fmt yuv420p \\\\\\n  -color_primaries bt709 -color_trc bt709 -colorspace bt709 \\\\\\n  -movflags +faststart \\\\\\n  -c:a aac -b:a 192k -ar 48000 \\\\\\n  youtube_1080p.mp4\\n\\n# YouTube 4K HDR\\nffmpeg -i input.mp4 \\\\\\n  -c:v libx265 -preset slow -crf 18 \\\\\\n  -s 3840x2160 -r 60 \\\\\\n  -pix_fmt yuv420p10le \\\\\\n  -color_primaries bt2020 -color_trc smpte2084 -colorspace bt2020nc \\\\\\n  -x265-params \\"hdr-opt=1:repeat-headers=1:colorprim=bt2020:transfer=smpte2084:colormatrix=bt2020nc:master-display=G(13250,34500)B(7500,3000)R(34000,16000)WP(15635,16450)L(10000000,1):max-cll=1000,400\\" \\\\\\n  -movflags +faststart \\\\\\n  -c:a aac -b:a 256k -ar 48000 \\\\\\n  youtube_4k_hdr.mp4\\n```\\n\\n### Instagram\\n\\n**Stories**\\n| Setting | Requirement |\\n|---------|-------------|\\n| Resolution | 1080x1920 (9:16) |\\n| Frame Rate | 30 fps |\\n| Max Duration | 60 seconds |\\n| Max File Size | 250 MB |\\n| Video Codec | H.264 |\\n| Audio | AAC 128kbps |\\n\\n**Reels**\\n| Setting | Requirement |\\n|---------|-------------|\\n| Resolution | 1080x1920 (9:16) |\\n| Frame Rate | 30 fps |\\n| Max Duration | 90 seconds |\\n| Max File Size | 4 GB |\\n| Video Codec | H.264 |\\n| Audio | AAC 128kbps |\\n\\n**Feed**\\n| Setting | Requirement |\\n|---------|-------------|\\n| Resolution | 1080x1080 (1:1) or 1080x1350 (4:5) |\\n| Frame Rate | 30 fps |\\n| Max Duration | 60 minutes |\\n| Video Codec | H.264 |\\n| Audio | AAC 128kbps |\\n\\n```bash\\n# Instagram Story\\nffmpeg -i input.mp4 \\\\\\n  -c:v libx264 -preset medium -crf 23 \\\\\\n  -s 1080x1920 -r 30 -t 60 \\\\\\n  -pix_fmt yuv420p \\\\\\n  -movflags +faststart \\\\\\n  -c:a aac -b:a 128k \\\\\\n  instagram_story.mp4\\n\\n# Instagram Reel\\nffmpeg -i input.mp4 \\\\\\n  -c:v libx264 -preset medium -crf 23 \\\\\\n  -s 1080x1920 -r 30 -t 90 \\\\\\n  -pix_fmt yuv420p \\\\\\n  -movflags +faststart \\\\\\n  -c:a aac -b:a 128k \\\\\\n  instagram_reel.mp4\\n\\n# Instagram Feed (4:5 portrait)\\nffmpeg -i input.mp4 \\\\\\n  -c:v libx264 -preset medium -crf 23 \\\\\\n  -vf \\"scale=1080:1350:force_original_aspect_ratio=decrease,pad=1080:1350:(ow-iw)/2:(oh-ih)/2\\" \\\\\\n  -r 30 \\\\\\n  -pix_fmt yuv420p \\\\\\n  -movflags +faststart \\\\\\n  -c:a aac -b:a 128k \\\\\\n  instagram_feed.mp4\\n```\\n\\n### TikTok\\n\\n| Setting | Requirement |\\n|---------|-------------|\\n| Resolution | 1080x1920 (9:16) |\\n| Frame Rate | 30 fps |\\n| Max Duration | 10 minutes |\\n| Max File Size | 287 MB (mobile), 1 GB (web) |\\n| Video Codec | H.264 |\\n| Audio | AAC 128kbps |\\n| Bitrate | 3-5 Mbps recommended |\\n\\n```bash\\n# TikTok\\nffmpeg -i input.mp4 \\\\\\n  -c:v libx264 -preset medium -crf 23 \\\\\\n  -s 1080x1920 -r 30 -t 600 \\\\\\n  -pix_fmt yuv420p \\\\\\n  -movflags +faststart \\\\\\n  -c:a aac -b:a 128k \\\\\\n  tiktok.mp4\\n```\\n\\n### Twitter/X\\n\\n| Setting | Requirement |\\n|---------|-------------|\\n| Resolution | 1920x1080 (16:9) or 1280x720 |\\n| Frame Rate | 30-60 fps |\\n| Max Duration | 2 minutes 20 seconds |\\n| Max File Size | 512 MB |\\n| Video Codec | H.264 |\\n| Max Bitrate | 25 Mbps |\\n| Audio | AAC, max 128kbps |\\n\\n```bash\\n# Twitter/X\\nffmpeg -i input.mp4 \\\\\\n  -c:v libx264 -preset medium -crf 23 \\\\\\n  -s 1280x720 -r 30 -t 140 \\\\\\n  -maxrate 5M -bufsize 10M \\\\\\n  -pix_fmt yuv420p \\\\\\n  -movflags +faststart \\\\\\n  -c:a aac -b:a 128k \\\\\\n  twitter.mp4\\n```\\n\\n### LinkedIn\\n\\n| Setting | Requirement |\\n|---------|-------------|\\n| Resolution | 1920x1080 (16:9) |\\n| Frame Rate | 30 fps |\\n| Max Duration | 10 minutes (native), 15 min (ads) |\\n| Max File Size | 5 GB |\\n| Video Codec | H.264 |\\n| Audio | AAC 128kbps |\\n\\n```bash\\n# LinkedIn\\nffmpeg -i input.mp4 \\\\\\n  -c:v libx264 -preset medium -crf 23 \\\\\\n  -s 1920x1080 -r 30 -t 600 \\\\\\n  -pix_fmt yuv420p \\\\\\n  -movflags +faststart \\\\\\n  -c:a aac -b:a 128k \\\\\\n  linkedin.mp4\\n```\\n\\n### Facebook\\n\\n| Setting | Requirement |\\n|---------|-------------|\\n| Resolution | 1920x1080 or 1080x1920 |\\n| Frame Rate | 30 fps |\\n| Max Duration | 240 minutes |\\n| Max File Size | 10 GB |\\n| Video Codec | H.264, H.265 |\\n| Audio | AAC, MP3 |\\n\\n```bash\\n# Facebook (landscape)\\nffmpeg -i input.mp4 \\\\\\n  -c:v libx264 -preset medium -crf 23 \\\\\\n  -s 1920x1080 -r 30 \\\\\\n  -pix_fmt yuv420p \\\\\\n  -movflags +faststart \\\\\\n  -c:a aac -b:a 128k \\\\\\n  facebook.mp4\\n\\n# Facebook Stories/Reels (vertical)\\nffmpeg -i input.mp4 \\\\\\n  -c:v libx264 -preset medium -crf 23 \\\\\\n  -s 1080x1920 -r 30 -t 60 \\\\\\n  -pix_fmt yuv420p \\\\\\n  -movflags +faststart \\\\\\n  -c:a aac -b:a 128k \\\\\\n  facebook_story.mp4\\n```\\n\\n## Web Optimization\\n\\n### HTML5 Video\\n\\n```bash\\n# Maximum compatibility (works everywhere)\\nffmpeg -i input.mp4 \\\\\\n  -c:v libx264 -preset medium -crf 23 \\\\\\n  -profile:v baseline -level 3.0 \\\\\\n  -s 1920x1080 -r 30 \\\\\\n  -pix_fmt yuv420p \\\\\\n  -movflags +faststart \\\\\\n  -c:a aac -b:a 128k -ar 48000 \\\\\\n  web_compatible.mp4\\n```\\n\\n### Adaptive Bitrate Streaming (HLS)\\n\\n```bash\\n# Generate HLS with multiple quality levels\\nffmpeg -i input.mp4 \\\\\\n  -filter_complex \\"[0:v]split=3[v1][v2][v3]\\" \\\\\\n  -map \\"[v1]\\" -c:v:0 libx264 -b:v:0 5M -s:v:0 1920x1080 -profile:v:0 high \\\\\\n  -map \\"[v2]\\" -c:v:1 libx264 -b:v:1 3M -s:v:1 1280x720 -profile:v:1 main \\\\\\n  -map \\"[v3]\\" -c:v:2 libx264 -b:v:2 1M -s:v:2 854x480 -profile:v:2 baseline \\\\\\n  -map 0:a -c:a aac -b:a 128k \\\\\\n  -var_stream_map \\"v:0,a:0 v:1,a:0 v:2,a:0\\" \\\\\\n  -master_pl_name master.m3u8 \\\\\\n  -f hls -hls_time 6 -hls_list_size 0 \\\\\\n  -hls_segment_filename \\"v%v/segment%d.ts\\" \\\\\\n  v%v/playlist.m3u8\\n```\\n\\n### WebM (VP9)\\n\\n```bash\\n# WebM for modern browsers (smaller file size)\\nffmpeg -i input.mp4 \\\\\\n  -c:v libvpx-vp9 -crf 30 -b:v 0 \\\\\\n  -s 1920x1080 -r 30 \\\\\\n  -c:a libopus -b:a 128k \\\\\\n  output.webm\\n```\\n\\n## Compression Optimization\\n\\n### Quality vs File Size\\n\\n| CRF Value | Quality | Use Case |\\n|-----------|---------|----------|\\n| 17-18 | Visually lossless | Archival, mastering |\\n| 19-21 | High quality | YouTube, streaming |\\n| 22-23 | Good quality | Social media |\\n| 24-26 | Acceptable | Mobile, bandwidth-limited |\\n| 27-28 | Lower quality | Drafts, previews |\\n\\n### Two-Pass Encoding (Target File Size)\\n\\n```bash\\n# Target 50 MB file for 60s video\\n# Calculate bitrate: (50 MB * 8) / 60s = 6.67 Mbps\\n# Subtract audio: 6.67 - 0.128 = 6.54 Mbps video\\n\\n# Pass 1\\nffmpeg -i input.mp4 -c:v libx264 -b:v 6500k -pass 1 -f null /dev/null\\n\\n# Pass 2\\nffmpeg -i input.mp4 -c:v libx264 -b:v 6500k -pass 2 -c:a aac -b:a 128k output.mp4\\n```\\n\\n### Preset Selection\\n\\n| Preset | Speed | Compression | Use Case |\\n|--------|-------|-------------|----------|\\n| ultrafast | Very fast | Poor | Drafts, testing |\\n| superfast | Fast | Poor | Quick previews |\\n| veryfast | Fast | OK | Streaming, live |\\n| faster | Fast | OK | Quick exports |\\n| fast | Medium | Good | Balance |\\n| medium | Medium | Good | Default |\\n| slow | Slow | Better | Final export |\\n| slower | Very slow | Best | Final mastering |\\n| veryslow | Extremely slow | Best | Archive quality |\\n\\n```bash\\n# Draft (fastest)\\nffmpeg -i input.mp4 -c:v libx264 -preset ultrafast -crf 28 draft.mp4\\n\\n# Final (best quality)\\nffmpeg -i input.mp4 -c:v libx264 -preset veryslow -crf 18 final.mp4\\n```\\n\\n## Progressive Download Optimization\\n\\n### faststart for Web\\n\\n```bash\\n# CRITICAL for web playback - moves moov atom to beginning\\nffmpeg -i input.mp4 -c copy -movflags +faststart output.mp4\\n\\n# For new encodes\\nffmpeg -i input.mp4 -c:v libx264 -crf 23 -movflags +faststart output.mp4\\n```\\n\\nWithout `-movflags +faststart`:\\n1. Browser downloads entire file\\n2. Finds moov atom at end\\n3. Only then can start playback\\n\\nWith `-movflags +faststart`:\\n1. Browser downloads beginning\\n2. Finds moov atom immediately\\n3. Playback starts instantly\\n\\n## Batch Export Script\\n\\n```bash\\n#!/bin/bash\\n# Export to all platforms at once\\n\\nINPUT=\\"$1\\"\\nBASENAME=\\"${INPUT%.*}\\"\\n\\n# YouTube 1080p\\nffmpeg -i \\"$INPUT\\" \\\\\\n  -c:v libx264 -preset slow -crf 18 \\\\\\n  -s 1920x1080 -r 30 \\\\\\n  -pix_fmt yuv420p \\\\\\n  -movflags +faststart \\\\\\n  -c:a aac -b:a 192k \\\\\\n  \\"${BASENAME}_youtube.mp4\\"\\n\\n# Instagram Reel\\nffmpeg -i \\"$INPUT\\" \\\\\\n  -c:v libx264 -preset medium -crf 23 \\\\\\n  -s 1080x1920 -r 30 -t 90 \\\\\\n  -pix_fmt yuv420p \\\\\\n  -movflags +faststart \\\\\\n  -c:a aac -b:a 128k \\\\\\n  \\"${BASENAME}_instagram.mp4\\"\\n\\n# Twitter\\nffmpeg -i \\"$INPUT\\" \\\\\\n  -c:v libx264 -preset medium -crf 23 \\\\\\n  -s 1280x720 -r 30 -t 140 \\\\\\n  -maxrate 5M -bufsize 10M \\\\\\n  -pix_fmt yuv420p \\\\\\n  -movflags +faststart \\\\\\n  -c:a aac -b:a 128k \\\\\\n  \\"${BASENAME}_twitter.mp4\\"\\n\\n# TikTok\\nffmpeg -i \\"$INPUT\\" \\\\\\n  -c:v libx264 -preset medium -crf 23 \\\\\\n  -s 1080x1920 -r 30 \\\\\\n  -pix_fmt yuv420p \\\\\\n  -movflags +faststart \\\\\\n  -c:a aac -b:a 128k \\\\\\n  \\"${BASENAME}_tiktok.mp4\\"\\n\\necho \\"Exports complete!\\"\\n```\\n\\n## Hardware Acceleration\\n\\n### NVIDIA NVENC\\n\\n```bash\\n# NVENC encoding (10-20x faster than CPU)\\nffmpeg -hwaccel cuda -i input.mp4 \\\\\\n  -c:v h264_nvenc -preset p7 -rc:v vbr -cq:v 19 \\\\\\n  -b:v 0 -s 1920x1080 \\\\\\n  -c:a aac -b:a 192k \\\\\\n  output.mp4\\n\\n# Presets: p1 (fastest) to p7 (best quality)\\n# cq:v: Quality level (lower = better, 19 \u2248 CRF 18)\\n```\\n\\n### Apple VideoToolbox (macOS)\\n\\n```bash\\n# VideoToolbox encoding (uses Apple Silicon/Intel GPU)\\nffmpeg -i input.mp4 \\\\\\n  -c:v h264_videotoolbox -q:v 60 \\\\\\n  -s 1920x1080 \\\\\\n  -c:a aac -b:a 192k \\\\\\n  output.mp4\\n\\n# q:v: Quality 1-100 (higher = better, 60 \u2248 CRF 20)\\n```\\n\\n### Intel Quick Sync\\n\\n```bash\\n# QSV encoding\\nffmpeg -hwaccel qsv -i input.mp4 \\\\\\n  -c:v h264_qsv -preset slow -global_quality 20 \\\\\\n  -s 1920x1080 \\\\\\n  -c:a aac -b:a 192k \\\\\\n  output.mp4\\n```\\n\\n## Verification\\n\\n### Check Output Quality\\n\\n```bash\\n# Get VMAF score (requires libvmaf)\\nffmpeg -i original.mp4 -i encoded.mp4 \\\\\\n  -lavfi libvmaf=\\"log_fmt=json:log_path=vmaf.json\\" \\\\\\n  -f null -\\n\\n# VMAF interpretation:\\n# 90+: Excellent (indistinguishable)\\n# 80-90: Good quality\\n# 70-80: Acceptable\\n# <70: Noticeable quality loss\\n```\\n\\n### Check File Metadata\\n\\n```bash\\n# Verify encoding settings\\nffprobe -v error -show_format -show_streams output.mp4\\n\\n# Check moov atom position (for faststart)\\nffprobe -v trace output.mp4 2>&1 | grep -E \\"moov|mdat\\"\\n# moov should appear before mdat for faststart\\n```\\n\\n### Check Platform Compatibility\\n\\n```bash\\n# Verify H.264 profile and level\\nffprobe -v error -select_streams v:0 \\\\\\n  -show_entries stream=profile,level \\\\\\n  -of default=noprint_wrappers=1 output.mp4\\n\\n# For maximum compatibility:\\n# profile: Baseline or Main\\n# level: 3.0 or 3.1\\n```\\n\\n---\\n\\nThis reference covers export optimization. For timeline concepts, see `timeline-editing.md`. For FFmpeg commands, see `ffmpeg-guide.md`.\\n"},{"name":"ffmpeg-guide.md","type":"file","path":"video-processing-editing/references/ffmpeg-guide.md","size":14079,"content":"# FFmpeg Complete Reference Guide\\n\\nComprehensive FFmpeg command reference for video processing and editing.\\n\\n## Essential Commands\\n\\n### Basic Conversion\\n\\n```bash\\n# Convert format (auto settings)\\nffmpeg -i input.mp4 output.avi\\n\\n# Convert with specific codec\\nffmpeg -i input.mp4 -c:v libx264 -c:a aac output.mp4\\n\\n# Copy streams without re-encoding (fast)\\nffmpeg -i input.mp4 -c copy output.mp4\\n```\\n\\n### Cutting and Trimming\\n\\n```bash\\n# Cut from start time, specific duration\\nffmpeg -i input.mp4 -ss 00:01:30 -t 00:00:30 output.mp4\\n\\n# Cut from start to end time\\nffmpeg -i input.mp4 -ss 00:01:00 -to 00:05:00 output.mp4\\n\\n# Two-pass cutting (fast + accurate)\\nffmpeg -ss 00:01:00 -i input.mp4 -ss 00:00:05 -t 00:01:30 -c:v libx264 -crf 18 output.mp4\\n```\\n\\n### Quality Control\\n\\n```bash\\n# CRF (Constant Rate Factor) - recommended\\n# Lower = better quality, larger file\\n# 18 = visually lossless, 23 = high quality, 28 = acceptable\\nffmpeg -i input.mp4 -c:v libx264 -crf 18 output.mp4\\n\\n# Two-pass encoding (best quality for target size)\\nffmpeg -i input.mp4 -c:v libx264 -b:v 2M -pass 1 -f null /dev/null\\nffmpeg -i input.mp4 -c:v libx264 -b:v 2M -pass 2 output.mp4\\n\\n# Preset (speed vs compression)\\n# ultrafast, superfast, veryfast, faster, fast, medium, slow, slower, veryslow\\nffmpeg -i input.mp4 -c:v libx264 -preset slow -crf 18 output.mp4\\n```\\n\\n### Resolution and Scaling\\n\\n```bash\\n# Scale to specific resolution\\nffmpeg -i input.mp4 -vf scale=1920:1080 output.mp4\\n\\n# Scale maintaining aspect ratio\\nffmpeg -i input.mp4 -vf scale=1920:-2 output.mp4  # width=1920, height=auto (even)\\nffmpeg -i input.mp4 -vf scale=-2:1080 output.mp4  # height=1080, width=auto\\n\\n# Scale with high-quality algorithm\\nffmpeg -i input.mp4 -vf scale=1920:1080:flags=lanczos output.mp4\\n```\\n\\n### Frame Rate\\n\\n```bash\\n# Change frame rate\\nffmpeg -i input.mp4 -r 30 output.mp4\\n\\n# Slow motion (interpolate frames)\\nffmpeg -i input.mp4 -filter:v \\"minterpolate=\'fps=60:mi_mode=mci\'\\" output.mp4\\n\\n# Speed up video\\nffmpeg -i input.mp4 -filter:v \\"setpts=0.5*PTS\\" output.mp4  # 2x speed\\n```\\n\\n### Audio Operations\\n\\n```bash\\n# Extract audio\\nffmpeg -i input.mp4 -vn -c:a copy audio.aac\\n\\n# Remove audio\\nffmpeg -i input.mp4 -an output.mp4\\n\\n# Replace audio\\nffmpeg -i video.mp4 -i audio.mp3 -c:v copy -c:a aac -map 0:v -map 1:a output.mp4\\n\\n# Mix audio (50/50)\\nffmpeg -i video.mp4 -i audio.mp3 -filter_complex \\"[0:a][1:a]amix=inputs=2[a]\\" -map 0:v -map \\"[a]\\" output.mp4\\n\\n# Adjust audio volume\\nffmpeg -i input.mp4 -af \\"volume=2.0\\" output.mp4  # 2x louder\\nffmpeg -i input.mp4 -af \\"volume=0.5\\" output.mp4  # half volume\\n\\n# Normalize audio\\nffmpeg -i input.mp4 -af \\"loudnorm=I=-16:TP=-1.5:LRA=11\\" output.mp4\\n```\\n\\n### Concatenation\\n\\n```bash\\n# Create concat file\\necho \\"file \'video1.mp4\'\\" > list.txt\\necho \\"file \'video2.mp4\'\\" >> list.txt\\necho \\"file \'video3.mp4\'\\" >> list.txt\\n\\n# Concatenate (must have same codec/resolution)\\nffmpeg -f concat -safe 0 -i list.txt -c copy output.mp4\\n\\n# Concatenate with re-encoding (different formats)\\nffmpeg -f concat -safe 0 -i list.txt -c:v libx264 -crf 18 output.mp4\\n\\n# Concatenate specific segments\\nffmpeg -i input.mp4 -filter_complex \\\\\\n  \\"[0:v]trim=0:5,setpts=PTS-STARTPTS[v1]; \\\\\\n   [0:v]trim=10:15,setpts=PTS-STARTPTS[v2]; \\\\\\n   [v1][v2]concat=n=2:v=1[out]\\" \\\\\\n  -map \\"[out]\\" output.mp4\\n```\\n\\n### Subtitles\\n\\n```bash\\n# Burn subtitles into video\\nffmpeg -i input.mp4 -vf subtitles=subs.srt output.mp4\\n\\n# Add soft subtitles (MP4)\\nffmpeg -i input.mp4 -i subs.srt -c copy -c:s mov_text output.mp4\\n\\n# Add soft subtitles (MKV)\\nffmpeg -i input.mp4 -i subs.srt -c copy -c:s srt output.mkv\\n\\n# Style burned subtitles\\nffmpeg -i input.mp4 -vf \\"subtitles=subs.srt:force_style=\'FontName=Arial,FontSize=24,PrimaryColour=&H00FFFF\'\\" output.mp4\\n```\\n\\n### Cropping and Padding\\n\\n```bash\\n# Crop video (remove edges)\\nffmpeg -i input.mp4 -vf \\"crop=1920:800:0:140\\" output.mp4\\n# Format: crop=width:height:x:y\\n\\n# Auto-detect crop (remove black bars)\\nffmpeg -i input.mp4 -vf \\"cropdetect=24:16:0\\" -f null -\\n# Use detected values in actual crop\\n\\n# Add padding (letterbox/pillarbox)\\nffmpeg -i input.mp4 -vf \\"pad=1920:1080:0:100:black\\" output.mp4\\n# Format: pad=width:height:x:y:color\\n```\\n\\n### Filters (Video Effects)\\n\\n```bash\\n# Brightness/Contrast\\nffmpeg -i input.mp4 -vf \\"eq=brightness=0.1:contrast=1.2\\" output.mp4\\n\\n# Saturation\\nffmpeg -i input.mp4 -vf \\"eq=saturation=1.5\\" output.mp4\\n\\n# Gamma correction\\nffmpeg -i input.mp4 -vf \\"eq=gamma=1.2\\" output.mp4\\n\\n# Sharpen\\nffmpeg -i input.mp4 -vf \\"unsharp=5:5:1.0\\" output.mp4\\n\\n# Blur\\nffmpeg -i input.mp4 -vf \\"boxblur=5:1\\" output.mp4\\n\\n# Denoise\\nffmpeg -i input.mp4 -vf \\"hqdn3d=4:3:6:4.5\\" output.mp4\\n\\n# Deinterlace\\nffmpeg -i input.mp4 -vf \\"yadif=0:-1:0\\" output.mp4\\n\\n# Rotate\\nffmpeg -i input.mp4 -vf \\"rotate=45*PI/180\\" output.mp4  # 45 degrees\\nffmpeg -i input.mp4 -vf \\"transpose=1\\" output.mp4  # 90 degrees clockwise\\n\\n# Flip\\nffmpeg -i input.mp4 -vf \\"hflip\\" output.mp4  # horizontal\\nffmpeg -i input.mp4 -vf \\"vflip\\" output.mp4  # vertical\\n```\\n\\n### Overlays and Watermarks\\n\\n```bash\\n# Add watermark (top-left)\\nffmpeg -i input.mp4 -i logo.png -filter_complex \\"overlay=10:10\\" output.mp4\\n\\n# Add watermark (bottom-right with padding)\\nffmpeg -i input.mp4 -i logo.png -filter_complex \\"overlay=W-w-10:H-h-10\\" output.mp4\\n\\n# Fade in watermark\\nffmpeg -i input.mp4 -i logo.png -filter_complex \\\\\\n  \\"[1:v]fade=in:st=0:d=1:alpha=1[logo]; \\\\\\n   [0:v][logo]overlay=W-w-10:H-h-10\\" output.mp4\\n\\n# Add text overlay\\nffmpeg -i input.mp4 -vf \\"drawtext=text=\'Hello World\':fontfile=/path/to/font.ttf:fontsize=24:fontcolor=white:x=10:y=10\\" output.mp4\\n```\\n\\n### Transitions\\n\\n```bash\\n# Crossfade between two videos\\nffmpeg -i video1.mp4 -i video2.mp4 -filter_complex \\\\\\n  \\"[0:v][1:v]xfade=transition=fade:duration=1:offset=5\\" output.mp4\\n\\n# Available transitions:\\n# fade, fadeblack, fadewhite, distance, wipeleft, wiperight,\\n# wipeup, wipedown, slideleft, slideright, slideup, slidedown,\\n# circlecrop, rectcrop, circleclose, circleopen, dissolve\\n```\\n\\n### Color Grading\\n\\n```bash\\n# Convert to grayscale\\nffmpeg -i input.mp4 -vf \\"hue=s=0\\" output.mp4\\n\\n# Adjust hue\\nffmpeg -i input.mp4 -vf \\"hue=h=90\\" output.mp4  # shift hue by 90 degrees\\n\\n# Color temperature (warm)\\nffmpeg -i input.mp4 -vf \\"eq=brightness=0.02:saturation=1.1,hue=h=10\\" output.mp4\\n\\n# Color temperature (cool)\\nffmpeg -i input.mp4 -vf \\"eq=brightness=-0.02:saturation=1.1,hue=h=-10\\" output.mp4\\n\\n# Vintage/sepia look\\nffmpeg -i input.mp4 -vf \\"colorchannelmixer=.393:.769:.189:0:.349:.686:.168:0:.272:.534:.131\\" output.mp4\\n\\n# High contrast B&W\\nffmpeg -i input.mp4 -vf \\"hue=s=0,eq=contrast=1.5:brightness=0.1\\" output.mp4\\n```\\n\\n### Color Space Conversion\\n\\n```bash\\n# SDR to HDR (basic)\\nffmpeg -i input.mp4 -vf \\"zscale=t=linear:npl=100,format=gbrpf32le,zscale=p=bt2020:t=smpte2084:m=bt2020nc:r=full,format=yuv420p10le\\" output.mp4\\n\\n# HDR to SDR (tonemap)\\nffmpeg -i input.mp4 -vf \\"zscale=t=linear:npl=100,format=gbrpf32le,zscale=p=bt709,tonemap=hable:desat=0,zscale=t=bt709:m=bt709:r=limited,format=yuv420p\\" output.mp4\\n\\n# BT.601 to BT.709 (SD to HD)\\nffmpeg -i input.mp4 -vf \\"scale=in_range=full:out_range=limited,colorspace=bt709:iall=bt601:fast=1\\" \\\\\\n  -color_primaries bt709 -color_trc bt709 -colorspace bt709 output.mp4\\n```\\n\\n### Encoding Presets\\n\\n```bash\\n# YouTube 1080p\\nffmpeg -i input.mp4 \\\\\\n  -c:v libx264 -preset slow -crf 18 \\\\\\n  -s 1920x1080 -r 30 \\\\\\n  -pix_fmt yuv420p \\\\\\n  -color_primaries bt709 -color_trc bt709 -colorspace bt709 \\\\\\n  -movflags +faststart \\\\\\n  -c:a aac -b:a 192k -ar 48000 \\\\\\n  youtube.mp4\\n\\n# Instagram Story (9:16)\\nffmpeg -i input.mp4 \\\\\\n  -c:v libx264 -preset medium -crf 23 \\\\\\n  -s 1080x1920 -r 30 -t 15 \\\\\\n  -pix_fmt yuv420p \\\\\\n  -movflags +faststart \\\\\\n  -c:a aac -b:a 128k \\\\\\n  instagram_story.mp4\\n\\n# Twitter (720p, 2:20 max)\\nffmpeg -i input.mp4 \\\\\\n  -c:v libx264 -preset medium -crf 23 \\\\\\n  -s 1280x720 -r 30 -t 140 \\\\\\n  -maxrate 5000k -bufsize 10000k \\\\\\n  -pix_fmt yuv420p \\\\\\n  -movflags +faststart \\\\\\n  -c:a aac -b:a 128k \\\\\\n  twitter.mp4\\n\\n# Web (HTML5 compatible)\\nffmpeg -i input.mp4 \\\\\\n  -c:v libx264 -preset medium -crf 23 \\\\\\n  -s 1920x1080 -r 30 \\\\\\n  -pix_fmt yuv420p \\\\\\n  -profile:v baseline -level 3.0 \\\\\\n  -movflags +faststart \\\\\\n  -c:a aac -b:a 128k -ar 48000 \\\\\\n  web.mp4\\n```\\n\\n### Hardware Acceleration\\n\\n```bash\\n# NVIDIA NVENC (GPU encoding)\\nffmpeg -hwaccel cuda -i input.mp4 \\\\\\n  -c:v h264_nvenc -preset slow -crf 18 \\\\\\n  output.mp4\\n\\n# Apple VideoToolbox (Mac)\\nffmpeg -i input.mp4 \\\\\\n  -c:v h264_videotoolbox -b:v 5M \\\\\\n  output.mp4\\n\\n# Intel Quick Sync (QSV)\\nffmpeg -hwaccel qsv -i input.mp4 \\\\\\n  -c:v h264_qsv -preset slow -global_quality 18 \\\\\\n  output.mp4\\n\\n# AMD VCE\\nffmpeg -i input.mp4 \\\\\\n  -c:v h264_amf -quality quality -rc cqp -qp_i 18 -qp_p 18 \\\\\\n  output.mp4\\n```\\n\\n### Advanced Filters\\n\\n```bash\\n# Stabilization (two-pass)\\n# Pass 1: Analyze\\nffmpeg -i input.mp4 -vf vidstabdetect=shakiness=10:accuracy=15 -f null -\\n\\n# Pass 2: Transform\\nffmpeg -i input.mp4 -vf vidstabtransform=smoothing=30:input=\\"transforms.trf\\" output.mp4\\n\\n# Picture-in-picture\\nffmpeg -i main.mp4 -i pip.mp4 -filter_complex \\\\\\n  \\"[1:v]scale=320:240[pip]; \\\\\\n   [0:v][pip]overlay=W-w-10:H-h-10\\" output.mp4\\n\\n# Side-by-side comparison\\nffmpeg -i video1.mp4 -i video2.mp4 -filter_complex \\\\\\n  \\"[0:v]scale=iw/2:ih[left]; \\\\\\n   [1:v]scale=iw/2:ih[right]; \\\\\\n   [left][right]hstack\\" output.mp4\\n\\n# Grid layout (2x2)\\nffmpeg -i v1.mp4 -i v2.mp4 -i v3.mp4 -i v4.mp4 -filter_complex \\\\\\n  \\"[0:v][1:v]hstack[top]; \\\\\\n   [2:v][3:v]hstack[bottom]; \\\\\\n   [top][bottom]vstack\\" output.mp4\\n```\\n\\n### Metadata\\n\\n```bash\\n# View metadata\\nffmpeg -i input.mp4 -f ffmetadata metadata.txt\\n\\n# Remove all metadata\\nffmpeg -i input.mp4 -map_metadata -1 -c copy output.mp4\\n\\n# Add metadata\\nffmpeg -i input.mp4 -metadata title=\\"My Video\\" -metadata author=\\"John Doe\\" -c copy output.mp4\\n\\n# Rotate metadata (without re-encoding)\\nffmpeg -i input.mp4 -metadata:s:v rotate=90 -c copy output.mp4\\n```\\n\\n### Stream Mapping\\n\\n```bash\\n# Extract specific stream\\nffmpeg -i input.mp4 -map 0:0 video_only.mp4  # First stream\\nffmpeg -i input.mp4 -map 0:a audio_only.aac  # All audio streams\\n\\n# Combine specific streams from multiple files\\nffmpeg -i video.mp4 -i audio.mp3 -i subs.srt \\\\\\n  -map 0:v -map 1:a -map 2:s \\\\\\n  -c:v copy -c:a copy -c:s mov_text \\\\\\n  output.mp4\\n\\n# Exclude specific stream\\nffmpeg -i input.mp4 -map 0 -map -0:s -c copy output.mp4  # Remove subtitles\\n```\\n\\n### Thumbnail Generation\\n\\n```bash\\n# Extract single frame\\nffmpeg -i input.mp4 -ss 00:00:05 -frames:v 1 thumbnail.jpg\\n\\n# Extract multiple frames (every 10 seconds)\\nffmpeg -i input.mp4 -vf \\"select=\'not(mod(n\\\\,300))\'\\" -vsync 0 frames/frame_%04d.jpg\\n\\n# Generate thumbnails grid\\nffmpeg -i input.mp4 -vf \\"select=\'not(mod(n\\\\,300))\',scale=320:240,tile=4x3\\" grid.jpg\\n```\\n\\n### Analysis and Probing\\n\\n```bash\\n# Get video info (JSON)\\nffprobe -v quiet -print_format json -show_format -show_streams input.mp4\\n\\n# Get duration\\nffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 input.mp4\\n\\n# Get resolution\\nffprobe -v error -select_streams v:0 -show_entries stream=width,height -of csv=s=x:p=0 input.mp4\\n\\n# Get frame rate\\nffprobe -v error -select_streams v:0 -show_entries stream=r_frame_rate -of default=noprint_wrappers=1:nokey=1 input.mp4\\n\\n# Detect black frames\\nffmpeg -i input.mp4 -vf \\"blackdetect=d=0.5:pix_th=0.10\\" -f null -\\n\\n# Detect scene changes\\nffmpeg -i input.mp4 -vf \\"select=\'gt(scene,0.4)\',showinfo\\" -f null -\\n\\n# Find keyframes\\nffprobe -select_streams v -show_frames -show_entries frame=pkt_pts_time,key_frame -of csv input.mp4 | grep \\",1$\\"\\n```\\n\\n### Performance Tips\\n\\n```bash\\n# Use fastest settings for drafts\\nffmpeg -i input.mp4 -c:v libx264 -preset ultrafast -crf 28 draft.mp4\\n\\n# Multi-threaded encoding\\nffmpeg -i input.mp4 -c:v libx264 -threads 8 -preset medium -crf 18 output.mp4\\n\\n# Limit memory usage\\nffmpeg -i input.mp4 -c:v libx264 -bufsize 2M -maxrate 2M output.mp4\\n\\n# Stream copy when possible (no re-encoding)\\nffmpeg -i input.mp4 -ss 10 -t 60 -c copy output.mp4\\n```\\n\\n## Common Codecs\\n\\n### Video Codecs\\n\\n| Codec | Library | Quality | Speed | Use Case |\\n|-------|---------|---------|-------|----------|\\n| H.264 | libx264 | Excellent | Fast | General purpose, web |\\n| H.265 | libx265 | Excellent | Slow | 4K, high compression |\\n| VP9 | libvpx-vp9 | Excellent | Slow | Web, YouTube |\\n| AV1 | libaom-av1 | Best | Very Slow | Future-proof, best compression |\\n| ProRes | prores | Lossless | Fast | Editing, intermediate |\\n| DNxHD | dnxhd | Lossless | Fast | Editing, intermediate |\\n\\n### Audio Codecs\\n\\n| Codec | Library | Quality | Bitrate | Use Case |\\n|-------|---------|---------|---------|----------|\\n| AAC | aac | Excellent | 128-256k | General purpose |\\n| MP3 | libmp3lame | Good | 128-320k | Universal compatibility |\\n| Opus | libopus | Excellent | 64-128k | Web, low bitrate |\\n| FLAC | flac | Lossless | Variable | Archival |\\n| PCM | pcm_s16le | Lossless | 1411k | Editing |\\n\\n## Pixel Formats\\n\\n```bash\\n# yuv420p - Most compatible (8-bit, 4:2:0)\\n-pix_fmt yuv420p\\n\\n# yuv420p10le - 10-bit HDR\\n-pix_fmt yuv420p10le\\n\\n# yuv444p - Full chroma resolution (4:4:4)\\n-pix_fmt yuv444p\\n\\n# rgb24 - RGB color space\\n-pix_fmt rgb24\\n```\\n\\n## Error Handling\\n\\n```bash\\n# Continue on errors\\nffmpeg -err_detect ignore_err -i input.mp4 output.mp4\\n\\n# Overwrite output files without asking\\nffmpeg -y -i input.mp4 output.mp4\\n\\n# Never overwrite\\nffmpeg -n -i input.mp4 output.mp4\\n\\n# Verbose output for debugging\\nffmpeg -v verbose -i input.mp4 output.mp4\\n\\n# Hide banner\\nffmpeg -hide_banner -i input.mp4 output.mp4\\n```\\n\\n## Best Practices\\n\\n1. **Always use `-movflags +faststart`** for web videos (enables progressive download)\\n2. **Use two-pass encoding for specific file sizes**\\n3. **Prefer CRF over bitrate** for quality-based encoding\\n4. **Use `-preset slow`** for final exports (better compression)\\n5. **Use `-preset ultrafast`** for drafts (faster encoding)\\n6. **Always specify `-pix_fmt yuv420p`** for broad compatibility\\n7. **Set color metadata** when converting color spaces\\n8. **Use stream copy (`-c copy`)** when possible to avoid re-encoding\\n9. **Align cuts to keyframes** for stream copy operations\\n10. **Normalize color spaces** before concatenating clips\\n\\n---\\n\\nThis guide covers 95% of common FFmpeg use cases. For more advanced operations, refer to the official FFmpeg documentation.\\n"},{"name":"timeline-editing.md","type":"file","path":"video-processing-editing/references/timeline-editing.md","size":8697,"content":"# Timeline Editing Reference\\n\\nMulti-track timeline editing concepts and FFmpeg implementation.\\n\\n## Timeline Concepts\\n\\n### Video Track Structure\\n\\n```\\nTimeline:\\n\u251c\u2500\u2500 Video Track 1 (Primary footage)\\n\u251c\u2500\u2500 Video Track 2 (Overlay/B-roll)\\n\u251c\u2500\u2500 Video Track 3 (Graphics/Text)\\n\u251c\u2500\u2500 Audio Track 1 (Dialogue)\\n\u251c\u2500\u2500 Audio Track 2 (Music)\\n\u2514\u2500\u2500 Audio Track 3 (Sound effects)\\n```\\n\\n### In/Out Points\\n\\n```bash\\n# Each clip has:\\n# - IN point:  Where the clip starts on timeline\\n# - OUT point: Where the clip ends on timeline\\n# - SRC IN:    Where to start reading from source\\n# - SRC OUT:   Where to stop reading from source\\n\\n# Example: Place 5s of source (from 10s-15s) at timeline position 30s\\nffmpeg -i source.mp4 -ss 10 -t 5 -i bg.mp4 \\\\\\n  -filter_complex \\"[1:v][0:v]overlay=enable=\'between(t,30,35)\'[out]\\" \\\\\\n  -map \\"[out]\\" output.mp4\\n```\\n\\n## Multi-Track Video Composition\\n\\n### Layering Videos (Overlay)\\n\\n```bash\\n# Layer video2 over video1 (picture-in-picture)\\nffmpeg -i background.mp4 -i overlay.mp4 -filter_complex \\\\\\n  \\"[1:v]scale=320:240[pip]; \\\\\\n   [0:v][pip]overlay=W-w-10:H-h-10:enable=\'between(t,5,15)\'[out]\\" \\\\\\n  -map \\"[out]\\" -map 0:a \\\\\\n  output.mp4\\n\\n# enable=\'between(t,5,15)\': Only show overlay from 5s to 15s\\n```\\n\\n### Split Screen\\n\\n```bash\\n# Side-by-side (2 videos)\\nffmpeg -i left.mp4 -i right.mp4 -filter_complex \\\\\\n  \\"[0:v]scale=960:1080[left]; \\\\\\n   [1:v]scale=960:1080[right]; \\\\\\n   [left][right]hstack[out]\\" \\\\\\n  -map \\"[out]\\" output.mp4\\n\\n# Top/bottom split\\nffmpeg -i top.mp4 -i bottom.mp4 -filter_complex \\\\\\n  \\"[0:v]scale=1920:540[top]; \\\\\\n   [1:v]scale=1920:540[bottom]; \\\\\\n   [top][bottom]vstack[out]\\" \\\\\\n  -map \\"[out]\\" output.mp4\\n\\n# 2x2 Grid\\nffmpeg -i v1.mp4 -i v2.mp4 -i v3.mp4 -i v4.mp4 -filter_complex \\\\\\n  \\"[0:v]scale=960:540[v1]; \\\\\\n   [1:v]scale=960:540[v2]; \\\\\\n   [2:v]scale=960:540[v3]; \\\\\\n   [3:v]scale=960:540[v4]; \\\\\\n   [v1][v2]hstack[top]; \\\\\\n   [v3][v4]hstack[bottom]; \\\\\\n   [top][bottom]vstack[out]\\" \\\\\\n  -map \\"[out]\\" output.mp4\\n```\\n\\n### Insert Edit (Non-destructive)\\n\\n```bash\\n# Insert clip B into clip A at specific point\\n# A: 0-60s, insert B (10s) at A\'s 30s mark\\n# Result: A[0-30], B[0-10], A[30-60]\\n\\n# Step 1: Split clip A\\nffmpeg -i A.mp4 -t 30 -c copy A_part1.mp4\\nffmpeg -i A.mp4 -ss 30 -c copy A_part2.mp4\\n\\n# Step 2: Concatenate with insert\\necho \\"file \'A_part1.mp4\'\\" > list.txt\\necho \\"file \'B.mp4\'\\" >> list.txt\\necho \\"file \'A_part2.mp4\'\\" >> list.txt\\n\\nffmpeg -f concat -safe 0 -i list.txt -c copy output.mp4\\n```\\n\\n### Overwrite Edit\\n\\n```bash\\n# Replace segment of clip A with clip B\\n# A: 0-60s, overwrite 20-30s with B (10s)\\n# Result: A[0-20], B[0-10], A[30-60]\\n\\nffmpeg -i A.mp4 -i B.mp4 -filter_complex \\\\\\n  \\"[0:v]trim=0:20,setpts=PTS-STARTPTS[v1]; \\\\\\n   [1:v]trim=0:10,setpts=PTS-STARTPTS[v2]; \\\\\\n   [0:v]trim=30:60,setpts=PTS-STARTPTS[v3]; \\\\\\n   [v1][v2][v3]concat=n=3:v=1[out]\\" \\\\\\n  -map \\"[out]\\" output.mp4\\n```\\n\\n## Multi-Track Audio\\n\\n### Audio Layering\\n\\n```bash\\n# Mix multiple audio tracks with individual volume control\\nffmpeg -i video.mp4 -i dialogue.wav -i music.mp3 -i sfx.wav \\\\\\n  -filter_complex \\\\\\n  \\"[1:a]volume=1.0[dlg]; \\\\\\n   [2:a]volume=0.3[mus]; \\\\\\n   [3:a]volume=0.5[sfx]; \\\\\\n   [dlg][mus][sfx]amix=inputs=3:duration=first[aout]\\" \\\\\\n  -map 0:v -map \\"[aout]\\" \\\\\\n  -c:v copy -c:a aac -b:a 256k \\\\\\n  output.mp4\\n```\\n\\n### Audio with Timeline Positioning\\n\\n```bash\\n# Position audio at specific timeline points\\nffmpeg -i video.mp4 -i sfx1.wav -i sfx2.wav -filter_complex \\\\\\n  \\"[1:a]adelay=5000|5000[sfx1]; \\\\\\n   [2:a]adelay=12000|12000[sfx2]; \\\\\\n   [0:a][sfx1][sfx2]amix=inputs=3:duration=first[aout]\\" \\\\\\n  -map 0:v -map \\"[aout]\\" \\\\\\n  output.mp4\\n\\n# adelay values are in milliseconds\\n# adelay=5000|5000 = delay 5s on left and right channels\\n```\\n\\n### Ducking (Lower music when dialogue plays)\\n\\n```bash\\n# Automatic ducking using sidechaincompress\\nffmpeg -i video.mp4 -i music.mp3 -filter_complex \\\\\\n  \\"[0:a]asplit[voice][duck_trigger]; \\\\\\n   [1:a][duck_trigger]sidechaincompress=threshold=0.1:ratio=10:attack=100:release=1000[ducked_music]; \\\\\\n   [voice][ducked_music]amix=inputs=2:duration=first[aout]\\" \\\\\\n  -map 0:v -map \\"[aout]\\" \\\\\\n  output.mp4\\n```\\n\\n## Keyframe Animation\\n\\n### Position Animation\\n\\n```bash\\n# Move overlay from left to right over 5 seconds\\nffmpeg -i bg.mp4 -i overlay.png -filter_complex \\\\\\n  \\"[1:v]scale=200:200[ovr]; \\\\\\n   [0:v][ovr]overlay=x=\'min(t*100,W-200)\':y=100[out]\\" \\\\\\n  -map \\"[out]\\" -t 10 output.mp4\\n\\n# x=\'min(t*100,W-200)\': Move 100 pixels/second until reaching right edge\\n```\\n\\n### Opacity Animation\\n\\n```bash\\n# Fade overlay in/out\\nffmpeg -i bg.mp4 -i overlay.png -filter_complex \\\\\\n  \\"[1:v]format=rgba,fade=in:st=0:d=1:alpha=1,fade=out:st=4:d=1:alpha=1[ovr]; \\\\\\n   [0:v][ovr]overlay=10:10[out]\\" \\\\\\n  -map \\"[out]\\" output.mp4\\n\\n# Fade in from 0-1s, fade out from 4-5s\\n```\\n\\n### Scale Animation (Zoom)\\n\\n```bash\\n# Zoom effect (Ken Burns)\\nffmpeg -i photo.jpg -filter_complex \\\\\\n  \\"zoompan=z=\'min(1.5,zoom+0.001)\':d=300:x=\'iw/2-(iw/zoom/2)\':y=\'ih/2-(ih/zoom/2)\':s=1920x1080\\" \\\\\\n  -c:v libx264 -t 10 output.mp4\\n\\n# z=\'min(1.5,zoom+0.001)\': Slowly zoom to 1.5x\\n# d=300: 300 frames duration (10s at 30fps)\\n```\\n\\n## Time Remapping\\n\\n### Speed Changes\\n\\n```bash\\n# 2x speed (fast forward)\\nffmpeg -i input.mp4 -filter:v \\"setpts=0.5*PTS\\" -filter:a \\"atempo=2.0\\" output.mp4\\n\\n# 0.5x speed (slow motion)\\nffmpeg -i input.mp4 -filter:v \\"setpts=2.0*PTS\\" -filter:a \\"atempo=0.5\\" output.mp4\\n\\n# Variable speed (speed ramp)\\n# Slow from 0-5s, normal 5-10s, fast 10-15s\\nffmpeg -i input.mp4 -filter_complex \\\\\\n  \\"[0:v]trim=0:5,setpts=2*PTS[slow]; \\\\\\n   [0:v]trim=5:10,setpts=PTS-STARTPTS[normal]; \\\\\\n   [0:v]trim=10:15,setpts=0.5*PTS-STARTPTS[fast]; \\\\\\n   [slow][normal][fast]concat=n=3:v=1[out]\\" \\\\\\n  -map \\"[out]\\" output.mp4\\n```\\n\\n### Reverse\\n\\n```bash\\n# Reverse video\\nffmpeg -i input.mp4 -vf reverse output.mp4\\n\\n# Reverse segment\\nffmpeg -i input.mp4 -filter_complex \\\\\\n  \\"[0:v]trim=5:10,setpts=PTS-STARTPTS,reverse[rev]; \\\\\\n   [0:v]trim=0:5,setpts=PTS-STARTPTS[before]; \\\\\\n   [0:v]trim=10:20,setpts=PTS-STARTPTS[after]; \\\\\\n   [before][rev][after]concat=n=3:v=1[out]\\" \\\\\\n  -map \\"[out]\\" output.mp4\\n```\\n\\n### Frame Hold (Freeze Frame)\\n\\n```bash\\n# Freeze at 5 seconds for 3 seconds\\nffmpeg -i input.mp4 -filter_complex \\\\\\n  \\"[0:v]trim=0:5[before]; \\\\\\n   [0:v]trim=5:5.033,loop=90:1:0[freeze]; \\\\\\n   [0:v]trim=5:,setpts=PTS-STARTPTS+3/TB[after]; \\\\\\n   [before][freeze][after]concat=n=3:v=1[out]\\" \\\\\\n  -map \\"[out]\\" output.mp4\\n\\n# loop=90:1:0 = 90 frames loop (3s at 30fps)\\n```\\n\\n## Markers and Sync Points\\n\\n### Scene Detection for Auto-Markers\\n\\n```bash\\n# Detect scene changes (potential cut points)\\nffmpeg -i input.mp4 -filter:v \\"select=\'gt(scene,0.3)\',showinfo\\" -f null - 2>&1 | grep pts_time\\n\\n# Output timestamps where scene changes occur (threshold 0.3)\\n```\\n\\n### Audio Sync with Clapboard\\n\\n```bash\\n# Find audio spike (clap) for sync\\nffmpeg -i input.mp4 -af \\"silencedetect=n=-30dB:d=0.5\\" -f null - 2>&1 | grep silence_end\\n\\n# This detects when silence ends (loud sound begins)\\n```\\n\\n### Align Two Clips by Audio\\n\\n```bash\\n# Step 1: Generate audio fingerprints\\n# (Requires external tool like sync-audio-tracks or Praat)\\n\\n# Step 2: Apply calculated offset\\nffmpeg -i video1.mp4 -itsoffset 0.250 -i video2.mp4 \\\\\\n  -map 0:v -map 1:a \\\\\\n  -c:v copy -c:a aac \\\\\\n  synced_output.mp4\\n```\\n\\n## Complex Timeline Example\\n\\n```bash\\n# Full multi-track edit:\\n# - Background video (0-60s)\\n# - B-roll insert (at 10-20s)\\n# - Lower third graphic (at 5-8s)\\n# - Dialogue audio (continuous)\\n# - Music bed (ducked, 30% volume)\\n\\nffmpeg \\\\\\n  -i main.mp4 \\\\\\n  -i broll.mp4 \\\\\\n  -i lowerthird.png \\\\\\n  -i music.mp3 \\\\\\n  -filter_complex \\\\\\n  \\"\\n  [0:v]trim=0:10,setpts=PTS-STARTPTS[main1];\\n  [1:v]trim=0:10,setpts=PTS-STARTPTS[broll];\\n  [0:v]trim=20:60,setpts=PTS-STARTPTS[main2];\\n  [main1][broll][main2]concat=n=3:v=1[base];\\n\\n  [2:v]format=rgba,fade=in:st=0:d=0.5:alpha=1,fade=out:st=2.5:d=0.5:alpha=1[lt];\\n  [base][lt]overlay=0:H-150:enable=\'between(t,5,8)\'[video];\\n\\n  [0:a]volume=1.0[dialogue];\\n  [3:a]volume=0.3[music];\\n  [dialogue][music]amix=inputs=2:duration=first[audio]\\n  \\" \\\\\\n  -map \\"[video]\\" -map \\"[audio]\\" \\\\\\n  -c:v libx264 -crf 18 -preset medium \\\\\\n  -c:a aac -b:a 192k \\\\\\n  final_edit.mp4\\n```\\n\\n## Best Practices\\n\\n1. **Plan your timeline on paper first** - Draw out tracks, timing, transitions\\n2. **Use intermediate files for complex edits** - Don\'t try to do everything in one command\\n3. **Match frame rates before combining** - Avoid judder from mismatched FPS\\n4. **Keep audio and video sync** - Always test playback at multiple points\\n5. **Use setpts=PTS-STARTPTS after trim** - Reset timestamps to avoid gaps\\n6. **Label your filter chains** - Use descriptive names like `[dialogue]` not `[a1]`\\n\\n---\\n\\nThis reference covers timeline editing concepts. For transitions between clips, see the main SKILL.md. For export settings, see `export-optimization.md`.\\n"}]},{"name":"scripts","type":"folder","path":"video-processing-editing/scripts","children":[{"name":"audio_mixer.py","type":"file","path":"video-processing-editing/scripts/audio_mixer.py","size":23697,"content":"#!/usr/bin/env python3\\n\\"\\"\\"\\nAudio Ducking and Mixing Tool\\n\\nAdvanced audio processing for video production:\\n- Automatic ducking (lower music when dialogue plays)\\n- Multi-track mixing with individual volume control\\n- Audio normalization (loudness standards)\\n- Compression and limiting\\n- EQ and filtering\\n\\nUsage:\\n    python audio_mixer.py duck --video input.mp4 --music music.mp3 -o output.mp4\\n    python audio_mixer.py mix --video input.mp4 --tracks audio1.wav audio2.wav -o output.mp4\\n    python audio_mixer.py normalize --video input.mp4 --target -16 -o output.mp4\\n    python audio_mixer.py replace --video input.mp4 --audio new_audio.wav -o output.mp4\\n\\"\\"\\"\\n\\nimport argparse\\nimport subprocess\\nimport sys\\nimport os\\nimport json\\nfrom pathlib import Path\\nfrom dataclasses import dataclass\\nfrom typing import List, Optional, Tuple\\n\\n\\n@dataclass\\nclass AudioTrack:\\n    \\"\\"\\"Represents an audio track for mixing.\\"\\"\\"\\n    path: str\\n    volume: float = 1.0\\n    delay_ms: int = 0\\n    pan: float = 0.0  # -1.0 (left) to 1.0 (right)\\n    fade_in: float = 0.0\\n    fade_out: float = 0.0\\n\\n\\n@dataclass\\nclass DuckingConfig:\\n    \\"\\"\\"Configuration for audio ducking.\\"\\"\\"\\n    threshold: float = 0.03  # Voice detection threshold (0.0-1.0)\\n    ratio: float = 4.0  # Compression ratio when ducking\\n    attack_ms: int = 200  # How fast to duck (ms)\\n    release_ms: int = 1000  # How fast to restore (ms)\\n    music_level: float = 0.2  # Music level when ducked (0.0-1.0)\\n    normal_music_level: float = 0.5  # Music level when not ducking\\n\\n\\ndef run_ffmpeg(cmd: list, description: str = \\"\\") -> bool:\\n    \\"\\"\\"Execute FFmpeg command.\\"\\"\\"\\n    print(f\\"\ud83c\udfb5 {description or \'Processing audio\'}...\\")\\n    try:\\n        result = subprocess.run(cmd, capture_output=True, text=True)\\n        if result.returncode != 0:\\n            print(f\\"\u26a0\ufe0f  {result.stderr[:500]}\\")\\n        return result.returncode == 0\\n    except Exception as e:\\n        print(f\\"\u274c Error: {e}\\")\\n        return False\\n\\n\\ndef get_audio_info(file_path: str) -> Optional[dict]:\\n    \\"\\"\\"Get audio stream information.\\"\\"\\"\\n    cmd = [\\n        \\"ffprobe\\", \\"-v\\", \\"quiet\\",\\n        \\"-print_format\\", \\"json\\",\\n        \\"-show_streams\\",\\n        \\"-select_streams\\", \\"a\\",\\n        file_path\\n    ]\\n    result = subprocess.run(cmd, capture_output=True, text=True)\\n    try:\\n        data = json.loads(result.stdout)\\n        streams = data.get(\\"streams\\", [])\\n        return streams[0] if streams else None\\n    except:\\n        return None\\n\\n\\ndef detect_silence(file_path: str, threshold_db: float = -30, min_duration: float = 0.5) -> List[Tuple[float, float]]:\\n    \\"\\"\\"Detect silent periods in audio.\\"\\"\\"\\n    cmd = [\\n        \\"ffmpeg\\", \\"-i\\", file_path,\\n        \\"-af\\", f\\"silencedetect=n={threshold_db}dB:d={min_duration}\\",\\n        \\"-f\\", \\"null\\", \\"-\\"\\n    ]\\n\\n    result = subprocess.run(cmd, capture_output=True, text=True)\\n\\n    silences = []\\n    current_start = None\\n\\n    for line in result.stderr.split(\\"\\\\n\\"):\\n        if \\"silence_start:\\" in line:\\n            try:\\n                current_start = float(line.split(\\"silence_start:\\")[1].strip())\\n            except:\\n                pass\\n        elif \\"silence_end:\\" in line and current_start is not None:\\n            try:\\n                end = float(line.split(\\"silence_end:\\")[1].split()[0])\\n                silences.append((current_start, end))\\n                current_start = None\\n            except:\\n                pass\\n\\n    return silences\\n\\n\\nclass AudioDucker:\\n    \\"\\"\\"Automatic audio ducking - lower music when dialogue plays.\\"\\"\\"\\n\\n    def __init__(self, config: DuckingConfig = None):\\n        self.config = config or DuckingConfig()\\n\\n    def duck_with_sidechain(\\n        self,\\n        video_path: str,\\n        music_path: str,\\n        output_path: str,\\n        voice_track: int = 0\\n    ) -> bool:\\n        \\"\\"\\"Duck music using sidechain compression from voice track.\\"\\"\\"\\n\\n        config = self.config\\n\\n        # Use sidechaincompress filter\\n        # The voice track triggers compression on the music track\\n        filter_complex = f\\"\\"\\"\\n        [0:a]asplit[voice][duck_trigger];\\n        [1:a]volume={config.normal_music_level}[music_scaled];\\n        [music_scaled][duck_trigger]sidechaincompress=\\n            threshold={config.threshold}:\\n            ratio={config.ratio}:\\n            attack={config.attack_ms}:\\n            release={config.release_ms}:\\n            level_in=1:\\n            level_sc=1\\n        [ducked_music];\\n        [voice][ducked_music]amix=inputs=2:duration=first:weights=1 {config.music_level}[aout]\\n        \\"\\"\\"\\n\\n        cmd = [\\n            \\"ffmpeg\\", \\"-y\\",\\n            \\"-i\\", video_path,\\n            \\"-i\\", music_path,\\n            \\"-filter_complex\\", filter_complex.replace(\\"\\\\n\\", \\"\\"),\\n            \\"-map\\", \\"0:v\\",\\n            \\"-map\\", \\"[aout]\\",\\n            \\"-c:v\\", \\"copy\\",\\n            \\"-c:a\\", \\"aac\\", \\"-b:a\\", \\"256k\\",\\n            output_path\\n        ]\\n\\n        return run_ffmpeg(cmd, \\"Applying sidechain ducking\\")\\n\\n    def duck_with_envelope(\\n        self,\\n        video_path: str,\\n        music_path: str,\\n        output_path: str\\n    ) -> bool:\\n        \\"\\"\\"Duck music based on voice envelope detection.\\"\\"\\"\\n\\n        config = self.config\\n\\n        # Alternative approach using volume envelope\\n        filter_complex = f\\"\\"\\"\\n        [0:a]aresample=48000,\\n             aformat=sample_fmts=fltp:channel_layouts=stereo[voice];\\n        [1:a]aresample=48000,\\n             aformat=sample_fmts=fltp:channel_layouts=stereo,\\n             volume={config.normal_music_level}[music];\\n        [voice]asplit[v1][v2];\\n        [v1]silencedetect=n=-30dB:d=0.3[voice_detect];\\n        [music][voice_detect]sidechaincompress=\\n            threshold=0.02:\\n            ratio={config.ratio}:\\n            attack={config.attack_ms}:\\n            release={config.release_ms}[ducked];\\n        [v2][ducked]amix=inputs=2:duration=first[aout]\\n        \\"\\"\\"\\n\\n        cmd = [\\n            \\"ffmpeg\\", \\"-y\\",\\n            \\"-i\\", video_path,\\n            \\"-i\\", music_path,\\n            \\"-filter_complex\\", filter_complex.replace(\\"\\\\n\\", \\"\\"),\\n            \\"-map\\", \\"0:v\\",\\n            \\"-map\\", \\"[aout]\\",\\n            \\"-c:v\\", \\"copy\\",\\n            \\"-c:a\\", \\"aac\\", \\"-b:a\\", \\"256k\\",\\n            output_path\\n        ]\\n\\n        return run_ffmpeg(cmd, \\"Applying envelope-based ducking\\")\\n\\n    def duck_simple(\\n        self,\\n        video_path: str,\\n        music_path: str,\\n        output_path: str,\\n        voice_volume: float = 1.0,\\n        music_volume: float = 0.3\\n    ) -> bool:\\n        \\"\\"\\"Simple mixing with fixed volumes (no dynamic ducking).\\"\\"\\"\\n\\n        filter_complex = f\\"\\"\\"\\n        [0:a]volume={voice_volume}[voice];\\n        [1:a]volume={music_volume}[music];\\n        [voice][music]amix=inputs=2:duration=first:dropout_transition=3[aout]\\n        \\"\\"\\"\\n\\n        cmd = [\\n            \\"ffmpeg\\", \\"-y\\",\\n            \\"-i\\", video_path,\\n            \\"-i\\", music_path,\\n            \\"-filter_complex\\", filter_complex.replace(\\"\\\\n\\", \\"\\"),\\n            \\"-map\\", \\"0:v\\",\\n            \\"-map\\", \\"[aout]\\",\\n            \\"-c:v\\", \\"copy\\",\\n            \\"-c:a\\", \\"aac\\", \\"-b:a\\", \\"256k\\",\\n            output_path\\n        ]\\n\\n        return run_ffmpeg(cmd, \\"Simple audio mixing\\")\\n\\n\\nclass AudioMixer:\\n    \\"\\"\\"Multi-track audio mixing.\\"\\"\\"\\n\\n    def mix_tracks(\\n        self,\\n        video_path: str,\\n        tracks: List[AudioTrack],\\n        output_path: str,\\n        keep_original: bool = True\\n    ) -> bool:\\n        \\"\\"\\"Mix multiple audio tracks with video.\\"\\"\\"\\n\\n        inputs = [\\"-i\\", video_path]\\n        for track in tracks:\\n            inputs.extend([\\"-i\\", track.path])\\n\\n        # Build filter complex\\n        filter_parts = []\\n        track_labels = []\\n\\n        # Process original video audio if keeping\\n        if keep_original:\\n            filter_parts.append(\\"[0:a]volume=1.0[orig]\\")\\n            track_labels.append(\\"[orig]\\")\\n\\n        # Process each additional track\\n        for i, track in enumerate(tracks):\\n            idx = i + 1  # Account for video input\\n            label = f\\"[t{i}]\\"\\n\\n            parts = [f\\"[{idx}:a]\\"]\\n\\n            # Apply delay if specified\\n            if track.delay_ms > 0:\\n                parts.append(f\\"adelay={track.delay_ms}|{track.delay_ms}\\")\\n\\n            # Apply volume\\n            parts.append(f\\"volume={track.volume}\\")\\n\\n            # Apply fade in/out\\n            if track.fade_in > 0:\\n                parts.append(f\\"afade=t=in:st=0:d={track.fade_in}\\")\\n            if track.fade_out > 0:\\n                parts.append(f\\"afade=t=out:st=0:d={track.fade_out}\\")\\n\\n            # Apply pan\\n            if track.pan != 0:\\n                left = 1.0 - max(0, track.pan)\\n                right = 1.0 + min(0, track.pan)\\n                parts.append(f\\"pan=stereo|c0={left}*c0|c1={right}*c1\\")\\n\\n            filter_parts.append(\\",\\".join(parts) + label)\\n            track_labels.append(label)\\n\\n        # Mix all tracks\\n        num_inputs = len(track_labels)\\n        mix_filter = \\"\\".join(track_labels) + f\\"amix=inputs={num_inputs}:duration=first:dropout_transition=3[aout]\\"\\n        filter_parts.append(mix_filter)\\n\\n        filter_complex = \\";\\".join(filter_parts)\\n\\n        cmd = [\\n            \\"ffmpeg\\", \\"-y\\",\\n            *inputs,\\n            \\"-filter_complex\\", filter_complex,\\n            \\"-map\\", \\"0:v\\",\\n            \\"-map\\", \\"[aout]\\",\\n            \\"-c:v\\", \\"copy\\",\\n            \\"-c:a\\", \\"aac\\", \\"-b:a\\", \\"256k\\",\\n            output_path\\n        ]\\n\\n        return run_ffmpeg(cmd, f\\"Mixing {num_inputs} audio tracks\\")\\n\\n    def mix_with_timing(\\n        self,\\n        video_path: str,\\n        audio_path: str,\\n        output_path: str,\\n        start_time: float = 0.0,\\n        volume: float = 1.0,\\n        loop: bool = False\\n    ) -> bool:\\n        \\"\\"\\"Add audio track starting at specific time.\\"\\"\\"\\n\\n        delay_ms = int(start_time * 1000)\\n\\n        if loop:\\n            # Loop audio to fill video duration\\n            filter_complex = f\\"\\"\\"\\n            [1:a]aloop=loop=-1:size=2e+09,adelay={delay_ms}|{delay_ms},volume={volume}[music];\\n            [0:a][music]amix=inputs=2:duration=first[aout]\\n            \\"\\"\\"\\n        else:\\n            filter_complex = f\\"\\"\\"\\n            [1:a]adelay={delay_ms}|{delay_ms},volume={volume}[music];\\n            [0:a][music]amix=inputs=2:duration=first[aout]\\n            \\"\\"\\"\\n\\n        cmd = [\\n            \\"ffmpeg\\", \\"-y\\",\\n            \\"-i\\", video_path,\\n            \\"-i\\", audio_path,\\n            \\"-filter_complex\\", filter_complex.replace(\\"\\\\n\\", \\"\\"),\\n            \\"-map\\", \\"0:v\\",\\n            \\"-map\\", \\"[aout]\\",\\n            \\"-c:v\\", \\"copy\\",\\n            \\"-c:a\\", \\"aac\\", \\"-b:a\\", \\"256k\\",\\n            output_path\\n        ]\\n\\n        return run_ffmpeg(cmd, f\\"Adding audio at {start_time}s\\")\\n\\n\\nclass AudioNormalizer:\\n    \\"\\"\\"Audio normalization and loudness processing.\\"\\"\\"\\n\\n    def normalize_loudness(\\n        self,\\n        input_path: str,\\n        output_path: str,\\n        target_lufs: float = -16.0,\\n        target_tp: float = -1.5,\\n        target_lra: float = 11.0\\n    ) -> bool:\\n        \\"\\"\\"Normalize audio to broadcast loudness standard (EBU R128).\\"\\"\\"\\n\\n        # Two-pass loudness normalization\\n        # Pass 1: Measure\\n        cmd_measure = [\\n            \\"ffmpeg\\", \\"-i\\", input_path,\\n            \\"-af\\", \\"loudnorm=I=-16:TP=-1.5:LRA=11:print_format=json\\",\\n            \\"-f\\", \\"null\\", \\"-\\"\\n        ]\\n\\n        print(\\"\ud83d\udccf Measuring loudness...\\")\\n        result = subprocess.run(cmd_measure, capture_output=True, text=True)\\n\\n        # Parse loudness measurements from stderr\\n        measured = {}\\n        try:\\n            # Find JSON block in output\\n            stderr = result.stderr\\n            json_start = stderr.rfind(\\"{\\")\\n            json_end = stderr.rfind(\\"}\\") + 1\\n            if json_start >= 0 and json_end > json_start:\\n                json_str = stderr[json_start:json_end]\\n                measured = json.loads(json_str)\\n        except:\\n            print(\\"\u26a0\ufe0f  Could not parse loudness data, using single-pass\\")\\n\\n        if measured:\\n            # Pass 2: Apply normalization with measured values\\n            filter_str = (\\n                f\\"loudnorm=I={target_lufs}:TP={target_tp}:LRA={target_lra}:\\"\\n                f\\"measured_I={measured.get(\'input_i\', -24)}:\\"\\n                f\\"measured_TP={measured.get(\'input_tp\', -2)}:\\"\\n                f\\"measured_LRA={measured.get(\'input_lra\', 7)}:\\"\\n                f\\"measured_thresh={measured.get(\'input_thresh\', -34)}:\\"\\n                f\\"offset={measured.get(\'target_offset\', 0)}:linear=true\\"\\n            )\\n        else:\\n            # Single pass\\n            filter_str = f\\"loudnorm=I={target_lufs}:TP={target_tp}:LRA={target_lra}\\"\\n\\n        # Check if input is video or audio only\\n        has_video = False\\n        probe_cmd = [\\"ffprobe\\", \\"-v\\", \\"quiet\\", \\"-show_streams\\", input_path]\\n        probe_result = subprocess.run(probe_cmd, capture_output=True, text=True)\\n        has_video = \\"codec_type=video\\" in probe_result.stdout\\n\\n        if has_video:\\n            cmd = [\\n                \\"ffmpeg\\", \\"-y\\",\\n                \\"-i\\", input_path,\\n                \\"-af\\", filter_str,\\n                \\"-c:v\\", \\"copy\\",\\n                \\"-c:a\\", \\"aac\\", \\"-b:a\\", \\"256k\\",\\n                output_path\\n            ]\\n        else:\\n            cmd = [\\n                \\"ffmpeg\\", \\"-y\\",\\n                \\"-i\\", input_path,\\n                \\"-af\\", filter_str,\\n                \\"-c:a\\", \\"aac\\", \\"-b:a\\", \\"256k\\",\\n                output_path\\n            ]\\n\\n        return run_ffmpeg(cmd, f\\"Normalizing to {target_lufs} LUFS\\")\\n\\n    def normalize_peak(\\n        self,\\n        input_path: str,\\n        output_path: str,\\n        target_db: float = -1.0\\n    ) -> bool:\\n        \\"\\"\\"Normalize audio to peak level.\\"\\"\\"\\n\\n        # Detect current peak\\n        cmd_detect = [\\n            \\"ffmpeg\\", \\"-i\\", input_path,\\n            \\"-af\\", \\"volumedetect\\",\\n            \\"-f\\", \\"null\\", \\"-\\"\\n        ]\\n\\n        result = subprocess.run(cmd_detect, capture_output=True, text=True)\\n\\n        max_volume = 0\\n        for line in result.stderr.split(\\"\\\\n\\"):\\n            if \\"max_volume:\\" in line:\\n                try:\\n                    max_volume = float(line.split(\\"max_volume:\\")[1].split()[0])\\n                except:\\n                    pass\\n\\n        adjustment = target_db - max_volume\\n\\n        cmd = [\\n            \\"ffmpeg\\", \\"-y\\",\\n            \\"-i\\", input_path,\\n            \\"-af\\", f\\"volume={adjustment}dB\\",\\n            \\"-c:v\\", \\"copy\\",\\n            \\"-c:a\\", \\"aac\\", \\"-b:a\\", \\"256k\\",\\n            output_path\\n        ]\\n\\n        return run_ffmpeg(cmd, f\\"Normalizing peak to {target_db}dB (adjustment: {adjustment:+.1f}dB)\\")\\n\\n\\nclass AudioProcessor:\\n    \\"\\"\\"Audio effects and processing.\\"\\"\\"\\n\\n    def apply_compression(\\n        self,\\n        input_path: str,\\n        output_path: str,\\n        threshold_db: float = -20,\\n        ratio: float = 4,\\n        attack_ms: float = 20,\\n        release_ms: float = 250\\n    ) -> bool:\\n        \\"\\"\\"Apply dynamic range compression.\\"\\"\\"\\n\\n        filter_str = (\\n            f\\"acompressor=threshold={threshold_db}dB:\\"\\n            f\\"ratio={ratio}:\\"\\n            f\\"attack={attack_ms}:\\"\\n            f\\"release={release_ms}:\\"\\n            f\\"makeup=2\\"\\n        )\\n\\n        cmd = [\\n            \\"ffmpeg\\", \\"-y\\",\\n            \\"-i\\", input_path,\\n            \\"-af\\", filter_str,\\n            \\"-c:v\\", \\"copy\\",\\n            \\"-c:a\\", \\"aac\\", \\"-b:a\\", \\"256k\\",\\n            output_path\\n        ]\\n\\n        return run_ffmpeg(cmd, \\"Applying compression\\")\\n\\n    def apply_eq(\\n        self,\\n        input_path: str,\\n        output_path: str,\\n        preset: str = \\"voice\\"\\n    ) -> bool:\\n        \\"\\"\\"Apply EQ preset.\\"\\"\\"\\n\\n        presets = {\\n            \\"voice\\": \\"highpass=f=80,lowpass=f=12000,equalizer=f=200:t=h:w=200:g=-3,equalizer=f=3000:t=h:w=1000:g=3\\",\\n            \\"music\\": \\"equalizer=f=60:t=h:w=50:g=2,equalizer=f=10000:t=h:w=2000:g=2\\",\\n            \\"podcast\\": \\"highpass=f=100,lowpass=f=10000,acompressor=threshold=-20dB:ratio=3:attack=20:release=250\\",\\n            \\"warm\\": \\"equalizer=f=100:t=h:w=100:g=3,equalizer=f=8000:t=h:w=2000:g=-2\\",\\n            \\"bright\\": \\"equalizer=f=100:t=h:w=100:g=-2,equalizer=f=5000:t=h:w=2000:g=3\\"\\n        }\\n\\n        filter_str = presets.get(preset, presets[\\"voice\\"])\\n\\n        cmd = [\\n            \\"ffmpeg\\", \\"-y\\",\\n            \\"-i\\", input_path,\\n            \\"-af\\", filter_str,\\n            \\"-c:v\\", \\"copy\\",\\n            \\"-c:a\\", \\"aac\\", \\"-b:a\\", \\"256k\\",\\n            output_path\\n        ]\\n\\n        return run_ffmpeg(cmd, f\\"Applying {preset} EQ\\")\\n\\n    def remove_noise(\\n        self,\\n        input_path: str,\\n        output_path: str,\\n        noise_reduction_db: float = 12\\n    ) -> bool:\\n        \\"\\"\\"Apply noise reduction (simple high-pass + gate).\\"\\"\\"\\n\\n        # Basic noise reduction using highpass and noise gate\\n        filter_str = f\\"highpass=f=80,agate=threshold=0.01:ratio=2:attack=25:release=100\\"\\n\\n        cmd = [\\n            \\"ffmpeg\\", \\"-y\\",\\n            \\"-i\\", input_path,\\n            \\"-af\\", filter_str,\\n            \\"-c:v\\", \\"copy\\",\\n            \\"-c:a\\", \\"aac\\", \\"-b:a\\", \\"256k\\",\\n            output_path\\n        ]\\n\\n        return run_ffmpeg(cmd, \\"Applying noise reduction\\")\\n\\n\\ndef main():\\n    parser = argparse.ArgumentParser(\\n        description=\\"Audio Ducking and Mixing Tool\\",\\n        formatter_class=argparse.RawDescriptionHelpFormatter,\\n        epilog=\\"\\"\\"\\nExamples:\\n    # Auto-duck music under dialogue\\n    python audio_mixer.py duck --video interview.mp4 --music background.mp3 -o output.mp4\\n\\n    # Simple mix with fixed volumes\\n    python audio_mixer.py mix-simple --video video.mp4 --music music.mp3 --voice-vol 1.0 --music-vol 0.3 -o output.mp4\\n\\n    # Mix multiple tracks\\n    python audio_mixer.py mix --video video.mp4 --tracks dialogue.wav music.mp3 sfx.wav -o output.mp4\\n\\n    # Normalize to broadcast standard\\n    python audio_mixer.py normalize --video input.mp4 --target -16 -o output.mp4\\n\\n    # Replace audio entirely\\n    python audio_mixer.py replace --video input.mp4 --audio new_audio.wav -o output.mp4\\n\\n    # Apply EQ preset\\n    python audio_mixer.py eq --video input.mp4 --preset voice -o output.mp4\\n\\n    # Add compression\\n    python audio_mixer.py compress --video input.mp4 --threshold -20 --ratio 4 -o output.mp4\\n        \\"\\"\\"\\n    )\\n\\n    subparsers = parser.add_subparsers(dest=\\"command\\", help=\\"Command to run\\")\\n\\n    # Duck command\\n    duck_parser = subparsers.add_parser(\\"duck\\", help=\\"Duck music under dialogue\\")\\n    duck_parser.add_argument(\\"--video\\", \\"-v\\", required=True, help=\\"Input video with dialogue\\")\\n    duck_parser.add_argument(\\"--music\\", \\"-m\\", required=True, help=\\"Music track to duck\\")\\n    duck_parser.add_argument(\\"--threshold\\", type=float, default=0.03, help=\\"Voice detection threshold\\")\\n    duck_parser.add_argument(\\"--ratio\\", type=float, default=4.0, help=\\"Ducking ratio\\")\\n    duck_parser.add_argument(\\"--attack\\", type=int, default=200, help=\\"Attack time (ms)\\")\\n    duck_parser.add_argument(\\"--release\\", type=int, default=1000, help=\\"Release time (ms)\\")\\n    duck_parser.add_argument(\\"-o\\", \\"--output\\", default=\\"ducked.mp4\\", help=\\"Output file\\")\\n\\n    # Mix simple command\\n    mix_simple_parser = subparsers.add_parser(\\"mix-simple\\", help=\\"Simple mix with fixed volumes\\")\\n    mix_simple_parser.add_argument(\\"--video\\", \\"-v\\", required=True, help=\\"Input video\\")\\n    mix_simple_parser.add_argument(\\"--music\\", \\"-m\\", required=True, help=\\"Music track\\")\\n    mix_simple_parser.add_argument(\\"--voice-vol\\", type=float, default=1.0, help=\\"Voice volume\\")\\n    mix_simple_parser.add_argument(\\"--music-vol\\", type=float, default=0.3, help=\\"Music volume\\")\\n    mix_simple_parser.add_argument(\\"-o\\", \\"--output\\", default=\\"mixed.mp4\\", help=\\"Output file\\")\\n\\n    # Mix command\\n    mix_parser = subparsers.add_parser(\\"mix\\", help=\\"Mix multiple tracks\\")\\n    mix_parser.add_argument(\\"--video\\", \\"-v\\", required=True, help=\\"Input video\\")\\n    mix_parser.add_argument(\\"--tracks\\", nargs=\\"+\\", required=True, help=\\"Audio tracks to mix\\")\\n    mix_parser.add_argument(\\"--volumes\\", nargs=\\"+\\", type=float, help=\\"Volume for each track\\")\\n    mix_parser.add_argument(\\"--no-original\\", action=\\"store_true\\", help=\\"Don\'t include original audio\\")\\n    mix_parser.add_argument(\\"-o\\", \\"--output\\", default=\\"mixed.mp4\\", help=\\"Output file\\")\\n\\n    # Normalize command\\n    norm_parser = subparsers.add_parser(\\"normalize\\", help=\\"Normalize audio loudness\\")\\n    norm_parser.add_argument(\\"--video\\", \\"-v\\", required=True, help=\\"Input file\\")\\n    norm_parser.add_argument(\\"--target\\", type=float, default=-16.0, help=\\"Target LUFS\\")\\n    norm_parser.add_argument(\\"--peak\\", action=\\"store_true\\", help=\\"Use peak normalization instead\\")\\n    norm_parser.add_argument(\\"-o\\", \\"--output\\", default=\\"normalized.mp4\\", help=\\"Output file\\")\\n\\n    # Replace command\\n    replace_parser = subparsers.add_parser(\\"replace\\", help=\\"Replace video audio\\")\\n    replace_parser.add_argument(\\"--video\\", \\"-v\\", required=True, help=\\"Input video\\")\\n    replace_parser.add_argument(\\"--audio\\", \\"-a\\", required=True, help=\\"New audio track\\")\\n    replace_parser.add_argument(\\"-o\\", \\"--output\\", default=\\"replaced.mp4\\", help=\\"Output file\\")\\n\\n    # EQ command\\n    eq_parser = subparsers.add_parser(\\"eq\\", help=\\"Apply EQ preset\\")\\n    eq_parser.add_argument(\\"--video\\", \\"-v\\", required=True, help=\\"Input file\\")\\n    eq_parser.add_argument(\\"--preset\\", choices=[\\"voice\\", \\"music\\", \\"podcast\\", \\"warm\\", \\"bright\\"], default=\\"voice\\")\\n    eq_parser.add_argument(\\"-o\\", \\"--output\\", default=\\"eq.mp4\\", help=\\"Output file\\")\\n\\n    # Compress command\\n    comp_parser = subparsers.add_parser(\\"compress\\", help=\\"Apply compression\\")\\n    comp_parser.add_argument(\\"--video\\", \\"-v\\", required=True, help=\\"Input file\\")\\n    comp_parser.add_argument(\\"--threshold\\", type=float, default=-20, help=\\"Threshold (dB)\\")\\n    comp_parser.add_argument(\\"--ratio\\", type=float, default=4, help=\\"Compression ratio\\")\\n    comp_parser.add_argument(\\"-o\\", \\"--output\\", default=\\"compressed.mp4\\", help=\\"Output file\\")\\n\\n    args = parser.parse_args()\\n\\n    if not args.command:\\n        parser.print_help()\\n        sys.exit(1)\\n\\n    success = False\\n\\n    if args.command == \\"duck\\":\\n        config = DuckingConfig(\\n            threshold=args.threshold,\\n            ratio=args.ratio,\\n            attack_ms=args.attack,\\n            release_ms=args.release\\n        )\\n        ducker = AudioDucker(config)\\n        success = ducker.duck_with_sidechain(args.video, args.music, args.output)\\n\\n    elif args.command == \\"mix-simple\\":\\n        ducker = AudioDucker()\\n        success = ducker.duck_simple(\\n            args.video, args.music, args.output,\\n            voice_volume=args.voice_vol, music_volume=args.music_vol\\n        )\\n\\n    elif args.command == \\"mix\\":\\n        volumes = args.volumes or [1.0] * len(args.tracks)\\n        tracks = [\\n            AudioTrack(path=p, volume=v)\\n            for p, v in zip(args.tracks, volumes)\\n        ]\\n        mixer = AudioMixer()\\n        success = mixer.mix_tracks(\\n            args.video, tracks, args.output,\\n            keep_original=not args.no_original\\n        )\\n\\n    elif args.command == \\"normalize\\":\\n        normalizer = AudioNormalizer()\\n        if args.peak:\\n            success = normalizer.normalize_peak(args.video, args.output, args.target)\\n        else:\\n            success = normalizer.normalize_loudness(args.video, args.output, args.target)\\n\\n    elif args.command == \\"replace\\":\\n        cmd = [\\n            \\"ffmpeg\\", \\"-y\\",\\n            \\"-i\\", args.video,\\n            \\"-i\\", args.audio,\\n            \\"-map\\", \\"0:v\\",\\n            \\"-map\\", \\"1:a\\",\\n            \\"-c:v\\", \\"copy\\",\\n            \\"-c:a\\", \\"aac\\", \\"-b:a\\", \\"256k\\",\\n            \\"-shortest\\",\\n            args.output\\n        ]\\n        success = run_ffmpeg(cmd, \\"Replacing audio\\")\\n\\n    elif args.command == \\"eq\\":\\n        processor = AudioProcessor()\\n        success = processor.apply_eq(args.video, args.output, args.preset)\\n\\n    elif args.command == \\"compress\\":\\n        processor = AudioProcessor()\\n        success = processor.apply_compression(\\n            args.video, args.output,\\n            threshold_db=args.threshold, ratio=args.ratio\\n        )\\n\\n    if success:\\n        print(f\\"\u2705 Created: {args.output}\\")\\n    else:\\n        print(\\"\u274c Failed\\")\\n        sys.exit(1)\\n\\n\\nif __name__ == \\"__main__\\":\\n    main()\\n"},{"name":"batch_processor.py","type":"file","path":"video-processing-editing/scripts/batch_processor.py","size":16695,"content":"#!/usr/bin/env python3\\n\\"\\"\\"\\nbatch_processor.py - Parallel batch video processing\\n\\nFeatures:\\n- Process multiple videos in parallel\\n- Platform-specific batch exports\\n- Progress tracking and logging\\n- Error handling and retries\\n- Resource management (CPU/GPU)\\n\\nUsage:\\n    python batch_processor.py export youtube videos/*.mp4 -o exports/\\n    python batch_processor.py cut videos/*.mp4 --start 5 --end 60 -o trimmed/\\n    python batch_processor.py resize videos/*.mp4 --width 1920 --height 1080 -o resized/\\n\\"\\"\\"\\n\\nimport argparse\\nimport json\\nimport multiprocessing\\nimport os\\nimport subprocess\\nimport sys\\nimport time\\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\\nfrom dataclasses import dataclass\\nfrom pathlib import Path\\nfrom typing import List, Optional, Callable, Dict, Any\\n\\n\\n@dataclass\\nclass ProcessingJob:\\n    \\"\\"\\"Single video processing job\\"\\"\\"\\n    input_file: Path\\n    output_file: Path\\n    operation: str\\n    params: Dict[str, Any]\\n    status: str = \'pending\'  # pending, running, completed, failed\\n    error: Optional[str] = None\\n    start_time: Optional[float] = None\\n    end_time: Optional[float] = None\\n\\n\\n@dataclass\\nclass BatchStats:\\n    \\"\\"\\"Batch processing statistics\\"\\"\\"\\n    total: int = 0\\n    completed: int = 0\\n    failed: int = 0\\n    skipped: int = 0\\n    total_time: float = 0.0\\n    avg_time: float = 0.0\\n\\n\\nclass BatchProcessor:\\n    \\"\\"\\"Parallel batch video processor\\"\\"\\"\\n\\n    def __init__(\\n        self,\\n        max_workers: Optional[int] = None,\\n        gpu_acceleration: bool = True,\\n        verbose: bool = False\\n    ):\\n        self.max_workers = max_workers or max(1, multiprocessing.cpu_count() - 1)\\n        self.gpu_acceleration = gpu_acceleration\\n        self.verbose = verbose\\n        self.jobs: List[ProcessingJob] = []\\n        self.stats = BatchStats()\\n\\n    def add_job(self, job: ProcessingJob):\\n        \\"\\"\\"Add processing job to queue\\"\\"\\"\\n        self.jobs.append(job)\\n        self.stats.total += 1\\n\\n    def process_all(self, skip_existing: bool = True) -> BatchStats:\\n        \\"\\"\\"Process all jobs in parallel\\"\\"\\"\\n        print(f\\"Processing {len(self.jobs)} videos with {self.max_workers} workers...\\")\\n\\n        start_time = time.time()\\n\\n        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:\\n            futures = {}\\n\\n            for job in self.jobs:\\n                if skip_existing and job.output_file.exists():\\n                    job.status = \'skipped\'\\n                    self.stats.skipped += 1\\n                    if self.verbose:\\n                        print(f\\"Skipping existing: {job.output_file}\\")\\n                    continue\\n\\n                future = executor.submit(self._process_job, job)\\n                futures[future] = job\\n\\n            for future in as_completed(futures):\\n                job = futures[future]\\n                try:\\n                    result = future.result()\\n                    if result.status == \'completed\':\\n                        self.stats.completed += 1\\n                    else:\\n                        self.stats.failed += 1\\n\\n                    self._print_progress()\\n\\n                except Exception as e:\\n                    job.status = \'failed\'\\n                    job.error = str(e)\\n                    self.stats.failed += 1\\n                    print(f\\"Error processing {job.input_file}: {e}\\")\\n\\n        self.stats.total_time = time.time() - start_time\\n        self.stats.avg_time = self.stats.total_time / max(1, self.stats.completed)\\n\\n        self._print_summary()\\n\\n        return self.stats\\n\\n    def _process_job(self, job: ProcessingJob) -> ProcessingJob:\\n        \\"\\"\\"Process single video job\\"\\"\\"\\n        job.status = \'running\'\\n        job.start_time = time.time()\\n\\n        try:\\n            if job.operation == \'export\':\\n                self._export_video(job)\\n            elif job.operation == \'cut\':\\n                self._cut_video(job)\\n            elif job.operation == \'resize\':\\n                self._resize_video(job)\\n            elif job.operation == \'convert\':\\n                self._convert_video(job)\\n            elif job.operation == \'audio_extract\':\\n                self._extract_audio(job)\\n            else:\\n                raise ValueError(f\\"Unknown operation: {job.operation}\\")\\n\\n            job.status = \'completed\'\\n\\n        except Exception as e:\\n            job.status = \'failed\'\\n            job.error = str(e)\\n\\n        finally:\\n            job.end_time = time.time()\\n\\n        return job\\n\\n    def _export_video(self, job: ProcessingJob):\\n        \\"\\"\\"Export video for platform\\"\\"\\"\\n        platform = job.params[\'platform\']\\n        quality = job.params.get(\'quality\', \'high\')\\n\\n        presets = {\\n            \'youtube\': {\\n                \'resolution\': \'1920x1080\',\\n                \'fps\': 30,\\n                \'crf\': {\'draft\': 28, \'medium\': 23, \'high\': 18},\\n                \'preset\': {\'draft\': \'ultrafast\', \'medium\': \'medium\', \'high\': \'slow\'},\\n                \'audio_bitrate\': \'192k\'\\n            },\\n            \'instagram_story\': {\\n                \'resolution\': \'1080x1920\',\\n                \'fps\': 30,\\n                \'crf\': {\'draft\': 28, \'medium\': 23, \'high\': 20},\\n                \'preset\': {\'draft\': \'ultrafast\', \'medium\': \'medium\', \'high\': \'medium\'},\\n                \'audio_bitrate\': \'128k\',\\n                \'max_duration\': 15\\n            },\\n            \'instagram_reel\': {\\n                \'resolution\': \'1080x1920\',\\n                \'fps\': 30,\\n                \'crf\': {\'draft\': 28, \'medium\': 23, \'high\': 20},\\n                \'preset\': {\'draft\': \'ultrafast\', \'medium\': \'medium\', \'high\': \'medium\'},\\n                \'audio_bitrate\': \'128k\',\\n                \'max_duration\': 90\\n            },\\n            \'twitter\': {\\n                \'resolution\': \'1280x720\',\\n                \'fps\': 30,\\n                \'crf\': {\'draft\': 28, \'medium\': 23, \'high\': 20},\\n                \'preset\': {\'draft\': \'ultrafast\', \'medium\': \'medium\', \'high\': \'medium\'},\\n                \'audio_bitrate\': \'128k\',\\n                \'maxrate\': \'5000k\',\\n                \'bufsize\': \'10000k\'\\n            },\\n            \'tiktok\': {\\n                \'resolution\': \'1080x1920\',\\n                \'fps\': 30,\\n                \'crf\': {\'draft\': 28, \'medium\': 23, \'high\': 20},\\n                \'preset\': {\'draft\': \'ultrafast\', \'medium\': \'medium\', \'high\': \'medium\'},\\n                \'audio_bitrate\': \'128k\'\\n            }\\n        }\\n\\n        config = presets[platform]\\n\\n        cmd = [\\n            \'ffmpeg\', \'-y\',\\n            \'-i\', str(job.input_file),\\n            \'-c:v\', \'libx264\',\\n            \'-preset\', config[\'preset\'][quality],\\n            \'-crf\', str(config[\'crf\'][quality]),\\n            \'-s\', config[\'resolution\'],\\n            \'-r\', str(config[\'fps\']),\\n            \'-pix_fmt\', \'yuv420p\',\\n            \'-color_primaries\', \'bt709\',\\n            \'-color_trc\', \'bt709\',\\n            \'-colorspace\', \'bt709\',\\n            \'-movflags\', \'+faststart\',\\n            \'-c:a\', \'aac\',\\n            \'-b:a\', config[\'audio_bitrate\'],\\n            \'-ar\', \'48000\'\\n        ]\\n\\n        if \'maxrate\' in config:\\n            cmd += [\'-maxrate\', config[\'maxrate\'], \'-bufsize\', config[\'bufsize\']]\\n\\n        if \'max_duration\' in config:\\n            cmd += [\'-t\', str(config[\'max_duration\'])]\\n\\n        cmd.append(str(job.output_file))\\n\\n        self._run_ffmpeg(cmd)\\n\\n    def _cut_video(self, job: ProcessingJob):\\n        \\"\\"\\"Cut/trim video\\"\\"\\"\\n        start = job.params[\'start\']\\n        end = job.params.get(\'end\')\\n\\n        cmd = [\\n            \'ffmpeg\', \'-y\',\\n            \'-ss\', str(max(0, start - 2)),\\n            \'-i\', str(job.input_file),\\n            \'-ss\', \'2\'\\n        ]\\n\\n        if end:\\n            duration = end - start\\n            cmd += [\'-t\', str(duration)]\\n\\n        cmd += [\\n            \'-c:v\', \'libx264\',\\n            \'-crf\', \'18\',\\n            \'-preset\', \'medium\',\\n            \'-c:a\', \'aac\',\\n            \'-b:a\', \'192k\',\\n            str(job.output_file)\\n        ]\\n\\n        self._run_ffmpeg(cmd)\\n\\n    def _resize_video(self, job: ProcessingJob):\\n        \\"\\"\\"Resize video\\"\\"\\"\\n        width = job.params.get(\'width\')\\n        height = job.params.get(\'height\')\\n\\n        if width and height:\\n            scale = f\\"{width}:{height}\\"\\n        elif width:\\n            scale = f\\"{width}:-2\\"\\n        elif height:\\n            scale = f\\"-2:{height}\\"\\n        else:\\n            raise ValueError(\\"Must specify width and/or height\\")\\n\\n        cmd = [\\n            \'ffmpeg\', \'-y\',\\n            \'-i\', str(job.input_file),\\n            \'-vf\', f\\"scale={scale}\\",\\n            \'-c:v\', \'libx264\',\\n            \'-crf\', \'18\',\\n            \'-preset\', \'medium\',\\n            \'-c:a\', \'copy\',\\n            str(job.output_file)\\n        ]\\n\\n        self._run_ffmpeg(cmd)\\n\\n    def _convert_video(self, job: ProcessingJob):\\n        \\"\\"\\"Convert video format\\"\\"\\"\\n        codec = job.params.get(\'codec\', \'libx264\')\\n        quality = job.params.get(\'quality\', 18)\\n\\n        cmd = [\\n            \'ffmpeg\', \'-y\',\\n            \'-i\', str(job.input_file),\\n            \'-c:v\', codec,\\n            \'-crf\', str(quality),\\n            \'-preset\', \'medium\',\\n            \'-c:a\', \'aac\',\\n            \'-b:a\', \'192k\',\\n            str(job.output_file)\\n        ]\\n\\n        self._run_ffmpeg(cmd)\\n\\n    def _extract_audio(self, job: ProcessingJob):\\n        \\"\\"\\"Extract audio from video\\"\\"\\"\\n        audio_format = job.params.get(\'format\', \'mp3\')\\n        bitrate = job.params.get(\'bitrate\', \'192k\')\\n\\n        cmd = [\\n            \'ffmpeg\', \'-y\',\\n            \'-i\', str(job.input_file),\\n            \'-vn\',  # No video\\n            \'-c:a\', \'libmp3lame\' if audio_format == \'mp3\' else \'aac\',\\n            \'-b:a\', bitrate,\\n            str(job.output_file)\\n        ]\\n\\n        self._run_ffmpeg(cmd)\\n\\n    def _run_ffmpeg(self, cmd: List[str]):\\n        \\"\\"\\"Run FFmpeg command with error handling\\"\\"\\"\\n        if self.verbose:\\n            print(f\\"Running: {\' \'.join(cmd)}\\")\\n\\n        result = subprocess.run(\\n            cmd,\\n            capture_output=True,\\n            text=True,\\n            check=False\\n        )\\n\\n        if result.returncode != 0:\\n            raise RuntimeError(f\\"FFmpeg failed: {result.stderr}\\")\\n\\n    def _print_progress(self):\\n        \\"\\"\\"Print current progress\\"\\"\\"\\n        processed = self.stats.completed + self.stats.failed\\n        total = self.stats.total - self.stats.skipped\\n\\n        if total > 0:\\n            percent = (processed / total) * 100\\n            print(f\\"Progress: {processed}/{total} ({percent:.1f}%) - \\"\\n                  f\\"Completed: {self.stats.completed}, Failed: {self.stats.failed}\\")\\n\\n    def _print_summary(self):\\n        \\"\\"\\"Print final summary\\"\\"\\"\\n        print(\\"\\\\n\\" + \\"=\\"*60)\\n        print(\\"BATCH PROCESSING COMPLETE\\")\\n        print(\\"=\\"*60)\\n        print(f\\"Total jobs:      {self.stats.total}\\")\\n        print(f\\"Completed:       {self.stats.completed}\\")\\n        print(f\\"Failed:          {self.stats.failed}\\")\\n        print(f\\"Skipped:         {self.stats.skipped}\\")\\n        print(f\\"Total time:      {self.stats.total_time:.1f}s\\")\\n        print(f\\"Avg time/video:  {self.stats.avg_time:.1f}s\\")\\n        print(\\"=\\"*60)\\n\\n        if self.stats.failed > 0:\\n            print(\\"\\\\nFailed jobs:\\")\\n            for job in self.jobs:\\n                if job.status == \'failed\':\\n                    print(f\\"  - {job.input_file}: {job.error}\\")\\n\\n\\ndef main():\\n    parser = argparse.ArgumentParser(description=\'Batch video processor\')\\n    parser.add_argument(\'-v\', \'--verbose\', action=\'store_true\', help=\'Verbose output\')\\n    parser.add_argument(\'-w\', \'--workers\', type=int, help=\'Max parallel workers\')\\n    parser.add_argument(\'--no-gpu\', action=\'store_true\', help=\'Disable GPU acceleration\')\\n    parser.add_argument(\'--overwrite\', action=\'store_true\', help=\'Overwrite existing files\')\\n\\n    subparsers = parser.add_subparsers(dest=\'command\', required=True)\\n\\n    # Export command\\n    export_parser = subparsers.add_parser(\'export\', help=\'Batch export for platform\')\\n    export_parser.add_argument(\'platform\', choices=[\\n        \'youtube\', \'instagram_story\', \'instagram_reel\', \'twitter\', \'tiktok\'\\n    ])\\n    export_parser.add_argument(\'inputs\', nargs=\'+\', help=\'Input video files (or glob pattern)\')\\n    export_parser.add_argument(\'-o\', \'--output-dir\', required=True, help=\'Output directory\')\\n    export_parser.add_argument(\'-q\', \'--quality\', choices=[\'draft\', \'medium\', \'high\'],\\n                                default=\'high\', help=\'Export quality\')\\n\\n    # Cut command\\n    cut_parser = subparsers.add_parser(\'cut\', help=\'Batch cut/trim\')\\n    cut_parser.add_argument(\'inputs\', nargs=\'+\', help=\'Input video files\')\\n    cut_parser.add_argument(\'-s\', \'--start\', type=float, required=True, help=\'Start time (seconds)\')\\n    cut_parser.add_argument(\'-e\', \'--end\', type=float, help=\'End time (seconds)\')\\n    cut_parser.add_argument(\'-o\', \'--output-dir\', required=True, help=\'Output directory\')\\n\\n    # Resize command\\n    resize_parser = subparsers.add_parser(\'resize\', help=\'Batch resize\')\\n    resize_parser.add_argument(\'inputs\', nargs=\'+\', help=\'Input video files\')\\n    resize_parser.add_argument(\'--width\', type=int, help=\'Target width\')\\n    resize_parser.add_argument(\'--height\', type=int, help=\'Target height\')\\n    resize_parser.add_argument(\'-o\', \'--output-dir\', required=True, help=\'Output directory\')\\n\\n    # Convert command\\n    convert_parser = subparsers.add_parser(\'convert\', help=\'Batch format conversion\')\\n    convert_parser.add_argument(\'inputs\', nargs=\'+\', help=\'Input video files\')\\n    convert_parser.add_argument(\'-f\', \'--format\', default=\'mp4\', help=\'Output format\')\\n    convert_parser.add_argument(\'-c\', \'--codec\', default=\'libx264\', help=\'Video codec\')\\n    convert_parser.add_argument(\'-q\', \'--quality\', type=int, default=18, help=\'CRF quality\')\\n    convert_parser.add_argument(\'-o\', \'--output-dir\', required=True, help=\'Output directory\')\\n\\n    # Extract audio command\\n    audio_parser = subparsers.add_parser(\'audio\', help=\'Batch audio extraction\')\\n    audio_parser.add_argument(\'inputs\', nargs=\'+\', help=\'Input video files\')\\n    audio_parser.add_argument(\'-f\', \'--format\', default=\'mp3\', choices=[\'mp3\', \'aac\', \'wav\'])\\n    audio_parser.add_argument(\'-b\', \'--bitrate\', default=\'192k\', help=\'Audio bitrate\')\\n    audio_parser.add_argument(\'-o\', \'--output-dir\', required=True, help=\'Output directory\')\\n\\n    args = parser.parse_args()\\n\\n    # Expand glob patterns\\n    input_files = []\\n    for pattern in args.inputs:\\n        input_files.extend(Path(\'.\').glob(pattern))\\n\\n    if not input_files:\\n        print(\\"No input files found\\", file=sys.stderr)\\n        sys.exit(1)\\n\\n    # Create output directory\\n    output_dir = Path(args.output_dir)\\n    output_dir.mkdir(parents=True, exist_ok=True)\\n\\n    # Create processor\\n    processor = BatchProcessor(\\n        max_workers=args.workers,\\n        gpu_acceleration=not args.no_gpu,\\n        verbose=args.verbose\\n    )\\n\\n    # Create jobs\\n    for input_file in input_files:\\n        input_path = Path(input_file)\\n\\n        if args.command == \'export\':\\n            output_file = output_dir / f\\"{input_path.stem}_{args.platform}.mp4\\"\\n            job = ProcessingJob(\\n                input_file=input_path,\\n                output_file=output_file,\\n                operation=\'export\',\\n                params={\'platform\': args.platform, \'quality\': args.quality}\\n            )\\n\\n        elif args.command == \'cut\':\\n            output_file = output_dir / f\\"{input_path.stem}_cut.mp4\\"\\n            job = ProcessingJob(\\n                input_file=input_path,\\n                output_file=output_file,\\n                operation=\'cut\',\\n                params={\'start\': args.start, \'end\': args.end}\\n            )\\n\\n        elif args.command == \'resize\':\\n            output_file = output_dir / f\\"{input_path.stem}_resized.mp4\\"\\n            job = ProcessingJob(\\n                input_file=input_path,\\n                output_file=output_file,\\n                operation=\'resize\',\\n                params={\'width\': args.width, \'height\': args.height}\\n            )\\n\\n        elif args.command == \'convert\':\\n            output_file = output_dir / f\\"{input_path.stem}.{args.format}\\"\\n            job = ProcessingJob(\\n                input_file=input_path,\\n                output_file=output_file,\\n                operation=\'convert\',\\n                params={\'codec\': args.codec, \'quality\': args.quality}\\n            )\\n\\n        elif args.command == \'audio\':\\n            output_file = output_dir / f\\"{input_path.stem}.{args.format}\\"\\n            job = ProcessingJob(\\n                input_file=input_path,\\n                output_file=output_file,\\n                operation=\'audio_extract\',\\n                params={\'format\': args.format, \'bitrate\': args.bitrate}\\n            )\\n\\n        processor.add_job(job)\\n\\n    # Process all jobs\\n    stats = processor.process_all(skip_existing=not args.overwrite)\\n\\n    # Exit with error if any jobs failed\\n    if stats.failed > 0:\\n        sys.exit(1)\\n\\n\\nif __name__ == \'__main__\':\\n    main()\\n"},{"name":"motion_graphics.py","type":"file","path":"video-processing-editing/scripts/motion_graphics.py","size":21163,"content":"#!/usr/bin/env python3\\n\\"\\"\\"\\nMotion Graphics Generator for FFmpeg\\n\\nCreates animated graphics, lower thirds, text animations, and overlays\\nusing FFmpeg\'s drawtext, overlay, and complex filter capabilities.\\n\\nUsage:\\n    python motion_graphics.py lower-third --text \\"John Smith\\" --subtitle \\"CEO\\" --output lower_third.mov\\n    python motion_graphics.py text-animate --text \\"Welcome\\" --style fade --output title.mov\\n    python motion_graphics.py progress-bar --duration 30 --output progress.mov\\n    python motion_graphics.py logo-overlay --video input.mp4 --logo logo.png --position bottom-right --output branded.mp4\\n\\"\\"\\"\\n\\nimport argparse\\nimport subprocess\\nimport sys\\nimport json\\nfrom pathlib import Path\\nfrom dataclasses import dataclass\\nfrom typing import Optional, Tuple\\nimport math\\n\\n\\n@dataclass\\nclass AnimationConfig:\\n    \\"\\"\\"Configuration for animation parameters.\\"\\"\\"\\n    duration: float = 5.0\\n    fps: int = 30\\n    width: int = 1920\\n    height: int = 1080\\n    background: str = \\"transparent\\"  # or hex color\\n\\n\\ndef run_ffmpeg(cmd: list, description: str = \\"\\") -> bool:\\n    \\"\\"\\"Execute FFmpeg command with error handling.\\"\\"\\"\\n    print(f\\"\ud83c\udfac {description or \'Running FFmpeg\'}...\\")\\n    try:\\n        result = subprocess.run(\\n            cmd,\\n            capture_output=True,\\n            text=True,\\n            check=True\\n        )\\n        return True\\n    except subprocess.CalledProcessError as e:\\n        print(f\\"\u274c FFmpeg error: {e.stderr}\\")\\n        return False\\n\\n\\nclass LowerThirdGenerator:\\n    \\"\\"\\"Generate professional lower third graphics.\\"\\"\\"\\n\\n    STYLES = {\\n        \\"modern\\": {\\n            \\"bg_color\\": \\"0x1a1a2e@0.85\\",\\n            \\"accent_color\\": \\"0x4ecdc4\\",\\n            \\"text_color\\": \\"white\\",\\n            \\"font\\": \\"Arial\\",\\n            \\"animation\\": \\"slide\\"\\n        },\\n        \\"minimal\\": {\\n            \\"bg_color\\": \\"0x000000@0.7\\",\\n            \\"accent_color\\": \\"0xffffff\\",\\n            \\"text_color\\": \\"white\\",\\n            \\"font\\": \\"Helvetica\\",\\n            \\"animation\\": \\"fade\\"\\n        },\\n        \\"corporate\\": {\\n            \\"bg_color\\": \\"0x2c3e50@0.9\\",\\n            \\"accent_color\\": \\"0x3498db\\",\\n            \\"text_color\\": \\"white\\",\\n            \\"font\\": \\"Arial\\",\\n            \\"animation\\": \\"slide\\"\\n        },\\n        \\"creative\\": {\\n            \\"bg_color\\": \\"0xff6b6b@0.85\\",\\n            \\"accent_color\\": \\"0xffd93d\\",\\n            \\"text_color\\": \\"white\\",\\n            \\"font\\": \\"Impact\\",\\n            \\"animation\\": \\"bounce\\"\\n        }\\n    }\\n\\n    def __init__(self, config: AnimationConfig):\\n        self.config = config\\n\\n    def generate(\\n        self,\\n        text: str,\\n        subtitle: str = \\"\\",\\n        style: str = \\"modern\\",\\n        output: str = \\"lower_third.mov\\",\\n        hold_duration: float = 3.0\\n    ) -> bool:\\n        \\"\\"\\"Generate an animated lower third graphic.\\"\\"\\"\\n\\n        style_config = self.STYLES.get(style, self.STYLES[\\"modern\\"])\\n\\n        # Calculate dimensions\\n        bar_height = 100\\n        bar_y = self.config.height - bar_height - 50\\n        accent_width = 8\\n\\n        # Animation timing\\n        fade_in = 0.5\\n        fade_out = 0.5\\n        total_duration = fade_in + hold_duration + fade_out\\n\\n        # Build complex filter for animated lower third\\n        filter_complex = f\\"\\"\\"\\n        color=c=black@0:s={self.config.width}x{self.config.height}:d={total_duration}:r={self.config.fps}[bg];\\n\\n        [bg]drawbox=x=0:y={bar_y}:w={self.config.width}:h={bar_height}:\\n            color={style_config[\'bg_color\']}:t=fill:\\n            enable=\'between(t,0,{total_duration})\'[box];\\n\\n        [box]drawbox=x=0:y={bar_y}:w={accent_width}:h={bar_height}:\\n            color={style_config[\'accent_color\']}:t=fill[accent];\\n\\n        [accent]drawtext=text=\'{text}\':\\n            fontfile=/System/Library/Fonts/Helvetica.ttc:\\n            fontsize=42:fontcolor={style_config[\'text_color\']}:\\n            x={accent_width + 20}:y={bar_y + 20}:\\n            alpha=\'if(lt(t,{fade_in}),t/{fade_in},if(lt(t,{fade_in + hold_duration}),1,(1-(t-{fade_in + hold_duration})/{fade_out})))\'[title];\\n\\n        [title]drawtext=text=\'{subtitle}\':\\n            fontfile=/System/Library/Fonts/Helvetica.ttc:\\n            fontsize=24:fontcolor={style_config[\'text_color\']}@0.8:\\n            x={accent_width + 20}:y={bar_y + 65}:\\n            alpha=\'if(lt(t,{fade_in}),t/{fade_in},if(lt(t,{fade_in + hold_duration}),1,(1-(t-{fade_in + hold_duration})/{fade_out})))\'[out]\\n        \\"\\"\\"\\n\\n        cmd = [\\n            \\"ffmpeg\\", \\"-y\\",\\n            \\"-f\\", \\"lavfi\\",\\n            \\"-i\\", f\\"color=c=black@0:s={self.config.width}x{self.config.height}:d={total_duration}:r={self.config.fps}\\",\\n            \\"-filter_complex\\", filter_complex.replace(\\"\\\\n\\", \\"\\"),\\n            \\"-map\\", \\"[out]\\",\\n            \\"-c:v\\", \\"prores_ks\\", \\"-profile:v\\", \\"4444\\",\\n            \\"-pix_fmt\\", \\"yuva444p10le\\",\\n            \\"-t\\", str(total_duration),\\n            output\\n        ]\\n\\n        return run_ffmpeg(cmd, f\\"Creating {style} lower third\\")\\n\\n\\nclass TextAnimator:\\n    \\"\\"\\"Generate animated text effects.\\"\\"\\"\\n\\n    def __init__(self, config: AnimationConfig):\\n        self.config = config\\n\\n    def fade_in_out(\\n        self,\\n        text: str,\\n        output: str,\\n        font_size: int = 72,\\n        color: str = \\"white\\",\\n        duration: float = 5.0\\n    ) -> bool:\\n        \\"\\"\\"Text that fades in, holds, then fades out.\\"\\"\\"\\n\\n        fade_time = 1.0\\n        hold_time = duration - (fade_time * 2)\\n\\n        filter_complex = f\\"\\"\\"\\n        color=c=black@0:s={self.config.width}x{self.config.height}:d={duration}:r={self.config.fps},\\n        drawtext=text=\'{text}\':\\n            fontfile=/System/Library/Fonts/Helvetica.ttc:\\n            fontsize={font_size}:fontcolor={color}:\\n            x=(w-text_w)/2:y=(h-text_h)/2:\\n            alpha=\'if(lt(t,{fade_time}),t/{fade_time},if(lt(t,{fade_time + hold_time}),1,(1-(t-{fade_time + hold_time})/{fade_time})))\'\\n        \\"\\"\\"\\n\\n        cmd = [\\n            \\"ffmpeg\\", \\"-y\\",\\n            \\"-f\\", \\"lavfi\\",\\n            \\"-i\\", f\\"color=c=black@0:s={self.config.width}x{self.config.height}:d={duration}\\",\\n            \\"-vf\\", filter_complex.replace(\\"\\\\n\\", \\"\\"),\\n            \\"-c:v\\", \\"prores_ks\\", \\"-profile:v\\", \\"4444\\",\\n            \\"-pix_fmt\\", \\"yuva444p10le\\",\\n            output\\n        ]\\n\\n        return run_ffmpeg(cmd, \\"Creating fade text animation\\")\\n\\n    def typewriter(\\n        self,\\n        text: str,\\n        output: str,\\n        font_size: int = 48,\\n        chars_per_second: float = 10.0\\n    ) -> bool:\\n        \\"\\"\\"Typewriter effect - text appears character by character.\\"\\"\\"\\n\\n        duration = len(text) / chars_per_second + 2.0  # Extra time at end\\n\\n        # Build drawtext with enable for each character\\n        filter_parts = [f\\"color=c=black@0:s={self.config.width}x{self.config.height}:d={duration}:r={self.config.fps}\\"]\\n\\n        for i, char in enumerate(text):\\n            if char == \\" \\":\\n                continue\\n            start_time = i / chars_per_second\\n            # Escape special characters\\n            escaped_char = char.replace(\\"\'\\", \\"\'\\\\\\\\\'\'\\").replace(\\":\\", \\"\\\\\\\\:\\")\\n            filter_parts.append(\\n                f\\"drawtext=text=\'{escaped_char}\':\\"\\n                f\\"fontfile=/System/Library/Fonts/Courier.dfont:\\"\\n                f\\"fontsize={font_size}:fontcolor=white:\\"\\n                f\\"x=100+{i}*{font_size*0.6}:y=(h-text_h)/2:\\"\\n                f\\"enable=\'gte(t,{start_time})\'\\"\\n            )\\n\\n        cmd = [\\n            \\"ffmpeg\\", \\"-y\\",\\n            \\"-f\\", \\"lavfi\\",\\n            \\"-i\\", f\\"color=c=black@0:s={self.config.width}x{self.config.height}:d={duration}\\",\\n            \\"-vf\\", \\",\\".join(filter_parts),\\n            \\"-c:v\\", \\"prores_ks\\", \\"-profile:v\\", \\"4444\\",\\n            \\"-pix_fmt\\", \\"yuva444p10le\\",\\n            output\\n        ]\\n\\n        return run_ffmpeg(cmd, \\"Creating typewriter animation\\")\\n\\n    def scale_bounce(\\n        self,\\n        text: str,\\n        output: str,\\n        font_size: int = 96,\\n        duration: float = 3.0\\n    ) -> bool:\\n        \\"\\"\\"Text that scales up with a bounce effect.\\"\\"\\"\\n\\n        # Using zoompan for scale animation\\n        filter_complex = f\\"\\"\\"\\n        color=c=black:s={self.config.width}x{self.config.height}:d={duration}:r={self.config.fps},\\n        drawtext=text=\'{text}\':\\n            fontfile=/System/Library/Fonts/Helvetica.ttc:\\n            fontsize={font_size}:fontcolor=white:\\n            x=(w-text_w)/2:y=(h-text_h)/2\\n        \\"\\"\\"\\n\\n        cmd = [\\n            \\"ffmpeg\\", \\"-y\\",\\n            \\"-f\\", \\"lavfi\\",\\n            \\"-i\\", f\\"color=c=black:s={self.config.width}x{self.config.height}:d={duration}\\",\\n            \\"-vf\\", filter_complex.replace(\\"\\\\n\\", \\"\\"),\\n            \\"-c:v\\", \\"libx264\\", \\"-crf\\", \\"18\\",\\n            output\\n        ]\\n\\n        return run_ffmpeg(cmd, \\"Creating scale bounce animation\\")\\n\\n\\nclass ProgressBarGenerator:\\n    \\"\\"\\"Generate animated progress bars and timers.\\"\\"\\"\\n\\n    def __init__(self, config: AnimationConfig):\\n        self.config = config\\n\\n    def horizontal_bar(\\n        self,\\n        output: str,\\n        duration: float = 10.0,\\n        bar_color: str = \\"0x4ecdc4\\",\\n        bg_color: str = \\"0x333333\\",\\n        height: int = 20\\n    ) -> bool:\\n        \\"\\"\\"Generate a horizontal progress bar that fills over time.\\"\\"\\"\\n\\n        bar_y = self.config.height - height - 20\\n\\n        filter_complex = f\\"\\"\\"\\n        color=c=black@0:s={self.config.width}x{self.config.height}:d={duration}:r={self.config.fps},\\n        drawbox=x=20:y={bar_y}:w={self.config.width - 40}:h={height}:\\n            color={bg_color}:t=fill,\\n        drawbox=x=20:y={bar_y}:w=\'(t/{duration})*{self.config.width - 40}\':h={height}:\\n            color={bar_color}:t=fill\\n        \\"\\"\\"\\n\\n        cmd = [\\n            \\"ffmpeg\\", \\"-y\\",\\n            \\"-f\\", \\"lavfi\\",\\n            \\"-i\\", f\\"color=c=black@0:s={self.config.width}x{self.config.height}:d={duration}\\",\\n            \\"-vf\\", filter_complex.replace(\\"\\\\n\\", \\"\\"),\\n            \\"-c:v\\", \\"prores_ks\\", \\"-profile:v\\", \\"4444\\",\\n            \\"-pix_fmt\\", \\"yuva444p10le\\",\\n            output\\n        ]\\n\\n        return run_ffmpeg(cmd, \\"Creating progress bar\\")\\n\\n    def countdown_timer(\\n        self,\\n        output: str,\\n        duration: float = 10.0,\\n        font_size: int = 120\\n    ) -> bool:\\n        \\"\\"\\"Generate a countdown timer.\\"\\"\\"\\n\\n        filter_complex = f\\"\\"\\"\\n        color=c=black@0:s={self.config.width}x{self.config.height}:d={duration}:r={self.config.fps},\\n        drawtext=text=\'%{{eif\\\\\\\\:{duration}-t\\\\\\\\:d}}\':\\n            fontfile=/System/Library/Fonts/Helvetica.ttc:\\n            fontsize={font_size}:fontcolor=white:\\n            x=(w-text_w)/2:y=(h-text_h)/2\\n        \\"\\"\\"\\n\\n        cmd = [\\n            \\"ffmpeg\\", \\"-y\\",\\n            \\"-f\\", \\"lavfi\\",\\n            \\"-i\\", f\\"color=c=black@0:s={self.config.width}x{self.config.height}:d={duration}\\",\\n            \\"-vf\\", filter_complex.replace(\\"\\\\n\\", \\"\\"),\\n            \\"-c:v\\", \\"prores_ks\\", \\"-profile:v\\", \\"4444\\",\\n            \\"-pix_fmt\\", \\"yuva444p10le\\",\\n            output\\n        ]\\n\\n        return run_ffmpeg(cmd, \\"Creating countdown timer\\")\\n\\n\\nclass LogoOverlay:\\n    \\"\\"\\"Overlay logos and watermarks on video.\\"\\"\\"\\n\\n    POSITIONS = {\\n        \\"top-left\\": \\"20:20\\",\\n        \\"top-right\\": \\"W-w-20:20\\",\\n        \\"bottom-left\\": \\"20:H-h-20\\",\\n        \\"bottom-right\\": \\"W-w-20:H-h-20\\",\\n        \\"center\\": \\"(W-w)/2:(H-h)/2\\"\\n    }\\n\\n    def __init__(self, config: AnimationConfig):\\n        self.config = config\\n\\n    def overlay(\\n        self,\\n        video: str,\\n        logo: str,\\n        output: str,\\n        position: str = \\"bottom-right\\",\\n        scale: float = 0.15,\\n        opacity: float = 0.8,\\n        fade_in: float = 0.5\\n    ) -> bool:\\n        \\"\\"\\"Overlay a logo on video with optional fade-in.\\"\\"\\"\\n\\n        pos = self.POSITIONS.get(position, self.POSITIONS[\\"bottom-right\\"])\\n\\n        filter_complex = f\\"\\"\\"\\n        [1:v]scale=iw*{scale}:ih*{scale},\\n        format=rgba,\\n        colorchannelmixer=aa={opacity}[logo];\\n        [0:v][logo]overlay={pos}:\\n            enable=\'gte(t,0)\'[out]\\n        \\"\\"\\"\\n\\n        cmd = [\\n            \\"ffmpeg\\", \\"-y\\",\\n            \\"-i\\", video,\\n            \\"-i\\", logo,\\n            \\"-filter_complex\\", filter_complex.replace(\\"\\\\n\\", \\"\\"),\\n            \\"-map\\", \\"[out]\\",\\n            \\"-map\\", \\"0:a?\\",\\n            \\"-c:v\\", \\"libx264\\", \\"-crf\\", \\"18\\",\\n            \\"-c:a\\", \\"copy\\",\\n            output\\n        ]\\n\\n        return run_ffmpeg(cmd, f\\"Overlaying logo at {position}\\")\\n\\n    def animated_watermark(\\n        self,\\n        video: str,\\n        logo: str,\\n        output: str,\\n        animation: str = \\"pulse\\"\\n    ) -> bool:\\n        \\"\\"\\"Overlay logo with animation (pulse, rotate, bounce).\\"\\"\\"\\n\\n        if animation == \\"pulse\\":\\n            # Pulsing opacity effect\\n            filter_complex = f\\"\\"\\"\\n            [1:v]scale=iw*0.1:ih*0.1,\\n            format=rgba,\\n            colorchannelmixer=aa=\'0.5+0.3*sin(t*3)\'[logo];\\n            [0:v][logo]overlay=W-w-20:H-h-20[out]\\n            \\"\\"\\"\\n        else:\\n            # Default static\\n            filter_complex = f\\"\\"\\"\\n            [1:v]scale=iw*0.1:ih*0.1[logo];\\n            [0:v][logo]overlay=W-w-20:H-h-20[out]\\n            \\"\\"\\"\\n\\n        cmd = [\\n            \\"ffmpeg\\", \\"-y\\",\\n            \\"-i\\", video,\\n            \\"-i\\", logo,\\n            \\"-filter_complex\\", filter_complex.replace(\\"\\\\n\\", \\"\\"),\\n            \\"-map\\", \\"[out]\\",\\n            \\"-map\\", \\"0:a?\\",\\n            \\"-c:v\\", \\"libx264\\", \\"-crf\\", \\"18\\",\\n            \\"-c:a\\", \\"copy\\",\\n            output\\n        ]\\n\\n        return run_ffmpeg(cmd, f\\"Creating {animation} watermark\\")\\n\\n\\nclass TransitionGenerator:\\n    \\"\\"\\"Generate video transitions.\\"\\"\\"\\n\\n    def __init__(self, config: AnimationConfig):\\n        self.config = config\\n\\n    def crossfade(\\n        self,\\n        video1: str,\\n        video2: str,\\n        output: str,\\n        duration: float = 1.0\\n    ) -> bool:\\n        \\"\\"\\"Crossfade between two videos.\\"\\"\\"\\n\\n        cmd = [\\n            \\"ffmpeg\\", \\"-y\\",\\n            \\"-i\\", video1,\\n            \\"-i\\", video2,\\n            \\"-filter_complex\\",\\n            f\\"[0:v][1:v]xfade=transition=fade:duration={duration}:offset=4[v];\\"\\n            f\\"[0:a][1:a]acrossfade=d={duration}[a]\\",\\n            \\"-map\\", \\"[v]\\",\\n            \\"-map\\", \\"[a]\\",\\n            \\"-c:v\\", \\"libx264\\", \\"-crf\\", \\"18\\",\\n            \\"-c:a\\", \\"aac\\", \\"-b:a\\", \\"192k\\",\\n            output\\n        ]\\n\\n        return run_ffmpeg(cmd, \\"Creating crossfade transition\\")\\n\\n    def wipe(\\n        self,\\n        video1: str,\\n        video2: str,\\n        output: str,\\n        direction: str = \\"left\\",\\n        duration: float = 1.0\\n    ) -> bool:\\n        \\"\\"\\"Wipe transition between two videos.\\"\\"\\"\\n\\n        wipe_map = {\\n            \\"left\\": \\"wipeleft\\",\\n            \\"right\\": \\"wiperight\\",\\n            \\"up\\": \\"wipeup\\",\\n            \\"down\\": \\"wipedown\\"\\n        }\\n\\n        transition = wipe_map.get(direction, \\"wipeleft\\")\\n\\n        cmd = [\\n            \\"ffmpeg\\", \\"-y\\",\\n            \\"-i\\", video1,\\n            \\"-i\\", video2,\\n            \\"-filter_complex\\",\\n            f\\"[0:v][1:v]xfade=transition={transition}:duration={duration}:offset=4[v]\\",\\n            \\"-map\\", \\"[v]\\",\\n            \\"-c:v\\", \\"libx264\\", \\"-crf\\", \\"18\\",\\n            output\\n        ]\\n\\n        return run_ffmpeg(cmd, f\\"Creating {direction} wipe transition\\")\\n\\n\\ndef main():\\n    parser = argparse.ArgumentParser(\\n        description=\\"Motion Graphics Generator for FFmpeg\\",\\n        formatter_class=argparse.RawDescriptionHelpFormatter,\\n        epilog=\\"\\"\\"\\nExamples:\\n    # Lower third\\n    python motion_graphics.py lower-third --text \\"John Smith\\" --subtitle \\"CEO\\" -o lower_third.mov\\n\\n    # Text animation\\n    python motion_graphics.py text-animate --text \\"Welcome\\" --style fade -o title.mov\\n\\n    # Progress bar\\n    python motion_graphics.py progress-bar --duration 30 -o progress.mov\\n\\n    # Logo overlay\\n    python motion_graphics.py logo-overlay --video input.mp4 --logo logo.png -o branded.mp4\\n\\n    # Countdown timer\\n    python motion_graphics.py countdown --duration 10 -o countdown.mov\\n\\n    # Transition\\n    python motion_graphics.py transition --video1 a.mp4 --video2 b.mp4 --type crossfade -o merged.mp4\\n        \\"\\"\\"\\n    )\\n\\n    subparsers = parser.add_subparsers(dest=\\"command\\", help=\\"Command to run\\")\\n\\n    # Lower third\\n    lt_parser = subparsers.add_parser(\\"lower-third\\", help=\\"Generate lower third graphic\\")\\n    lt_parser.add_argument(\\"--text\\", required=True, help=\\"Main text\\")\\n    lt_parser.add_argument(\\"--subtitle\\", default=\\"\\", help=\\"Subtitle text\\")\\n    lt_parser.add_argument(\\"--style\\", choices=[\\"modern\\", \\"minimal\\", \\"corporate\\", \\"creative\\"], default=\\"modern\\")\\n    lt_parser.add_argument(\\"--hold\\", type=float, default=3.0, help=\\"Hold duration in seconds\\")\\n    lt_parser.add_argument(\\"-o\\", \\"--output\\", default=\\"lower_third.mov\\", help=\\"Output file\\")\\n\\n    # Text animation\\n    text_parser = subparsers.add_parser(\\"text-animate\\", help=\\"Generate animated text\\")\\n    text_parser.add_argument(\\"--text\\", required=True, help=\\"Text to animate\\")\\n    text_parser.add_argument(\\"--style\\", choices=[\\"fade\\", \\"typewriter\\", \\"bounce\\"], default=\\"fade\\")\\n    text_parser.add_argument(\\"--font-size\\", type=int, default=72)\\n    text_parser.add_argument(\\"--duration\\", type=float, default=5.0)\\n    text_parser.add_argument(\\"-o\\", \\"--output\\", default=\\"text_animation.mov\\", help=\\"Output file\\")\\n\\n    # Progress bar\\n    progress_parser = subparsers.add_parser(\\"progress-bar\\", help=\\"Generate progress bar\\")\\n    progress_parser.add_argument(\\"--duration\\", type=float, default=10.0)\\n    progress_parser.add_argument(\\"--color\\", default=\\"0x4ecdc4\\", help=\\"Bar color\\")\\n    progress_parser.add_argument(\\"--height\\", type=int, default=20)\\n    progress_parser.add_argument(\\"-o\\", \\"--output\\", default=\\"progress.mov\\", help=\\"Output file\\")\\n\\n    # Logo overlay\\n    logo_parser = subparsers.add_parser(\\"logo-overlay\\", help=\\"Overlay logo on video\\")\\n    logo_parser.add_argument(\\"--video\\", required=True, help=\\"Input video\\")\\n    logo_parser.add_argument(\\"--logo\\", required=True, help=\\"Logo image\\")\\n    logo_parser.add_argument(\\"--position\\", choices=[\\"top-left\\", \\"top-right\\", \\"bottom-left\\", \\"bottom-right\\", \\"center\\"], default=\\"bottom-right\\")\\n    logo_parser.add_argument(\\"--scale\\", type=float, default=0.15, help=\\"Logo scale (0.0-1.0)\\")\\n    logo_parser.add_argument(\\"--opacity\\", type=float, default=0.8, help=\\"Logo opacity (0.0-1.0)\\")\\n    logo_parser.add_argument(\\"-o\\", \\"--output\\", default=\\"branded.mp4\\", help=\\"Output file\\")\\n\\n    # Countdown\\n    countdown_parser = subparsers.add_parser(\\"countdown\\", help=\\"Generate countdown timer\\")\\n    countdown_parser.add_argument(\\"--duration\\", type=float, default=10.0)\\n    countdown_parser.add_argument(\\"--font-size\\", type=int, default=120)\\n    countdown_parser.add_argument(\\"-o\\", \\"--output\\", default=\\"countdown.mov\\", help=\\"Output file\\")\\n\\n    # Transition\\n    trans_parser = subparsers.add_parser(\\"transition\\", help=\\"Generate video transition\\")\\n    trans_parser.add_argument(\\"--video1\\", required=True, help=\\"First video\\")\\n    trans_parser.add_argument(\\"--video2\\", required=True, help=\\"Second video\\")\\n    trans_parser.add_argument(\\"--type\\", choices=[\\"crossfade\\", \\"wipe-left\\", \\"wipe-right\\", \\"wipe-up\\", \\"wipe-down\\"], default=\\"crossfade\\")\\n    trans_parser.add_argument(\\"--duration\\", type=float, default=1.0)\\n    trans_parser.add_argument(\\"-o\\", \\"--output\\", default=\\"transition.mp4\\", help=\\"Output file\\")\\n\\n    args = parser.parse_args()\\n\\n    if not args.command:\\n        parser.print_help()\\n        sys.exit(1)\\n\\n    config = AnimationConfig()\\n\\n    if args.command == \\"lower-third\\":\\n        generator = LowerThirdGenerator(config)\\n        success = generator.generate(\\n            text=args.text,\\n            subtitle=args.subtitle,\\n            style=args.style,\\n            output=args.output,\\n            hold_duration=args.hold\\n        )\\n\\n    elif args.command == \\"text-animate\\":\\n        animator = TextAnimator(config)\\n        if args.style == \\"fade\\":\\n            success = animator.fade_in_out(args.text, args.output, args.font_size, duration=args.duration)\\n        elif args.style == \\"typewriter\\":\\n            success = animator.typewriter(args.text, args.output, args.font_size)\\n        elif args.style == \\"bounce\\":\\n            success = animator.scale_bounce(args.text, args.output, args.font_size, args.duration)\\n\\n    elif args.command == \\"progress-bar\\":\\n        generator = ProgressBarGenerator(config)\\n        success = generator.horizontal_bar(\\n            output=args.output,\\n            duration=args.duration,\\n            bar_color=args.color,\\n            height=args.height\\n        )\\n\\n    elif args.command == \\"logo-overlay\\":\\n        overlay = LogoOverlay(config)\\n        success = overlay.overlay(\\n            video=args.video,\\n            logo=args.logo,\\n            output=args.output,\\n            position=args.position,\\n            scale=args.scale,\\n            opacity=args.opacity\\n        )\\n\\n    elif args.command == \\"countdown\\":\\n        generator = ProgressBarGenerator(config)\\n        success = generator.countdown_timer(\\n            output=args.output,\\n            duration=args.duration,\\n            font_size=args.font_size\\n        )\\n\\n    elif args.command == \\"transition\\":\\n        generator = TransitionGenerator(config)\\n        if args.type == \\"crossfade\\":\\n            success = generator.crossfade(args.video1, args.video2, args.output, args.duration)\\n        else:\\n            direction = args.type.replace(\\"wipe-\\", \\"\\")\\n            success = generator.wipe(args.video1, args.video2, args.output, direction, args.duration)\\n\\n    if success:\\n        print(f\\"\u2705 Created: {args.output}\\")\\n    else:\\n        print(\\"\u274c Generation failed\\")\\n        sys.exit(1)\\n\\n\\nif __name__ == \\"__main__\\":\\n    main()\\n"},{"name":"quality_assessment.py","type":"file","path":"video-processing-editing/scripts/quality_assessment.py","size":26257,"content":"#!/usr/bin/env python3\\n\\"\\"\\"\\nVideo Quality Assessment Tool\\n\\nMeasures video quality using industry-standard metrics:\\n- VMAF (Video Multi-Method Assessment Fusion) - Netflix\'s perceptual quality metric\\n- PSNR (Peak Signal-to-Noise Ratio) - Traditional quality metric\\n- SSIM (Structural Similarity Index) - Perceptual quality metric\\n\\nUsage:\\n    python quality_assessment.py compare --reference original.mp4 --distorted encoded.mp4\\n    python quality_assessment.py analyze --video encoded.mp4\\n    python quality_assessment.py batch --dir ./videos --reference master.mp4\\n    python quality_assessment.py report --reference orig.mp4 --distorted enc.mp4 --format html\\n\\"\\"\\"\\n\\nimport argparse\\nimport subprocess\\nimport sys\\nimport json\\nimport os\\nfrom pathlib import Path\\nfrom dataclasses import dataclass, asdict\\nfrom typing import Optional, List, Dict, Any\\nfrom datetime import datetime\\nimport statistics\\n\\n\\n@dataclass\\nclass QualityMetrics:\\n    \\"\\"\\"Container for video quality metrics.\\"\\"\\"\\n    vmaf: Optional[float] = None\\n    vmaf_min: Optional[float] = None\\n    vmaf_max: Optional[float] = None\\n    vmaf_std: Optional[float] = None\\n    psnr: Optional[float] = None\\n    psnr_min: Optional[float] = None\\n    psnr_max: Optional[float] = None\\n    ssim: Optional[float] = None\\n    ssim_min: Optional[float] = None\\n    ssim_max: Optional[float] = None\\n    ms_ssim: Optional[float] = None\\n\\n\\n@dataclass\\nclass VideoInfo:\\n    \\"\\"\\"Container for video metadata.\\"\\"\\"\\n    filename: str\\n    duration: float\\n    width: int\\n    height: int\\n    fps: float\\n    bitrate: int\\n    codec: str\\n    pixel_format: str\\n    file_size: int\\n\\n\\n@dataclass\\nclass QualityReport:\\n    \\"\\"\\"Complete quality assessment report.\\"\\"\\"\\n    reference: VideoInfo\\n    distorted: VideoInfo\\n    metrics: QualityMetrics\\n    timestamp: str\\n    compression_ratio: float\\n    quality_grade: str\\n\\n\\ndef get_video_info(video_path: str) -> Optional[VideoInfo]:\\n    \\"\\"\\"Extract video metadata using ffprobe.\\"\\"\\"\\n    cmd = [\\n        \\"ffprobe\\",\\n        \\"-v\\", \\"quiet\\",\\n        \\"-print_format\\", \\"json\\",\\n        \\"-show_format\\",\\n        \\"-show_streams\\",\\n        video_path\\n    ]\\n\\n    try:\\n        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\\n        data = json.loads(result.stdout)\\n\\n        # Find video stream\\n        video_stream = None\\n        for stream in data.get(\\"streams\\", []):\\n            if stream.get(\\"codec_type\\") == \\"video\\":\\n                video_stream = stream\\n                break\\n\\n        if not video_stream:\\n            print(f\\"\u274c No video stream found in {video_path}\\")\\n            return None\\n\\n        format_info = data.get(\\"format\\", {})\\n\\n        # Parse frame rate\\n        fps_str = video_stream.get(\\"r_frame_rate\\", \\"30/1\\")\\n        if \\"/\\" in fps_str:\\n            num, den = fps_str.split(\\"/\\")\\n            fps = float(num) / float(den)\\n        else:\\n            fps = float(fps_str)\\n\\n        return VideoInfo(\\n            filename=os.path.basename(video_path),\\n            duration=float(format_info.get(\\"duration\\", 0)),\\n            width=int(video_stream.get(\\"width\\", 0)),\\n            height=int(video_stream.get(\\"height\\", 0)),\\n            fps=fps,\\n            bitrate=int(format_info.get(\\"bit_rate\\", 0)),\\n            codec=video_stream.get(\\"codec_name\\", \\"unknown\\"),\\n            pixel_format=video_stream.get(\\"pix_fmt\\", \\"unknown\\"),\\n            file_size=int(format_info.get(\\"size\\", 0))\\n        )\\n\\n    except subprocess.CalledProcessError as e:\\n        print(f\\"\u274c ffprobe error: {e.stderr}\\")\\n        return None\\n    except json.JSONDecodeError as e:\\n        print(f\\"\u274c JSON parse error: {e}\\")\\n        return None\\n\\n\\ndef calculate_vmaf(reference: str, distorted: str, log_path: str = None) -> Optional[Dict[str, float]]:\\n    \\"\\"\\"Calculate VMAF score between reference and distorted video.\\"\\"\\"\\n\\n    log_path = log_path or \\"/tmp/vmaf_log.json\\"\\n\\n    # Check if libvmaf is available\\n    cmd = [\\n        \\"ffmpeg\\", \\"-y\\",\\n        \\"-i\\", distorted,\\n        \\"-i\\", reference,\\n        \\"-lavfi\\", f\\"libvmaf=log_fmt=json:log_path={log_path}:n_threads=4\\",\\n        \\"-f\\", \\"null\\", \\"-\\"\\n    ]\\n\\n    print(\\"\ud83d\udcca Calculating VMAF (this may take a while)...\\")\\n\\n    try:\\n        result = subprocess.run(cmd, capture_output=True, text=True)\\n\\n        # Parse VMAF log\\n        if os.path.exists(log_path):\\n            with open(log_path) as f:\\n                vmaf_data = json.load(f)\\n\\n            pooled = vmaf_data.get(\\"pooled_metrics\\", {})\\n            vmaf_info = pooled.get(\\"vmaf\\", {})\\n\\n            return {\\n                \\"vmaf\\": vmaf_info.get(\\"mean\\"),\\n                \\"vmaf_min\\": vmaf_info.get(\\"min\\"),\\n                \\"vmaf_max\\": vmaf_info.get(\\"max\\"),\\n                \\"vmaf_std\\": vmaf_info.get(\\"harmonic_mean\\")\\n            }\\n\\n    except subprocess.CalledProcessError as e:\\n        print(f\\"\u26a0\ufe0f  VMAF calculation failed: {e.stderr}\\")\\n        print(\\"   Make sure FFmpeg is compiled with libvmaf support\\")\\n\\n    return None\\n\\n\\ndef calculate_psnr_ssim(reference: str, distorted: str) -> Dict[str, float]:\\n    \\"\\"\\"Calculate PSNR and SSIM between reference and distorted video.\\"\\"\\"\\n\\n    cmd = [\\n        \\"ffmpeg\\", \\"-y\\",\\n        \\"-i\\", distorted,\\n        \\"-i\\", reference,\\n        \\"-lavfi\\", \\"[0:v][1:v]psnr=stats_file=/tmp/psnr.log;[0:v][1:v]ssim=stats_file=/tmp/ssim.log\\",\\n        \\"-f\\", \\"null\\", \\"-\\"\\n    ]\\n\\n    print(\\"\ud83d\udcca Calculating PSNR and SSIM...\\")\\n\\n    metrics = {}\\n\\n    try:\\n        result = subprocess.run(cmd, capture_output=True, text=True)\\n\\n        # Parse PSNR from stderr (FFmpeg outputs stats there)\\n        stderr = result.stderr\\n        for line in stderr.split(\\"\\\\n\\"):\\n            if \\"PSNR\\" in line and \\"average\\" in line:\\n                # Extract average PSNR\\n                parts = line.split()\\n                for i, part in enumerate(parts):\\n                    if part.startswith(\\"average:\\"):\\n                        metrics[\\"psnr\\"] = float(parts[i].replace(\\"average:\\", \\"\\"))\\n                    elif part.startswith(\\"min:\\"):\\n                        metrics[\\"psnr_min\\"] = float(parts[i].replace(\\"min:\\", \\"\\"))\\n                    elif part.startswith(\\"max:\\"):\\n                        metrics[\\"psnr_max\\"] = float(parts[i].replace(\\"max:\\", \\"\\"))\\n\\n            if \\"SSIM\\" in line and \\"All:\\" in line:\\n                # Extract SSIM\\n                parts = line.split()\\n                for i, part in enumerate(parts):\\n                    if part.startswith(\\"All:\\"):\\n                        ssim_val = parts[i].replace(\\"All:\\", \\"\\")\\n                        metrics[\\"ssim\\"] = float(ssim_val)\\n\\n        # Parse log files for more detailed stats\\n        if os.path.exists(\\"/tmp/psnr.log\\"):\\n            psnr_values = []\\n            with open(\\"/tmp/psnr.log\\") as f:\\n                for line in f:\\n                    if \\"psnr_avg\\" in line:\\n                        parts = line.split()\\n                        for part in parts:\\n                            if part.startswith(\\"psnr_avg:\\"):\\n                                psnr_values.append(float(part.replace(\\"psnr_avg:\\", \\"\\")))\\n            if psnr_values:\\n                metrics[\\"psnr\\"] = statistics.mean(psnr_values)\\n                metrics[\\"psnr_min\\"] = min(psnr_values)\\n                metrics[\\"psnr_max\\"] = max(psnr_values)\\n\\n        if os.path.exists(\\"/tmp/ssim.log\\"):\\n            ssim_values = []\\n            with open(\\"/tmp/ssim.log\\") as f:\\n                for line in f:\\n                    if \\"All:\\" in line:\\n                        parts = line.split()\\n                        for part in parts:\\n                            if part.startswith(\\"All:\\"):\\n                                ssim_values.append(float(part.replace(\\"All:\\", \\"\\")))\\n            if ssim_values:\\n                metrics[\\"ssim\\"] = statistics.mean(ssim_values)\\n                metrics[\\"ssim_min\\"] = min(ssim_values)\\n                metrics[\\"ssim_max\\"] = max(ssim_values)\\n\\n    except subprocess.CalledProcessError as e:\\n        print(f\\"\u26a0\ufe0f  PSNR/SSIM calculation failed: {e.stderr}\\")\\n\\n    return metrics\\n\\n\\ndef calculate_bitrate_quality(video_path: str) -> Dict[str, Any]:\\n    \\"\\"\\"Analyze video quality based on bitrate and resolution.\\"\\"\\"\\n\\n    info = get_video_info(video_path)\\n    if not info:\\n        return {}\\n\\n    # Calculate bits per pixel\\n    pixels_per_frame = info.width * info.height\\n    bits_per_second = info.bitrate\\n    bpp = bits_per_second / (pixels_per_frame * info.fps)\\n\\n    # Quality estimation based on BPP\\n    # These are rough guidelines for H.264\\n    if bpp >= 0.1:\\n        estimated_quality = \\"Excellent\\"\\n    elif bpp >= 0.07:\\n        estimated_quality = \\"Good\\"\\n    elif bpp >= 0.05:\\n        estimated_quality = \\"Acceptable\\"\\n    elif bpp >= 0.03:\\n        estimated_quality = \\"Low\\"\\n    else:\\n        estimated_quality = \\"Poor\\"\\n\\n    return {\\n        \\"bits_per_pixel\\": round(bpp, 4),\\n        \\"estimated_quality\\": estimated_quality,\\n        \\"bitrate_mbps\\": round(info.bitrate / 1_000_000, 2),\\n        \\"resolution\\": f\\"{info.width}x{info.height}\\",\\n        \\"file_size_mb\\": round(info.file_size / 1_000_000, 2)\\n    }\\n\\n\\ndef quality_grade(metrics: QualityMetrics) -> str:\\n    \\"\\"\\"Assign a letter grade based on VMAF score.\\"\\"\\"\\n    if metrics.vmaf is None:\\n        if metrics.ssim and metrics.ssim >= 0.95:\\n            return \\"A\\"\\n        elif metrics.psnr and metrics.psnr >= 40:\\n            return \\"A\\"\\n        return \\"Unknown\\"\\n\\n    vmaf = metrics.vmaf\\n    if vmaf >= 93:\\n        return \\"A+\\"\\n    elif vmaf >= 87:\\n        return \\"A\\"\\n    elif vmaf >= 80:\\n        return \\"B+\\"\\n    elif vmaf >= 70:\\n        return \\"B\\"\\n    elif vmaf >= 60:\\n        return \\"C\\"\\n    elif vmaf >= 50:\\n        return \\"D\\"\\n    else:\\n        return \\"F\\"\\n\\n\\ndef generate_html_report(report: QualityReport, output_path: str):\\n    \\"\\"\\"Generate an HTML quality report.\\"\\"\\"\\n\\n    html = f\\"\\"\\"<!DOCTYPE html>\\n<html lang=\\"en\\">\\n<head>\\n    <meta charset=\\"UTF-8\\">\\n    <meta name=\\"viewport\\" content=\\"width=device-width, initial-scale=1.0\\">\\n    <title>Video Quality Assessment Report</title>\\n    <style>\\n        :root {{\\n            --bg-dark: #1a1a2e;\\n            --bg-card: #16213e;\\n            --accent: #4ecdc4;\\n            --accent-alt: #ff6b6b;\\n            --text: #eee;\\n            --text-muted: #aaa;\\n        }}\\n        * {{ box-sizing: border-box; margin: 0; padding: 0; }}\\n        body {{\\n            font-family: -apple-system, BlinkMacSystemFont, \'Segoe UI\', Roboto, sans-serif;\\n            background: var(--bg-dark);\\n            color: var(--text);\\n            padding: 2rem;\\n            line-height: 1.6;\\n        }}\\n        .container {{ max-width: 1200px; margin: 0 auto; }}\\n        h1 {{\\n            font-size: 2rem;\\n            margin-bottom: 0.5rem;\\n            background: linear-gradient(135deg, var(--accent), var(--accent-alt));\\n            -webkit-background-clip: text;\\n            -webkit-text-fill-color: transparent;\\n        }}\\n        .timestamp {{ color: var(--text-muted); margin-bottom: 2rem; }}\\n        .grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 1.5rem; }}\\n        .card {{\\n            background: var(--bg-card);\\n            border-radius: 12px;\\n            padding: 1.5rem;\\n            box-shadow: 0 4px 20px rgba(0,0,0,0.3);\\n        }}\\n        .card h2 {{\\n            font-size: 1rem;\\n            color: var(--accent);\\n            margin-bottom: 1rem;\\n            text-transform: uppercase;\\n            letter-spacing: 0.5px;\\n        }}\\n        .metric {{\\n            display: flex;\\n            justify-content: space-between;\\n            padding: 0.5rem 0;\\n            border-bottom: 1px solid rgba(255,255,255,0.1);\\n        }}\\n        .metric:last-child {{ border-bottom: none; }}\\n        .metric-label {{ color: var(--text-muted); }}\\n        .metric-value {{ font-weight: 600; }}\\n        .grade {{\\n            font-size: 4rem;\\n            font-weight: bold;\\n            text-align: center;\\n            padding: 2rem;\\n            background: linear-gradient(135deg, var(--accent), var(--accent-alt));\\n            -webkit-background-clip: text;\\n            -webkit-text-fill-color: transparent;\\n        }}\\n        .grade-label {{\\n            text-align: center;\\n            color: var(--text-muted);\\n            font-size: 0.9rem;\\n        }}\\n        .vmaf-bar {{\\n            height: 20px;\\n            background: linear-gradient(90deg, #ff6b6b 0%, #ffd93d 50%, #4ecdc4 100%);\\n            border-radius: 10px;\\n            position: relative;\\n            margin: 1rem 0;\\n        }}\\n        .vmaf-marker {{\\n            position: absolute;\\n            top: -8px;\\n            width: 4px;\\n            height: 36px;\\n            background: white;\\n            border-radius: 2px;\\n            transform: translateX(-50%);\\n        }}\\n        .comparison {{ margin-top: 2rem; }}\\n        .comparison table {{ width: 100%; border-collapse: collapse; }}\\n        .comparison th, .comparison td {{\\n            padding: 0.75rem;\\n            text-align: left;\\n            border-bottom: 1px solid rgba(255,255,255,0.1);\\n        }}\\n        .comparison th {{ color: var(--accent); font-weight: 500; }}\\n        .good {{ color: #4ecdc4; }}\\n        .warning {{ color: #ffd93d; }}\\n        .bad {{ color: #ff6b6b; }}\\n    </style>\\n</head>\\n<body>\\n    <div class=\\"container\\">\\n        <h1>\ud83c\udfac Video Quality Assessment</h1>\\n        <p class=\\"timestamp\\">Generated: {report.timestamp}</p>\\n\\n        <div class=\\"grid\\">\\n            <div class=\\"card\\">\\n                <h2>Quality Grade</h2>\\n                <div class=\\"grade\\">{report.quality_grade}</div>\\n                <div class=\\"grade-label\\">Based on VMAF Score</div>\\n            </div>\\n\\n            <div class=\\"card\\">\\n                <h2>VMAF Score</h2>\\n                <div class=\\"vmaf-bar\\">\\n                    <div class=\\"vmaf-marker\\" style=\\"left: {report.metrics.vmaf or 0}%\\"></div>\\n                </div>\\n                <div class=\\"metric\\">\\n                    <span class=\\"metric-label\\">Mean</span>\\n                    <span class=\\"metric-value\\">{report.metrics.vmaf or \'N/A\':.2f}</span>\\n                </div>\\n                <div class=\\"metric\\">\\n                    <span class=\\"metric-label\\">Min</span>\\n                    <span class=\\"metric-value\\">{report.metrics.vmaf_min or \'N/A\'}</span>\\n                </div>\\n                <div class=\\"metric\\">\\n                    <span class=\\"metric-label\\">Max</span>\\n                    <span class=\\"metric-value\\">{report.metrics.vmaf_max or \'N/A\'}</span>\\n                </div>\\n            </div>\\n\\n            <div class=\\"card\\">\\n                <h2>Technical Metrics</h2>\\n                <div class=\\"metric\\">\\n                    <span class=\\"metric-label\\">PSNR</span>\\n                    <span class=\\"metric-value\\">{report.metrics.psnr or \'N/A\':.2f} dB</span>\\n                </div>\\n                <div class=\\"metric\\">\\n                    <span class=\\"metric-label\\">SSIM</span>\\n                    <span class=\\"metric-value\\">{report.metrics.ssim or \'N/A\':.4f}</span>\\n                </div>\\n                <div class=\\"metric\\">\\n                    <span class=\\"metric-label\\">Compression</span>\\n                    <span class=\\"metric-value\\">{report.compression_ratio:.1f}x</span>\\n                </div>\\n            </div>\\n\\n            <div class=\\"card\\">\\n                <h2>Reference Video</h2>\\n                <div class=\\"metric\\">\\n                    <span class=\\"metric-label\\">File</span>\\n                    <span class=\\"metric-value\\">{report.reference.filename}</span>\\n                </div>\\n                <div class=\\"metric\\">\\n                    <span class=\\"metric-label\\">Resolution</span>\\n                    <span class=\\"metric-value\\">{report.reference.width}x{report.reference.height}</span>\\n                </div>\\n                <div class=\\"metric\\">\\n                    <span class=\\"metric-label\\">Bitrate</span>\\n                    <span class=\\"metric-value\\">{report.reference.bitrate // 1000} kbps</span>\\n                </div>\\n                <div class=\\"metric\\">\\n                    <span class=\\"metric-label\\">Size</span>\\n                    <span class=\\"metric-value\\">{report.reference.file_size // 1000000} MB</span>\\n                </div>\\n            </div>\\n\\n            <div class=\\"card\\">\\n                <h2>Encoded Video</h2>\\n                <div class=\\"metric\\">\\n                    <span class=\\"metric-label\\">File</span>\\n                    <span class=\\"metric-value\\">{report.distorted.filename}</span>\\n                </div>\\n                <div class=\\"metric\\">\\n                    <span class=\\"metric-label\\">Resolution</span>\\n                    <span class=\\"metric-value\\">{report.distorted.width}x{report.distorted.height}</span>\\n                </div>\\n                <div class=\\"metric\\">\\n                    <span class=\\"metric-label\\">Bitrate</span>\\n                    <span class=\\"metric-value\\">{report.distorted.bitrate // 1000} kbps</span>\\n                </div>\\n                <div class=\\"metric\\">\\n                    <span class=\\"metric-label\\">Size</span>\\n                    <span class=\\"metric-value\\">{report.distorted.file_size // 1000000} MB</span>\\n                </div>\\n            </div>\\n        </div>\\n\\n        <div class=\\"comparison card\\" style=\\"margin-top: 2rem;\\">\\n            <h2>Quality Interpretation</h2>\\n            <table>\\n                <tr>\\n                    <th>VMAF Range</th>\\n                    <th>Quality</th>\\n                    <th>Description</th>\\n                </tr>\\n                <tr class=\\"{\'good\' if (report.metrics.vmaf or 0) >= 93 else \'\'}\\">\\n                    <td>93-100</td>\\n                    <td>Excellent (A+)</td>\\n                    <td>Visually indistinguishable from reference</td>\\n                </tr>\\n                <tr class=\\"{\'good\' if 87 <= (report.metrics.vmaf or 0) < 93 else \'\'}\\">\\n                    <td>87-93</td>\\n                    <td>Very Good (A)</td>\\n                    <td>High quality, minor differences possible</td>\\n                </tr>\\n                <tr class=\\"{\'warning\' if 70 <= (report.metrics.vmaf or 0) < 87 else \'\'}\\">\\n                    <td>70-87</td>\\n                    <td>Good (B)</td>\\n                    <td>Acceptable for most use cases</td>\\n                </tr>\\n                <tr class=\\"{\'bad\' if (report.metrics.vmaf or 0) < 70 else \'\'}\\">\\n                    <td>&lt;70</td>\\n                    <td>Fair/Poor</td>\\n                    <td>Visible quality loss, consider higher bitrate</td>\\n                </tr>\\n            </table>\\n        </div>\\n    </div>\\n</body>\\n</html>\\"\\"\\"\\n\\n    with open(output_path, \\"w\\") as f:\\n        f.write(html)\\n\\n    print(f\\"\ud83d\udcc4 HTML report saved to: {output_path}\\")\\n\\n\\ndef compare_videos(reference: str, distorted: str, output_format: str = \\"text\\") -> Optional[QualityReport]:\\n    \\"\\"\\"Compare two videos and generate quality metrics.\\"\\"\\"\\n\\n    print(f\\"\\\\n\ud83c\udfac Comparing videos:\\")\\n    print(f\\"   Reference: {reference}\\")\\n    print(f\\"   Distorted: {distorted}\\\\n\\")\\n\\n    # Get video info\\n    ref_info = get_video_info(reference)\\n    dist_info = get_video_info(distorted)\\n\\n    if not ref_info or not dist_info:\\n        return None\\n\\n    # Calculate metrics\\n    metrics = QualityMetrics()\\n\\n    # VMAF (may not be available on all systems)\\n    vmaf_result = calculate_vmaf(reference, distorted)\\n    if vmaf_result:\\n        metrics.vmaf = vmaf_result.get(\\"vmaf\\")\\n        metrics.vmaf_min = vmaf_result.get(\\"vmaf_min\\")\\n        metrics.vmaf_max = vmaf_result.get(\\"vmaf_max\\")\\n\\n    # PSNR and SSIM\\n    psnr_ssim = calculate_psnr_ssim(reference, distorted)\\n    metrics.psnr = psnr_ssim.get(\\"psnr\\")\\n    metrics.psnr_min = psnr_ssim.get(\\"psnr_min\\")\\n    metrics.psnr_max = psnr_ssim.get(\\"psnr_max\\")\\n    metrics.ssim = psnr_ssim.get(\\"ssim\\")\\n    metrics.ssim_min = psnr_ssim.get(\\"ssim_min\\")\\n    metrics.ssim_max = psnr_ssim.get(\\"ssim_max\\")\\n\\n    # Calculate compression ratio\\n    compression_ratio = ref_info.file_size / dist_info.file_size if dist_info.file_size > 0 else 1.0\\n\\n    # Generate report\\n    report = QualityReport(\\n        reference=ref_info,\\n        distorted=dist_info,\\n        metrics=metrics,\\n        timestamp=datetime.now().isoformat(),\\n        compression_ratio=compression_ratio,\\n        quality_grade=quality_grade(metrics)\\n    )\\n\\n    return report\\n\\n\\ndef print_report(report: QualityReport):\\n    \\"\\"\\"Print quality report to console.\\"\\"\\"\\n\\n    print(\\"\\\\n\\" + \\"=\\" * 60)\\n    print(\\"\ud83d\udcca VIDEO QUALITY ASSESSMENT REPORT\\")\\n    print(\\"=\\" * 60)\\n\\n    print(f\\"\\\\n\ud83c\udfaf Quality Grade: {report.quality_grade}\\")\\n    print(f\\"\ud83d\udce6 Compression Ratio: {report.compression_ratio:.2f}x\\")\\n\\n    print(\\"\\\\n\ud83d\udcc8 Metrics:\\")\\n    print(f\\"   VMAF:  {report.metrics.vmaf:.2f}\\" if report.metrics.vmaf else \\"   VMAF:  N/A (libvmaf not available)\\")\\n    print(f\\"   PSNR:  {report.metrics.psnr:.2f} dB\\" if report.metrics.psnr else \\"   PSNR:  N/A\\")\\n    print(f\\"   SSIM:  {report.metrics.ssim:.4f}\\" if report.metrics.ssim else \\"   SSIM:  N/A\\")\\n\\n    print(\\"\\\\n\ud83d\udcc1 Reference Video:\\")\\n    print(f\\"   File: {report.reference.filename}\\")\\n    print(f\\"   Resolution: {report.reference.width}x{report.reference.height}\\")\\n    print(f\\"   Bitrate: {report.reference.bitrate // 1000} kbps\\")\\n    print(f\\"   Size: {report.reference.file_size // 1000000} MB\\")\\n\\n    print(\\"\\\\n\ud83d\udcc1 Encoded Video:\\")\\n    print(f\\"   File: {report.distorted.filename}\\")\\n    print(f\\"   Resolution: {report.distorted.width}x{report.distorted.height}\\")\\n    print(f\\"   Bitrate: {report.distorted.bitrate // 1000} kbps\\")\\n    print(f\\"   Size: {report.distorted.file_size // 1000000} MB\\")\\n\\n    print(\\"\\\\n\\" + \\"=\\" * 60)\\n\\n\\ndef main():\\n    parser = argparse.ArgumentParser(\\n        description=\\"Video Quality Assessment Tool\\",\\n        formatter_class=argparse.RawDescriptionHelpFormatter,\\n        epilog=\\"\\"\\"\\nExamples:\\n    # Compare two videos\\n    python quality_assessment.py compare --reference original.mp4 --distorted encoded.mp4\\n\\n    # Generate HTML report\\n    python quality_assessment.py compare --reference orig.mp4 --distorted enc.mp4 --format html -o report.html\\n\\n    # Analyze single video (bitrate quality estimation)\\n    python quality_assessment.py analyze --video encoded.mp4\\n\\n    # Batch comparison\\n    python quality_assessment.py batch --reference master.mp4 --dir ./encodes/\\n\\nQuality Grade Interpretation:\\n    A+ (93+):  Visually indistinguishable from reference\\n    A  (87+):  Excellent quality, minimal artifacts\\n    B+ (80+):  Very good quality\\n    B  (70+):  Good quality, acceptable for most uses\\n    C  (60+):  Acceptable, some visible artifacts\\n    D  (50+):  Poor quality, noticeable degradation\\n    F  (<50):  Unacceptable quality\\n        \\"\\"\\"\\n    )\\n\\n    subparsers = parser.add_subparsers(dest=\\"command\\", help=\\"Command to run\\")\\n\\n    # Compare command\\n    compare_parser = subparsers.add_parser(\\"compare\\", help=\\"Compare two videos\\")\\n    compare_parser.add_argument(\\"--reference\\", \\"-r\\", required=True, help=\\"Reference (original) video\\")\\n    compare_parser.add_argument(\\"--distorted\\", \\"-d\\", required=True, help=\\"Distorted (encoded) video\\")\\n    compare_parser.add_argument(\\"--format\\", choices=[\\"text\\", \\"json\\", \\"html\\"], default=\\"text\\", help=\\"Output format\\")\\n    compare_parser.add_argument(\\"-o\\", \\"--output\\", help=\\"Output file for report\\")\\n\\n    # Analyze command\\n    analyze_parser = subparsers.add_parser(\\"analyze\\", help=\\"Analyze single video quality\\")\\n    analyze_parser.add_argument(\\"--video\\", \\"-v\\", required=True, help=\\"Video to analyze\\")\\n\\n    # Batch command\\n    batch_parser = subparsers.add_parser(\\"batch\\", help=\\"Batch compare multiple videos\\")\\n    batch_parser.add_argument(\\"--reference\\", \\"-r\\", required=True, help=\\"Reference video\\")\\n    batch_parser.add_argument(\\"--dir\\", \\"-d\\", required=True, help=\\"Directory of videos to compare\\")\\n    batch_parser.add_argument(\\"--format\\", choices=[\\"text\\", \\"json\\"], default=\\"text\\", help=\\"Output format\\")\\n\\n    args = parser.parse_args()\\n\\n    if not args.command:\\n        parser.print_help()\\n        sys.exit(1)\\n\\n    if args.command == \\"compare\\":\\n        report = compare_videos(args.reference, args.distorted)\\n\\n        if report:\\n            if args.format == \\"text\\":\\n                print_report(report)\\n            elif args.format == \\"json\\":\\n                output = args.output or \\"quality_report.json\\"\\n                with open(output, \\"w\\") as f:\\n                    json.dump(asdict(report), f, indent=2, default=str)\\n                print(f\\"\u2705 JSON report saved to: {output}\\")\\n            elif args.format == \\"html\\":\\n                output = args.output or \\"quality_report.html\\"\\n                generate_html_report(report, output)\\n\\n    elif args.command == \\"analyze\\":\\n        info = get_video_info(args.video)\\n        if info:\\n            analysis = calculate_bitrate_quality(args.video)\\n            print(\\"\\\\n\ud83d\udcca Video Analysis:\\")\\n            print(f\\"   File: {info.filename}\\")\\n            print(f\\"   Resolution: {info.width}x{info.height}\\")\\n            print(f\\"   Codec: {info.codec}\\")\\n            print(f\\"   Bitrate: {analysis.get(\'bitrate_mbps\')} Mbps\\")\\n            print(f\\"   Bits/Pixel: {analysis.get(\'bits_per_pixel\')}\\")\\n            print(f\\"   Estimated Quality: {analysis.get(\'estimated_quality\')}\\")\\n            print(f\\"   File Size: {analysis.get(\'file_size_mb\')} MB\\")\\n\\n    elif args.command == \\"batch\\":\\n        video_dir = Path(args.dir)\\n        videos = list(video_dir.glob(\\"*.mp4\\")) + list(video_dir.glob(\\"*.mkv\\")) + list(video_dir.glob(\\"*.mov\\"))\\n\\n        results = []\\n        for video in videos:\\n            if str(video) == args.reference:\\n                continue\\n            report = compare_videos(args.reference, str(video))\\n            if report:\\n                results.append({\\n                    \\"file\\": report.distorted.filename,\\n                    \\"vmaf\\": report.metrics.vmaf,\\n                    \\"psnr\\": report.metrics.psnr,\\n                    \\"ssim\\": report.metrics.ssim,\\n                    \\"grade\\": report.quality_grade,\\n                    \\"compression\\": report.compression_ratio\\n                })\\n\\n        if results:\\n            print(\\"\\\\n\ud83d\udcca Batch Comparison Results:\\")\\n            print(\\"-\\" * 80)\\n            print(f\\"{\'File\':<30} {\'VMAF\':>8} {\'PSNR\':>8} {\'SSIM\':>8} {\'Grade\':>6} {\'Ratio\':>8}\\")\\n            print(\\"-\\" * 80)\\n            for r in results:\\n                vmaf = f\\"{r[\'vmaf\']:.1f}\\" if r[\'vmaf\'] else \\"N/A\\"\\n                psnr = f\\"{r[\'psnr\']:.1f}\\" if r[\'psnr\'] else \\"N/A\\"\\n                ssim = f\\"{r[\'ssim\']:.3f}\\" if r[\'ssim\'] else \\"N/A\\"\\n                print(f\\"{r[\'file\']:<30} {vmaf:>8} {psnr:>8} {ssim:>8} {r[\'grade\']:>6} {r[\'compression\']:>7.1f}x\\")\\n\\n\\nif __name__ == \\"__main__\\":\\n    main()\\n"},{"name":"thumbnail_generator.py","type":"file","path":"video-processing-editing/scripts/thumbnail_generator.py","size":17455,"content":"#!/usr/bin/env python3\\n\\"\\"\\"\\nSmart Thumbnail Generator\\n\\nGenerates optimal video thumbnails using scene detection, quality analysis,\\nand composition scoring. Finds the most visually appealing frames.\\n\\nFeatures:\\n- Scene detection for key moments\\n- Quality scoring (sharpness, contrast, brightness)\\n- Face detection integration (optional)\\n- Composition analysis (rule of thirds, visual interest)\\n- Batch thumbnail generation at intervals\\n\\nUsage:\\n    python thumbnail_generator.py auto --video input.mp4 --count 5\\n    python thumbnail_generator.py extract --video input.mp4 --time 30.5\\n    python thumbnail_generator.py scenes --video input.mp4 --threshold 0.3\\n    python thumbnail_generator.py grid --video input.mp4 --cols 4 --rows 3\\n\\"\\"\\"\\n\\nimport argparse\\nimport subprocess\\nimport sys\\nimport json\\nimport os\\nimport tempfile\\nfrom pathlib import Path\\nfrom dataclasses import dataclass\\nfrom typing import List, Optional, Tuple\\nimport statistics\\nimport struct\\n\\n\\n@dataclass\\nclass FrameScore:\\n    \\"\\"\\"Quality score for a video frame.\\"\\"\\"\\n    timestamp: float\\n    sharpness: float\\n    contrast: float\\n    brightness: float\\n    colorfulness: float\\n    overall_score: float\\n    frame_path: Optional[str] = None\\n\\n\\n@dataclass\\nclass SceneChange:\\n    \\"\\"\\"Detected scene change.\\"\\"\\"\\n    timestamp: float\\n    score: float\\n\\n\\ndef run_ffmpeg(cmd: list, capture_output: bool = True) -> subprocess.CompletedProcess:\\n    \\"\\"\\"Run FFmpeg command.\\"\\"\\"\\n    try:\\n        return subprocess.run(cmd, capture_output=capture_output, text=True)\\n    except subprocess.CalledProcessError as e:\\n        print(f\\"\u274c FFmpeg error: {e.stderr}\\")\\n        raise\\n\\n\\ndef get_video_duration(video_path: str) -> float:\\n    \\"\\"\\"Get video duration in seconds.\\"\\"\\"\\n    cmd = [\\n        \\"ffprobe\\", \\"-v\\", \\"quiet\\",\\n        \\"-show_entries\\", \\"format=duration\\",\\n        \\"-of\\", \\"default=noprint_wrappers=1:nokey=1\\",\\n        video_path\\n    ]\\n    result = subprocess.run(cmd, capture_output=True, text=True)\\n    return float(result.stdout.strip())\\n\\n\\ndef extract_frame(video_path: str, timestamp: float, output_path: str, size: str = \\"1920x1080\\") -> bool:\\n    \\"\\"\\"Extract a single frame from video.\\"\\"\\"\\n    cmd = [\\n        \\"ffmpeg\\", \\"-y\\",\\n        \\"-ss\\", str(timestamp),\\n        \\"-i\\", video_path,\\n        \\"-vframes\\", \\"1\\",\\n        \\"-s\\", size,\\n        \\"-q:v\\", \\"2\\",  # High quality JPEG\\n        output_path\\n    ]\\n    result = run_ffmpeg(cmd)\\n    return result.returncode == 0\\n\\n\\ndef detect_scenes(video_path: str, threshold: float = 0.3) -> List[SceneChange]:\\n    \\"\\"\\"Detect scene changes in video using FFmpeg\'s scene filter.\\"\\"\\"\\n\\n    print(f\\"\ud83d\udd0d Detecting scenes (threshold: {threshold})...\\")\\n\\n    cmd = [\\n        \\"ffmpeg\\", \\"-i\\", video_path,\\n        \\"-vf\\", f\\"select=\'gt(scene,{threshold})\',showinfo\\",\\n        \\"-f\\", \\"null\\", \\"-\\"\\n    ]\\n\\n    result = subprocess.run(cmd, capture_output=True, text=True)\\n\\n    scenes = []\\n    for line in result.stderr.split(\\"\\\\n\\"):\\n        if \\"pts_time:\\" in line:\\n            # Parse timestamp from showinfo output\\n            parts = line.split()\\n            for i, part in enumerate(parts):\\n                if part.startswith(\\"pts_time:\\"):\\n                    timestamp = float(part.replace(\\"pts_time:\\", \\"\\"))\\n                    # Get scene score if available\\n                    score = threshold  # Use threshold as default score\\n                    scenes.append(SceneChange(timestamp=timestamp, score=score))\\n                    break\\n\\n    print(f\\"   Found {len(scenes)} scene changes\\")\\n    return scenes\\n\\n\\ndef analyze_frame_quality(frame_path: str) -> FrameScore:\\n    \\"\\"\\"Analyze frame quality using FFmpeg filters.\\"\\"\\"\\n\\n    # Get frame statistics using signalstats filter\\n    cmd = [\\n        \\"ffmpeg\\", \\"-i\\", frame_path,\\n        \\"-vf\\", \\"signalstats,metadata=print:file=-\\",\\n        \\"-f\\", \\"null\\", \\"-\\"\\n    ]\\n\\n    result = subprocess.run(cmd, capture_output=True, text=True)\\n\\n    # Parse statistics\\n    stats = {}\\n    for line in result.stderr.split(\\"\\\\n\\"):\\n        if \\"=\\" in line and \\"lavfi\\" in line:\\n            parts = line.split(\\"=\\")\\n            if len(parts) >= 2:\\n                key = parts[0].split(\\".\\")[-1].strip()\\n                try:\\n                    stats[key] = float(parts[1].strip())\\n                except ValueError:\\n                    pass\\n\\n    # Calculate quality metrics\\n    # Brightness (YAVG - average luminance)\\n    brightness = stats.get(\\"YAVG\\", 128) / 255.0\\n\\n    # Contrast (YDIF - luminance difference)\\n    contrast = min(stats.get(\\"YDIF\\", 50) / 100.0, 1.0)\\n\\n    # Colorfulness (using U/V variance)\\n    u_var = stats.get(\\"UAVG\\", 128)\\n    v_var = stats.get(\\"VAVG\\", 128)\\n    colorfulness = abs(u_var - 128) + abs(v_var - 128)\\n    colorfulness = min(colorfulness / 100.0, 1.0)\\n\\n    # Sharpness (estimate using high-frequency content)\\n    # Using edge detection as proxy\\n    sharpness_cmd = [\\n        \\"ffmpeg\\", \\"-i\\", frame_path,\\n        \\"-vf\\", \\"edgedetect=low=0.1:high=0.4,entropy\\",\\n        \\"-f\\", \\"null\\", \\"-\\"\\n    ]\\n    sharp_result = subprocess.run(sharpness_cmd, capture_output=True, text=True)\\n\\n    # Parse entropy as sharpness proxy (higher = more detail)\\n    sharpness = 0.5  # Default\\n    for line in sharp_result.stderr.split(\\"\\\\n\\"):\\n        if \\"entropy\\" in line.lower():\\n            try:\\n                # Extract entropy value\\n                parts = line.split()\\n                for part in parts:\\n                    if part.replace(\\".\\", \\"\\").replace(\\"-\\", \\"\\").isdigit():\\n                        sharpness = min(float(part) / 8.0, 1.0)\\n                        break\\n            except:\\n                pass\\n\\n    # Overall score (weighted combination)\\n    # Penalize very dark or very bright frames\\n    brightness_score = 1.0 - abs(brightness - 0.5) * 2\\n    brightness_score = max(0, brightness_score)\\n\\n    overall = (\\n        sharpness * 0.35 +\\n        contrast * 0.25 +\\n        brightness_score * 0.20 +\\n        colorfulness * 0.20\\n    )\\n\\n    return FrameScore(\\n        timestamp=0,  # Set by caller\\n        sharpness=sharpness,\\n        contrast=contrast,\\n        brightness=brightness,\\n        colorfulness=colorfulness,\\n        overall_score=overall,\\n        frame_path=frame_path\\n    )\\n\\n\\ndef find_best_frames(\\n    video_path: str,\\n    count: int = 5,\\n    sample_interval: float = 1.0,\\n    avoid_start: float = 2.0,\\n    avoid_end: float = 2.0\\n) -> List[FrameScore]:\\n    \\"\\"\\"Find the best thumbnail candidates by sampling and scoring frames.\\"\\"\\"\\n\\n    duration = get_video_duration(video_path)\\n    print(f\\"\ud83d\udcf9 Video duration: {duration:.1f}s\\")\\n\\n    # Calculate sample points\\n    start = avoid_start\\n    end = duration - avoid_end\\n    num_samples = int((end - start) / sample_interval)\\n\\n    print(f\\"\ud83d\udd0d Sampling {num_samples} frames...\\")\\n\\n    scores = []\\n\\n    with tempfile.TemporaryDirectory() as tmpdir:\\n        for i in range(num_samples):\\n            timestamp = start + (i * sample_interval)\\n            frame_path = os.path.join(tmpdir, f\\"frame_{i:04d}.jpg\\")\\n\\n            if extract_frame(video_path, timestamp, frame_path):\\n                score = analyze_frame_quality(frame_path)\\n                score.timestamp = timestamp\\n                score.frame_path = frame_path\\n                scores.append(score)\\n\\n                if (i + 1) % 10 == 0:\\n                    print(f\\"   Analyzed {i + 1}/{num_samples} frames...\\")\\n\\n    # Sort by overall score\\n    scores.sort(key=lambda x: x.overall_score, reverse=True)\\n\\n    # Return top N, but ensure they\'re spread out\\n    selected = []\\n    min_gap = duration / (count * 2)  # Minimum time gap between selections\\n\\n    for score in scores:\\n        if len(selected) >= count:\\n            break\\n\\n        # Check if this frame is far enough from already selected frames\\n        too_close = False\\n        for sel in selected:\\n            if abs(score.timestamp - sel.timestamp) < min_gap:\\n                too_close = True\\n                break\\n\\n        if not too_close:\\n            selected.append(score)\\n\\n    # Sort by timestamp\\n    selected.sort(key=lambda x: x.timestamp)\\n\\n    return selected\\n\\n\\ndef generate_contact_sheet(\\n    video_path: str,\\n    output_path: str,\\n    cols: int = 4,\\n    rows: int = 3,\\n    tile_width: int = 480,\\n    tile_height: int = 270,\\n    timestamps: bool = True\\n) -> bool:\\n    \\"\\"\\"Generate a contact sheet / thumbnail grid.\\"\\"\\"\\n\\n    print(f\\"\ud83d\udcca Generating {cols}x{rows} contact sheet...\\")\\n\\n    total_frames = cols * rows\\n\\n    # Build FFmpeg filter for grid\\n    filter_parts = []\\n\\n    # Extract frames at regular intervals\\n    duration = get_video_duration(video_path)\\n    interval = duration / (total_frames + 1)\\n\\n    # Select frames at intervals and tile them\\n    select_expr = \\"+\\".join([f\\"eq(n,{int((i+1)*interval*30)})\\" for i in range(total_frames)])\\n\\n    filter_complex = f\\"select=\'{select_expr}\',scale={tile_width}:{tile_height},tile={cols}x{rows}\\"\\n\\n    if timestamps:\\n        # Add timestamps (simplified - just add to output)\\n        pass  # Timestamps would require more complex filtering\\n\\n    cmd = [\\n        \\"ffmpeg\\", \\"-y\\",\\n        \\"-i\\", video_path,\\n        \\"-vf\\", filter_complex,\\n        \\"-frames:v\\", \\"1\\",\\n        \\"-q:v\\", \\"2\\",\\n        output_path\\n    ]\\n\\n    result = run_ffmpeg(cmd)\\n    return result.returncode == 0\\n\\n\\ndef generate_scene_thumbnails(\\n    video_path: str,\\n    output_dir: str,\\n    threshold: float = 0.3,\\n    size: str = \\"1920x1080\\"\\n) -> List[str]:\\n    \\"\\"\\"Generate thumbnails at scene changes.\\"\\"\\"\\n\\n    os.makedirs(output_dir, exist_ok=True)\\n\\n    scenes = detect_scenes(video_path, threshold)\\n\\n    if not scenes:\\n        print(\\"\u26a0\ufe0f  No scene changes detected, using regular intervals\\")\\n        duration = get_video_duration(video_path)\\n        scenes = [SceneChange(i * 10, 0.5) for i in range(int(duration / 10))]\\n\\n    thumbnails = []\\n    for i, scene in enumerate(scenes[:20]):  # Limit to 20 thumbnails\\n        output_path = os.path.join(output_dir, f\\"scene_{i:03d}_{scene.timestamp:.1f}s.jpg\\")\\n\\n        if extract_frame(video_path, scene.timestamp, output_path, size):\\n            thumbnails.append(output_path)\\n            print(f\\"   \u2713 {output_path}\\")\\n\\n    return thumbnails\\n\\n\\ndef generate_animated_thumbnail(\\n    video_path: str,\\n    output_path: str,\\n    duration: float = 3.0,\\n    fps: int = 10,\\n    width: int = 480,\\n    start_time: Optional[float] = None\\n) -> bool:\\n    \\"\\"\\"Generate an animated GIF thumbnail.\\"\\"\\"\\n\\n    if start_time is None:\\n        # Find an interesting section\\n        video_duration = get_video_duration(video_path)\\n        start_time = video_duration * 0.3  # Start at 30%\\n\\n    print(f\\"\ud83c\udf9e\ufe0f  Generating animated thumbnail ({duration}s @ {fps}fps)...\\")\\n\\n    # Two-pass for better GIF quality\\n    palette_path = \\"/tmp/palette.png\\"\\n\\n    # Generate palette\\n    cmd_palette = [\\n        \\"ffmpeg\\", \\"-y\\",\\n        \\"-ss\\", str(start_time),\\n        \\"-t\\", str(duration),\\n        \\"-i\\", video_path,\\n        \\"-vf\\", f\\"fps={fps},scale={width}:-1:flags=lanczos,palettegen=stats_mode=diff\\",\\n        palette_path\\n    ]\\n    run_ffmpeg(cmd_palette)\\n\\n    # Generate GIF using palette\\n    cmd_gif = [\\n        \\"ffmpeg\\", \\"-y\\",\\n        \\"-ss\\", str(start_time),\\n        \\"-t\\", str(duration),\\n        \\"-i\\", video_path,\\n        \\"-i\\", palette_path,\\n        \\"-lavfi\\", f\\"fps={fps},scale={width}:-1:flags=lanczos[x];[x][1:v]paletteuse=dither=bayer:bayer_scale=5:diff_mode=rectangle\\",\\n        output_path\\n    ]\\n\\n    result = run_ffmpeg(cmd_gif)\\n    return result.returncode == 0\\n\\n\\ndef auto_thumbnail(\\n    video_path: str,\\n    output_path: str,\\n    size: str = \\"1920x1080\\"\\n) -> bool:\\n    \\"\\"\\"Automatically select and extract the best thumbnail.\\"\\"\\"\\n\\n    print(\\"\ud83e\udd16 Auto-selecting best thumbnail...\\")\\n\\n    # Find best frame\\n    best_frames = find_best_frames(video_path, count=1, sample_interval=0.5)\\n\\n    if not best_frames:\\n        print(\\"\u26a0\ufe0f  Could not find suitable frames, using middle of video\\")\\n        duration = get_video_duration(video_path)\\n        timestamp = duration / 2\\n    else:\\n        timestamp = best_frames[0].timestamp\\n        print(f\\"   Selected timestamp: {timestamp:.2f}s (score: {best_frames[0].overall_score:.3f})\\")\\n\\n    return extract_frame(video_path, timestamp, output_path, size)\\n\\n\\ndef main():\\n    parser = argparse.ArgumentParser(\\n        description=\\"Smart Thumbnail Generator\\",\\n        formatter_class=argparse.RawDescriptionHelpFormatter,\\n        epilog=\\"\\"\\"\\nExamples:\\n    # Auto-select best thumbnail\\n    python thumbnail_generator.py auto --video input.mp4 -o thumbnail.jpg\\n\\n    # Extract multiple best candidates\\n    python thumbnail_generator.py best --video input.mp4 --count 5 --output-dir ./thumbs/\\n\\n    # Extract at specific timestamp\\n    python thumbnail_generator.py extract --video input.mp4 --time 30.5 -o thumb.jpg\\n\\n    # Generate thumbnails at scene changes\\n    python thumbnail_generator.py scenes --video input.mp4 --output-dir ./scenes/\\n\\n    # Generate contact sheet grid\\n    python thumbnail_generator.py grid --video input.mp4 --cols 4 --rows 3 -o contact.jpg\\n\\n    # Generate animated GIF thumbnail\\n    python thumbnail_generator.py gif --video input.mp4 --duration 3 -o preview.gif\\n        \\"\\"\\"\\n    )\\n\\n    subparsers = parser.add_subparsers(dest=\\"command\\", help=\\"Command to run\\")\\n\\n    # Auto command\\n    auto_parser = subparsers.add_parser(\\"auto\\", help=\\"Auto-select best thumbnail\\")\\n    auto_parser.add_argument(\\"--video\\", \\"-v\\", required=True, help=\\"Input video\\")\\n    auto_parser.add_argument(\\"--size\\", \\"-s\\", default=\\"1920x1080\\", help=\\"Output size\\")\\n    auto_parser.add_argument(\\"-o\\", \\"--output\\", default=\\"thumbnail.jpg\\", help=\\"Output file\\")\\n\\n    # Best command\\n    best_parser = subparsers.add_parser(\\"best\\", help=\\"Find multiple best thumbnails\\")\\n    best_parser.add_argument(\\"--video\\", \\"-v\\", required=True, help=\\"Input video\\")\\n    best_parser.add_argument(\\"--count\\", \\"-c\\", type=int, default=5, help=\\"Number of thumbnails\\")\\n    best_parser.add_argument(\\"--output-dir\\", \\"-d\\", default=\\"./thumbnails\\", help=\\"Output directory\\")\\n    best_parser.add_argument(\\"--size\\", \\"-s\\", default=\\"1920x1080\\", help=\\"Output size\\")\\n\\n    # Extract command\\n    extract_parser = subparsers.add_parser(\\"extract\\", help=\\"Extract frame at timestamp\\")\\n    extract_parser.add_argument(\\"--video\\", \\"-v\\", required=True, help=\\"Input video\\")\\n    extract_parser.add_argument(\\"--time\\", \\"-t\\", type=float, required=True, help=\\"Timestamp in seconds\\")\\n    extract_parser.add_argument(\\"--size\\", \\"-s\\", default=\\"1920x1080\\", help=\\"Output size\\")\\n    extract_parser.add_argument(\\"-o\\", \\"--output\\", default=\\"frame.jpg\\", help=\\"Output file\\")\\n\\n    # Scenes command\\n    scenes_parser = subparsers.add_parser(\\"scenes\\", help=\\"Generate thumbnails at scene changes\\")\\n    scenes_parser.add_argument(\\"--video\\", \\"-v\\", required=True, help=\\"Input video\\")\\n    scenes_parser.add_argument(\\"--threshold\\", type=float, default=0.3, help=\\"Scene detection threshold\\")\\n    scenes_parser.add_argument(\\"--output-dir\\", \\"-d\\", default=\\"./scenes\\", help=\\"Output directory\\")\\n    scenes_parser.add_argument(\\"--size\\", \\"-s\\", default=\\"1920x1080\\", help=\\"Output size\\")\\n\\n    # Grid command\\n    grid_parser = subparsers.add_parser(\\"grid\\", help=\\"Generate contact sheet\\")\\n    grid_parser.add_argument(\\"--video\\", \\"-v\\", required=True, help=\\"Input video\\")\\n    grid_parser.add_argument(\\"--cols\\", type=int, default=4, help=\\"Number of columns\\")\\n    grid_parser.add_argument(\\"--rows\\", type=int, default=3, help=\\"Number of rows\\")\\n    grid_parser.add_argument(\\"-o\\", \\"--output\\", default=\\"contact_sheet.jpg\\", help=\\"Output file\\")\\n\\n    # GIF command\\n    gif_parser = subparsers.add_parser(\\"gif\\", help=\\"Generate animated GIF thumbnail\\")\\n    gif_parser.add_argument(\\"--video\\", \\"-v\\", required=True, help=\\"Input video\\")\\n    gif_parser.add_argument(\\"--duration\\", \\"-d\\", type=float, default=3.0, help=\\"GIF duration in seconds\\")\\n    gif_parser.add_argument(\\"--fps\\", type=int, default=10, help=\\"GIF frame rate\\")\\n    gif_parser.add_argument(\\"--width\\", \\"-w\\", type=int, default=480, help=\\"GIF width\\")\\n    gif_parser.add_argument(\\"--start\\", \\"-s\\", type=float, help=\\"Start timestamp\\")\\n    gif_parser.add_argument(\\"-o\\", \\"--output\\", default=\\"preview.gif\\", help=\\"Output file\\")\\n\\n    args = parser.parse_args()\\n\\n    if not args.command:\\n        parser.print_help()\\n        sys.exit(1)\\n\\n    success = False\\n\\n    if args.command == \\"auto\\":\\n        success = auto_thumbnail(args.video, args.output, args.size)\\n\\n    elif args.command == \\"best\\":\\n        os.makedirs(args.output_dir, exist_ok=True)\\n        best_frames = find_best_frames(args.video, count=args.count)\\n\\n        print(f\\"\\\\n\ud83d\udcf8 Extracting {len(best_frames)} thumbnails...\\")\\n        for i, frame in enumerate(best_frames):\\n            output_path = os.path.join(args.output_dir, f\\"thumb_{i:02d}_{frame.timestamp:.1f}s.jpg\\")\\n            if extract_frame(args.video, frame.timestamp, output_path, args.size):\\n                print(f\\"   \u2713 {output_path} (score: {frame.overall_score:.3f})\\")\\n        success = True\\n\\n    elif args.command == \\"extract\\":\\n        success = extract_frame(args.video, args.time, args.output, args.size)\\n\\n    elif args.command == \\"scenes\\":\\n        thumbnails = generate_scene_thumbnails(\\n            args.video, args.output_dir, args.threshold, args.size\\n        )\\n        success = len(thumbnails) > 0\\n\\n    elif args.command == \\"grid\\":\\n        success = generate_contact_sheet(\\n            args.video, args.output, args.cols, args.rows\\n        )\\n\\n    elif args.command == \\"gif\\":\\n        success = generate_animated_thumbnail(\\n            args.video, args.output, args.duration, args.fps, args.width, args.start\\n        )\\n\\n    if success:\\n        print(f\\"\u2705 Done!\\")\\n    else:\\n        print(\\"\u274c Failed\\")\\n        sys.exit(1)\\n\\n\\nif __name__ == \\"__main__\\":\\n    main()\\n"},{"name":"timelapse_creator.py","type":"file","path":"video-processing-editing/scripts/timelapse_creator.py","size":19632,"content":"#!/usr/bin/env python3\\n\\"\\"\\"\\nTime-Lapse Creator\\n\\nCreate time-lapse videos from:\\n- Image sequences (photos taken at intervals)\\n- Video footage (speed up existing video)\\n- Webcam/camera captures\\n\\nFeatures:\\n- Deflicker for smooth exposure transitions\\n- Motion blur for smoother appearance\\n- Day-to-night transitions\\n- Ken Burns zoom/pan effects\\n- Music synchronization\\n\\nUsage:\\n    python timelapse_creator.py from-images --input ./photos/ --fps 30 -o timelapse.mp4\\n    python timelapse_creator.py from-video --input long_video.mp4 --speed 60x -o timelapse.mp4\\n    python timelapse_creator.py hyperlapse --input walking.mp4 --stabilize -o hyperlapse.mp4\\n\\"\\"\\"\\n\\nimport argparse\\nimport subprocess\\nimport sys\\nimport os\\nimport glob\\nfrom pathlib import Path\\nfrom dataclasses import dataclass\\nfrom typing import List, Optional, Tuple\\nimport re\\n\\n\\n@dataclass\\nclass TimeLapseConfig:\\n    \\"\\"\\"Configuration for time-lapse creation.\\"\\"\\"\\n    output_fps: int = 30\\n    output_resolution: str = \\"1920x1080\\"\\n    codec: str = \\"libx264\\"\\n    crf: int = 18\\n    preset: str = \\"slow\\"\\n    pixel_format: str = \\"yuv420p\\"\\n\\n\\ndef run_ffmpeg(cmd: list, description: str = \\"\\") -> bool:\\n    \\"\\"\\"Execute FFmpeg command.\\"\\"\\"\\n    print(f\\"\ud83c\udfac {description or \'Processing\'}...\\")\\n    try:\\n        result = subprocess.run(cmd, capture_output=True, text=True)\\n        if result.returncode != 0:\\n            print(f\\"\u26a0\ufe0f  Warning: {result.stderr[:500]}\\")\\n        return result.returncode == 0\\n    except Exception as e:\\n        print(f\\"\u274c Error: {e}\\")\\n        return False\\n\\n\\ndef get_video_info(video_path: str) -> dict:\\n    \\"\\"\\"Get video metadata.\\"\\"\\"\\n    cmd = [\\n        \\"ffprobe\\", \\"-v\\", \\"quiet\\",\\n        \\"-print_format\\", \\"json\\",\\n        \\"-show_format\\", \\"-show_streams\\",\\n        video_path\\n    ]\\n    result = subprocess.run(cmd, capture_output=True, text=True)\\n    import json\\n    return json.loads(result.stdout)\\n\\n\\ndef find_images(input_dir: str, pattern: str = \\"*\\") -> List[str]:\\n    \\"\\"\\"Find image files in directory.\\"\\"\\"\\n    extensions = [\\"jpg\\", \\"jpeg\\", \\"png\\", \\"tiff\\", \\"tif\\", \\"bmp\\"]\\n    images = []\\n\\n    for ext in extensions:\\n        images.extend(glob.glob(os.path.join(input_dir, f\\"{pattern}.{ext}\\")))\\n        images.extend(glob.glob(os.path.join(input_dir, f\\"{pattern}.{ext.upper()}\\")))\\n\\n    # Sort naturally (handle IMG_001, IMG_002, etc.)\\n    def natural_sort_key(s):\\n        return [int(t) if t.isdigit() else t.lower() for t in re.split(r\'(\\\\d+)\', s)]\\n\\n    images.sort(key=natural_sort_key)\\n    return images\\n\\n\\ndef create_image_list(images: List[str], output_path: str, frame_duration: float = 1/30) -> str:\\n    \\"\\"\\"Create FFmpeg concat list file.\\"\\"\\"\\n    with open(output_path, \'w\') as f:\\n        for img in images:\\n            # Escape special characters in path\\n            escaped_path = img.replace(\\"\'\\", \\"\'\\\\\\\\\'\'\\")\\n            f.write(f\\"file \'{escaped_path}\'\\\\n\\")\\n            f.write(f\\"duration {frame_duration}\\\\n\\")\\n        # Add last image again (FFmpeg concat quirk)\\n        if images:\\n            f.write(f\\"file \'{images[-1]}\'\\\\n\\")\\n    return output_path\\n\\n\\nclass ImageSequenceTimeLapse:\\n    \\"\\"\\"Create time-lapse from image sequence.\\"\\"\\"\\n\\n    def __init__(self, config: TimeLapseConfig):\\n        self.config = config\\n\\n    def create(\\n        self,\\n        input_dir: str,\\n        output_path: str,\\n        fps: int = 30,\\n        deflicker: bool = False,\\n        crossfade: float = 0,\\n        music: Optional[str] = None\\n    ) -> bool:\\n        \\"\\"\\"Create time-lapse from images.\\"\\"\\"\\n\\n        images = find_images(input_dir)\\n        if not images:\\n            print(f\\"\u274c No images found in {input_dir}\\")\\n            return False\\n\\n        print(f\\"\ud83d\udcf8 Found {len(images)} images\\")\\n\\n        # Method 1: Using glob pattern (if images are numbered sequentially)\\n        # Check if images follow a pattern like IMG_0001.jpg\\n        first_image = images[0]\\n        img_dir = os.path.dirname(first_image)\\n        img_name = os.path.basename(first_image)\\n\\n        # Try to find numbering pattern\\n        match = re.search(r\'(\\\\d{3,})\', img_name)\\n\\n        if match:\\n            # Use FFmpeg\'s image sequence input\\n            prefix = img_name[:match.start()]\\n            suffix = img_name[match.end():]\\n            num_digits = len(match.group(1))\\n            pattern = f\\"{img_dir}/{prefix}%0{num_digits}d{suffix}\\"\\n\\n            filter_complex = []\\n\\n            # Scale to output resolution\\n            filter_complex.append(f\\"scale={self.config.output_resolution}:force_original_aspect_ratio=decrease\\")\\n            filter_complex.append(f\\"pad={self.config.output_resolution}:(ow-iw)/2:(oh-ih)/2\\")\\n\\n            # Deflicker if requested\\n            if deflicker:\\n                filter_complex.append(\\"deflicker=size=5:mode=am\\")\\n\\n            cmd = [\\n                \\"ffmpeg\\", \\"-y\\",\\n                \\"-framerate\\", str(fps),\\n                \\"-i\\", pattern,\\n                \\"-vf\\", \\",\\".join(filter_complex),\\n                \\"-c:v\\", self.config.codec,\\n                \\"-crf\\", str(self.config.crf),\\n                \\"-preset\\", self.config.preset,\\n                \\"-pix_fmt\\", self.config.pixel_format,\\n                \\"-movflags\\", \\"+faststart\\"\\n            ]\\n\\n        else:\\n            # Use concat demuxer for non-sequential images\\n            list_file = \\"/tmp/timelapse_images.txt\\"\\n            create_image_list(images, list_file, frame_duration=1/fps)\\n\\n            filter_complex = []\\n            filter_complex.append(f\\"scale={self.config.output_resolution}:force_original_aspect_ratio=decrease\\")\\n            filter_complex.append(f\\"pad={self.config.output_resolution}:(ow-iw)/2:(oh-ih)/2\\")\\n\\n            if deflicker:\\n                filter_complex.append(\\"deflicker=size=5:mode=am\\")\\n\\n            cmd = [\\n                \\"ffmpeg\\", \\"-y\\",\\n                \\"-f\\", \\"concat\\",\\n                \\"-safe\\", \\"0\\",\\n                \\"-i\\", list_file,\\n                \\"-vf\\", \\",\\".join(filter_complex),\\n                \\"-c:v\\", self.config.codec,\\n                \\"-crf\\", str(self.config.crf),\\n                \\"-preset\\", self.config.preset,\\n                \\"-pix_fmt\\", self.config.pixel_format,\\n                \\"-movflags\\", \\"+faststart\\"\\n            ]\\n\\n        # Add music if provided\\n        if music:\\n            cmd.extend([\\"-i\\", music])\\n            cmd.extend([\\"-c:a\\", \\"aac\\", \\"-b:a\\", \\"192k\\"])\\n            cmd.extend([\\"-shortest\\"])\\n\\n        cmd.append(output_path)\\n\\n        return run_ffmpeg(cmd, f\\"Creating time-lapse from {len(images)} images\\")\\n\\n    def create_with_ken_burns(\\n        self,\\n        input_dir: str,\\n        output_path: str,\\n        fps: int = 30,\\n        duration_per_image: float = 3.0,\\n        zoom_factor: float = 1.2\\n    ) -> bool:\\n        \\"\\"\\"Create time-lapse with Ken Burns (pan/zoom) effect.\\"\\"\\"\\n\\n        images = find_images(input_dir)\\n        if not images:\\n            print(f\\"\u274c No images found in {input_dir}\\")\\n            return False\\n\\n        print(f\\"\ud83d\udcf8 Found {len(images)} images (Ken Burns effect)\\")\\n\\n        # For Ken Burns, we process each image with zoompan\\n        # Then concatenate\\n        temp_clips = []\\n\\n        for i, img in enumerate(images):\\n            print(f\\"   Processing image {i+1}/{len(images)}...\\")\\n            temp_clip = f\\"/tmp/kenburns_{i:04d}.mp4\\"\\n\\n            frames = int(duration_per_image * fps)\\n\\n            # Alternate between zoom in and zoom out\\n            if i % 2 == 0:\\n                # Zoom in\\n                zoom_expr = f\\"min({zoom_factor},zoom+0.001)\\"\\n            else:\\n                # Zoom out\\n                zoom_expr = f\\"max(1,zoom-0.001)\\"\\n\\n            cmd = [\\n                \\"ffmpeg\\", \\"-y\\",\\n                \\"-loop\\", \\"1\\",\\n                \\"-i\\", img,\\n                \\"-vf\\", f\\"scale=8000:-1,zoompan=z=\'{zoom_expr}\':x=\'iw/2-(iw/zoom/2)\':y=\'ih/2-(ih/zoom/2)\':d={frames}:s={self.config.output_resolution}:fps={fps}\\",\\n                \\"-t\\", str(duration_per_image),\\n                \\"-c:v\\", self.config.codec,\\n                \\"-crf\\", str(self.config.crf),\\n                \\"-pix_fmt\\", self.config.pixel_format,\\n                temp_clip\\n            ]\\n\\n            if not run_ffmpeg(cmd, f\\"Ken Burns effect on image {i+1}\\"):\\n                continue\\n\\n            temp_clips.append(temp_clip)\\n\\n        if not temp_clips:\\n            print(\\"\u274c No clips generated\\")\\n            return False\\n\\n        # Concatenate all clips\\n        list_file = \\"/tmp/kenburns_list.txt\\"\\n        with open(list_file, \'w\') as f:\\n            for clip in temp_clips:\\n                f.write(f\\"file \'{clip}\'\\\\n\\")\\n\\n        cmd = [\\n            \\"ffmpeg\\", \\"-y\\",\\n            \\"-f\\", \\"concat\\",\\n            \\"-safe\\", \\"0\\",\\n            \\"-i\\", list_file,\\n            \\"-c:v\\", self.config.codec,\\n            \\"-crf\\", str(self.config.crf),\\n            \\"-movflags\\", \\"+faststart\\",\\n            output_path\\n        ]\\n\\n        success = run_ffmpeg(cmd, \\"Concatenating Ken Burns clips\\")\\n\\n        # Cleanup\\n        for clip in temp_clips:\\n            try:\\n                os.remove(clip)\\n            except:\\n                pass\\n\\n        return success\\n\\n\\nclass VideoSpeedUp:\\n    \\"\\"\\"Create time-lapse by speeding up video.\\"\\"\\"\\n\\n    def __init__(self, config: TimeLapseConfig):\\n        self.config = config\\n\\n    def create(\\n        self,\\n        input_path: str,\\n        output_path: str,\\n        speed_factor: float = 10.0,\\n        motion_blur: bool = False,\\n        audio: bool = False\\n    ) -> bool:\\n        \\"\\"\\"Speed up video to create time-lapse effect.\\"\\"\\"\\n\\n        print(f\\"\u23e9 Speeding up video by {speed_factor}x...\\")\\n\\n        # Calculate setpts value (1/speed for speedup)\\n        pts_factor = 1.0 / speed_factor\\n\\n        filter_complex = [f\\"setpts={pts_factor}*PTS\\"]\\n\\n        # Add motion blur using minterpolate (creates smoother result)\\n        if motion_blur:\\n            filter_complex.append(\\"minterpolate=fps=30:mi_mode=mci:mc_mode=aobmc:me_mode=bidir:vsbmc=1\\")\\n\\n        # Scale to output resolution\\n        filter_complex.append(f\\"scale={self.config.output_resolution}:force_original_aspect_ratio=decrease\\")\\n        filter_complex.append(f\\"pad={self.config.output_resolution}:(ow-iw)/2:(oh-ih)/2\\")\\n\\n        cmd = [\\n            \\"ffmpeg\\", \\"-y\\",\\n            \\"-i\\", input_path,\\n            \\"-vf\\", \\",\\".join(filter_complex),\\n            \\"-c:v\\", self.config.codec,\\n            \\"-crf\\", str(self.config.crf),\\n            \\"-preset\\", self.config.preset,\\n            \\"-pix_fmt\\", self.config.pixel_format,\\n            \\"-movflags\\", \\"+faststart\\"\\n        ]\\n\\n        if audio and speed_factor <= 2.0:\\n            # Audio can only be sped up by 2x with atempo\\n            cmd.extend([\\"-af\\", f\\"atempo={speed_factor}\\"])\\n            cmd.extend([\\"-c:a\\", \\"aac\\", \\"-b:a\\", \\"128k\\"])\\n        else:\\n            cmd.extend([\\"-an\\"])  # No audio\\n\\n        cmd.append(output_path)\\n\\n        return run_ffmpeg(cmd, f\\"Creating {speed_factor}x speed time-lapse\\")\\n\\n    def create_hyperlapse(\\n        self,\\n        input_path: str,\\n        output_path: str,\\n        speed_factor: float = 8.0,\\n        stabilize: bool = True\\n    ) -> bool:\\n        \\"\\"\\"Create hyperlapse with optional stabilization.\\"\\"\\"\\n\\n        if stabilize:\\n            print(\\"\ud83d\udcd0 Analyzing video for stabilization...\\")\\n\\n            # Two-pass stabilization\\n            # Pass 1: Analyze\\n            vectors_file = \\"/tmp/transforms.trf\\"\\n            cmd_analyze = [\\n                \\"ffmpeg\\", \\"-y\\",\\n                \\"-i\\", input_path,\\n                \\"-vf\\", f\\"vidstabdetect=stepsize=6:shakiness=8:accuracy=9:result={vectors_file}\\",\\n                \\"-f\\", \\"null\\", \\"-\\"\\n            ]\\n\\n            if not run_ffmpeg(cmd_analyze, \\"Analyzing motion\\"):\\n                print(\\"\u26a0\ufe0f  Stabilization analysis failed, continuing without\\")\\n                stabilize = False\\n\\n        pts_factor = 1.0 / speed_factor\\n\\n        filter_complex = [f\\"setpts={pts_factor}*PTS\\"]\\n\\n        if stabilize:\\n            filter_complex.append(f\\"vidstabtransform=input={vectors_file}:zoom=1:smoothing=30\\")\\n            filter_complex.append(\\"unsharp=5:5:0.8:3:3:0.4\\")\\n\\n        filter_complex.append(f\\"scale={self.config.output_resolution}:force_original_aspect_ratio=decrease\\")\\n        filter_complex.append(f\\"pad={self.config.output_resolution}:(ow-iw)/2:(oh-ih)/2\\")\\n\\n        cmd = [\\n            \\"ffmpeg\\", \\"-y\\",\\n            \\"-i\\", input_path,\\n            \\"-vf\\", \\",\\".join(filter_complex),\\n            \\"-c:v\\", self.config.codec,\\n            \\"-crf\\", str(self.config.crf),\\n            \\"-preset\\", self.config.preset,\\n            \\"-pix_fmt\\", self.config.pixel_format,\\n            \\"-an\\",\\n            \\"-movflags\\", \\"+faststart\\",\\n            output_path\\n        ]\\n\\n        return run_ffmpeg(cmd, f\\"Creating stabilized {speed_factor}x hyperlapse\\")\\n\\n\\nclass DayNightTimeLapse:\\n    \\"\\"\\"Create day-to-night or night-to-day time-lapse with deflickering.\\"\\"\\"\\n\\n    def __init__(self, config: TimeLapseConfig):\\n        self.config = config\\n\\n    def create(\\n        self,\\n        input_dir: str,\\n        output_path: str,\\n        fps: int = 30,\\n        deflicker_strength: int = 5\\n    ) -> bool:\\n        \\"\\"\\"Create day-night time-lapse with deflickering.\\"\\"\\"\\n\\n        images = find_images(input_dir)\\n        if not images:\\n            print(f\\"\u274c No images found in {input_dir}\\")\\n            return False\\n\\n        print(f\\"\ud83c\udf05 Creating day-night time-lapse from {len(images)} images\\")\\n\\n        # Check for sequential numbering\\n        first_image = images[0]\\n        img_dir = os.path.dirname(first_image)\\n        img_name = os.path.basename(first_image)\\n        match = re.search(r\'(\\\\d{3,})\', img_name)\\n\\n        if match:\\n            prefix = img_name[:match.start()]\\n            suffix = img_name[match.end():]\\n            num_digits = len(match.group(1))\\n            pattern = f\\"{img_dir}/{prefix}%0{num_digits}d{suffix}\\"\\n\\n            # Deflicker is crucial for day-night transitions\\n            # size determines how many frames to average for exposure smoothing\\n            filter_complex = [\\n                f\\"scale={self.config.output_resolution}:force_original_aspect_ratio=decrease\\",\\n                f\\"pad={self.config.output_resolution}:(ow-iw)/2:(oh-ih)/2\\",\\n                f\\"deflicker=size={deflicker_strength}:mode=am\\"\\n            ]\\n\\n            cmd = [\\n                \\"ffmpeg\\", \\"-y\\",\\n                \\"-framerate\\", str(fps),\\n                \\"-i\\", pattern,\\n                \\"-vf\\", \\",\\".join(filter_complex),\\n                \\"-c:v\\", self.config.codec,\\n                \\"-crf\\", str(self.config.crf),\\n                \\"-preset\\", self.config.preset,\\n                \\"-pix_fmt\\", self.config.pixel_format,\\n                \\"-movflags\\", \\"+faststart\\",\\n                output_path\\n            ]\\n\\n            return run_ffmpeg(cmd, \\"Creating deflickered day-night time-lapse\\")\\n\\n        else:\\n            print(\\"\u274c Images must be numbered sequentially for day-night processing\\")\\n            return False\\n\\n\\ndef main():\\n    parser = argparse.ArgumentParser(\\n        description=\\"Time-Lapse Creator\\",\\n        formatter_class=argparse.RawDescriptionHelpFormatter,\\n        epilog=\\"\\"\\"\\nExamples:\\n    # From image sequence\\n    python timelapse_creator.py from-images --input ./photos/ --fps 30 -o timelapse.mp4\\n\\n    # With deflicker (for varying exposure)\\n    python timelapse_creator.py from-images --input ./photos/ --fps 30 --deflicker -o timelapse.mp4\\n\\n    # Ken Burns effect (pan/zoom)\\n    python timelapse_creator.py ken-burns --input ./photos/ --duration 3 -o slideshow.mp4\\n\\n    # Speed up video\\n    python timelapse_creator.py from-video --input long_video.mp4 --speed 60 -o timelapse.mp4\\n\\n    # Hyperlapse with stabilization\\n    python timelapse_creator.py hyperlapse --input walking.mp4 --speed 8 --stabilize -o hyperlapse.mp4\\n\\n    # Day-night transition\\n    python timelapse_creator.py day-night --input ./photos/ --deflicker-strength 7 -o daynight.mp4\\n        \\"\\"\\"\\n    )\\n\\n    subparsers = parser.add_subparsers(dest=\\"command\\", help=\\"Command to run\\")\\n\\n    # From images\\n    img_parser = subparsers.add_parser(\\"from-images\\", help=\\"Create from image sequence\\")\\n    img_parser.add_argument(\\"--input\\", \\"-i\\", required=True, help=\\"Input directory\\")\\n    img_parser.add_argument(\\"--fps\\", type=int, default=30, help=\\"Output frame rate\\")\\n    img_parser.add_argument(\\"--deflicker\\", action=\\"store_true\\", help=\\"Apply deflicker filter\\")\\n    img_parser.add_argument(\\"--music\\", help=\\"Background music file\\")\\n    img_parser.add_argument(\\"-o\\", \\"--output\\", default=\\"timelapse.mp4\\", help=\\"Output file\\")\\n\\n    # Ken Burns\\n    kb_parser = subparsers.add_parser(\\"ken-burns\\", help=\\"Create with Ken Burns effect\\")\\n    kb_parser.add_argument(\\"--input\\", \\"-i\\", required=True, help=\\"Input directory\\")\\n    kb_parser.add_argument(\\"--fps\\", type=int, default=30, help=\\"Output frame rate\\")\\n    kb_parser.add_argument(\\"--duration\\", type=float, default=3.0, help=\\"Duration per image\\")\\n    kb_parser.add_argument(\\"--zoom\\", type=float, default=1.2, help=\\"Zoom factor\\")\\n    kb_parser.add_argument(\\"-o\\", \\"--output\\", default=\\"slideshow.mp4\\", help=\\"Output file\\")\\n\\n    # From video\\n    vid_parser = subparsers.add_parser(\\"from-video\\", help=\\"Speed up existing video\\")\\n    vid_parser.add_argument(\\"--input\\", \\"-i\\", required=True, help=\\"Input video\\")\\n    vid_parser.add_argument(\\"--speed\\", type=float, default=10.0, help=\\"Speed multiplier\\")\\n    vid_parser.add_argument(\\"--motion-blur\\", action=\\"store_true\\", help=\\"Add motion blur\\")\\n    vid_parser.add_argument(\\"--keep-audio\\", action=\\"store_true\\", help=\\"Keep audio (up to 2x)\\")\\n    vid_parser.add_argument(\\"-o\\", \\"--output\\", default=\\"timelapse.mp4\\", help=\\"Output file\\")\\n\\n    # Hyperlapse\\n    hyper_parser = subparsers.add_parser(\\"hyperlapse\\", help=\\"Create stabilized hyperlapse\\")\\n    hyper_parser.add_argument(\\"--input\\", \\"-i\\", required=True, help=\\"Input video\\")\\n    hyper_parser.add_argument(\\"--speed\\", type=float, default=8.0, help=\\"Speed multiplier\\")\\n    hyper_parser.add_argument(\\"--stabilize\\", action=\\"store_true\\", help=\\"Apply stabilization\\")\\n    hyper_parser.add_argument(\\"-o\\", \\"--output\\", default=\\"hyperlapse.mp4\\", help=\\"Output file\\")\\n\\n    # Day-night\\n    dn_parser = subparsers.add_parser(\\"day-night\\", help=\\"Create day-night timelapse\\")\\n    dn_parser.add_argument(\\"--input\\", \\"-i\\", required=True, help=\\"Input directory\\")\\n    dn_parser.add_argument(\\"--fps\\", type=int, default=30, help=\\"Output frame rate\\")\\n    dn_parser.add_argument(\\"--deflicker-strength\\", type=int, default=5, help=\\"Deflicker window size\\")\\n    dn_parser.add_argument(\\"-o\\", \\"--output\\", default=\\"daynight.mp4\\", help=\\"Output file\\")\\n\\n    args = parser.parse_args()\\n\\n    if not args.command:\\n        parser.print_help()\\n        sys.exit(1)\\n\\n    config = TimeLapseConfig()\\n    success = False\\n\\n    if args.command == \\"from-images\\":\\n        creator = ImageSequenceTimeLapse(config)\\n        success = creator.create(\\n            args.input, args.output, args.fps,\\n            deflicker=args.deflicker, music=args.music\\n        )\\n\\n    elif args.command == \\"ken-burns\\":\\n        creator = ImageSequenceTimeLapse(config)\\n        success = creator.create_with_ken_burns(\\n            args.input, args.output, args.fps, args.duration, args.zoom\\n        )\\n\\n    elif args.command == \\"from-video\\":\\n        creator = VideoSpeedUp(config)\\n        success = creator.create(\\n            args.input, args.output, args.speed,\\n            motion_blur=args.motion_blur, audio=args.keep_audio\\n        )\\n\\n    elif args.command == \\"hyperlapse\\":\\n        creator = VideoSpeedUp(config)\\n        success = creator.create_hyperlapse(\\n            args.input, args.output, args.speed, stabilize=args.stabilize\\n        )\\n\\n    elif args.command == \\"day-night\\":\\n        creator = DayNightTimeLapse(config)\\n        success = creator.create(\\n            args.input, args.output, args.fps, args.deflicker_strength\\n        )\\n\\n    if success:\\n        print(f\\"\u2705 Created: {args.output}\\")\\n    else:\\n        print(\\"\u274c Failed\\")\\n        sys.exit(1)\\n\\n\\nif __name__ == \\"__main__\\":\\n    main()\\n"},{"name":"video_editor.py","type":"file","path":"video-processing-editing/scripts/video_editor.py","size":22890,"content":"#!/usr/bin/env python3\\n\\"\\"\\"\\nvideo_editor.py - Professional video editing with FFmpeg\\n\\nFeatures:\\n- Cut, trim, concatenate clips\\n- Transitions (fade, dissolve, wipe)\\n- Audio mixing and normalization\\n- Color grading\\n- Subtitle handling\\n- Platform-specific exports\\n\\nUsage:\\n    python video_editor.py cut input.mp4 --start 10 --end 60 -o output.mp4\\n    python video_editor.py concat clip1.mp4 clip2.mp4 clip3.mp4 -o final.mp4\\n    python video_editor.py transition fade clip1.mp4 clip2.mp4 -o output.mp4\\n    python video_editor.py export youtube input.mp4 -o youtube.mp4\\n\\"\\"\\"\\n\\nimport argparse\\nimport json\\nimport os\\nimport subprocess\\nimport sys\\nfrom dataclasses import dataclass\\nfrom pathlib import Path\\nfrom typing import List, Optional, Tuple\\n\\n\\n@dataclass\\nclass VideoInfo:\\n    \\"\\"\\"Video metadata from ffprobe\\"\\"\\"\\n    duration: float\\n    width: int\\n    height: int\\n    fps: float\\n    codec: str\\n    bitrate: int\\n    color_space: str\\n    color_primaries: str\\n    color_transfer: str\\n    pix_fmt: str\\n\\n\\nclass VideoEditor:\\n    \\"\\"\\"Professional video editor using FFmpeg\\"\\"\\"\\n\\n    def __init__(self, verbose: bool = False):\\n        self.verbose = verbose\\n\\n    def run_ffmpeg(self, args: List[str], check: bool = True) -> subprocess.CompletedProcess:\\n        \\"\\"\\"Run ffmpeg command with error handling\\"\\"\\"\\n        cmd = [\'ffmpeg\', \'-y\'] + args\\n        if self.verbose:\\n            print(f\\"Running: {\' \'.join(cmd)}\\")\\n\\n        result = subprocess.run(\\n            cmd,\\n            capture_output=True,\\n            text=True,\\n            check=False\\n        )\\n\\n        if check and result.returncode != 0:\\n            print(f\\"FFmpeg error: {result.stderr}\\", file=sys.stderr)\\n            sys.exit(1)\\n\\n        return result\\n\\n    def run_ffprobe(self, input_file: str) -> dict:\\n        \\"\\"\\"Get video metadata using ffprobe\\"\\"\\"\\n        cmd = [\\n            \'ffprobe\',\\n            \'-v\', \'quiet\',\\n            \'-print_format\', \'json\',\\n            \'-show_format\',\\n            \'-show_streams\',\\n            input_file\\n        ]\\n\\n        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\\n        return json.loads(result.stdout)\\n\\n    def get_video_info(self, input_file: str) -> VideoInfo:\\n        \\"\\"\\"Extract video metadata\\"\\"\\"\\n        probe = self.run_ffprobe(input_file)\\n\\n        video_stream = next(\\n            (s for s in probe[\'streams\'] if s[\'codec_type\'] == \'video\'),\\n            None\\n        )\\n\\n        if not video_stream:\\n            raise ValueError(f\\"No video stream found in {input_file}\\")\\n\\n        return VideoInfo(\\n            duration=float(probe[\'format\'][\'duration\']),\\n            width=video_stream[\'width\'],\\n            height=video_stream[\'height\'],\\n            fps=eval(video_stream[\'r_frame_rate\']),\\n            codec=video_stream[\'codec_name\'],\\n            bitrate=int(probe[\'format\'].get(\'bit_rate\', 0)),\\n            color_space=video_stream.get(\'color_space\', \'unknown\'),\\n            color_primaries=video_stream.get(\'color_primaries\', \'unknown\'),\\n            color_transfer=video_stream.get(\'color_transfer\', \'unknown\'),\\n            pix_fmt=video_stream.get(\'pix_fmt\', \'yuv420p\')\\n        )\\n\\n    def find_keyframes(self, input_file: str) -> List[float]:\\n        \\"\\"\\"Find all keyframe timestamps\\"\\"\\"\\n        cmd = [\\n            \'ffprobe\',\\n            \'-v\', \'error\',\\n            \'-select_streams\', \'v\',\\n            \'-show_frames\',\\n            \'-show_entries\', \'frame=pkt_pts_time,key_frame\',\\n            \'-of\', \'csv\',\\n            input_file\\n        ]\\n\\n        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\\n\\n        keyframes = []\\n        for line in result.stdout.strip().split(\'\\\\n\'):\\n            if line.endswith(\',1\'):\\n                timestamp = float(line.split(\',\')[1])\\n                keyframes.append(timestamp)\\n\\n        return keyframes\\n\\n    def find_nearest_keyframe(self, input_file: str, timestamp: float) -> float:\\n        \\"\\"\\"Find nearest keyframe to given timestamp\\"\\"\\"\\n        keyframes = self.find_keyframes(input_file)\\n\\n        if not keyframes:\\n            return timestamp\\n\\n        return min(keyframes, key=lambda k: abs(k - timestamp))\\n\\n    def cut(\\n        self,\\n        input_file: str,\\n        output_file: str,\\n        start: float,\\n        end: Optional[float] = None,\\n        precise: bool = True,\\n        reencode: bool = False\\n    ):\\n        \\"\\"\\"\\n        Cut/trim video clip\\n\\n        Args:\\n            input_file: Input video path\\n            output_file: Output video path\\n            start: Start time in seconds\\n            end: End time in seconds (None = to end of file)\\n            precise: Use frame-accurate cutting (slower)\\n            reencode: Force re-encoding (highest quality)\\n        \\"\\"\\"\\n        info = self.get_video_info(input_file)\\n\\n        if end and end > info.duration:\\n            end = info.duration\\n\\n        duration = (end - start) if end else None\\n\\n        if reencode or precise:\\n            # Two-pass cutting: fast seek + precise cut\\n            args = [\\n                \'-ss\', str(max(0, start - 2)),  # Seek 2s before for safety\\n                \'-i\', input_file,\\n                \'-ss\', \'2\',  # Precise offset after input\\n            ]\\n\\n            if duration:\\n                args += [\'-t\', str(duration)]\\n\\n            args += [\\n                \'-c:v\', \'libx264\',\\n                \'-crf\', \'18\',\\n                \'-preset\', \'medium\',\\n                \'-c:a\', \'aac\',\\n                \'-b:a\', \'192k\',\\n                output_file\\n            ]\\n        else:\\n            # Fast keyframe-aligned cutting\\n            start_kf = self.find_nearest_keyframe(input_file, start)\\n\\n            args = [\'-i\', input_file, \'-ss\', str(start_kf)]\\n\\n            if end:\\n                end_kf = self.find_nearest_keyframe(input_file, end)\\n                args += [\'-to\', str(end_kf)]\\n\\n            args += [\'-c\', \'copy\', output_file]\\n\\n        self.run_ffmpeg(args)\\n        print(f\\"Cut complete: {output_file}\\")\\n\\n    def concat(\\n        self,\\n        input_files: List[str],\\n        output_file: str,\\n        normalize_color: bool = True,\\n        transitions: Optional[str] = None\\n    ):\\n        \\"\\"\\"\\n        Concatenate multiple video clips\\n\\n        Args:\\n            input_files: List of input video paths\\n            output_file: Output video path\\n            normalize_color: Normalize color spaces before concat\\n            transitions: Transition type (\'fade\', \'dissolve\', \'wipe\')\\n        \\"\\"\\"\\n        if len(input_files) < 2:\\n            raise ValueError(\\"Need at least 2 input files to concatenate\\")\\n\\n        if normalize_color:\\n            # Normalize all clips to BT.709\\n            normalized_files = []\\n\\n            for i, input_file in enumerate(input_files):\\n                info = self.get_video_info(input_file)\\n                normalized = f\\"temp_normalized_{i}.mp4\\"\\n\\n                if info.color_space == \'bt709\':\\n                    # Already normalized\\n                    normalized_files.append(input_file)\\n                else:\\n                    # Convert to BT.709\\n                    args = [\\n                        \'-i\', input_file,\\n                        \'-vf\', \'scale=in_range=full:out_range=limited,colorspace=bt709:iall=bt601:fast=1\',\\n                        \'-color_primaries\', \'bt709\',\\n                        \'-color_trc\', \'bt709\',\\n                        \'-colorspace\', \'bt709\',\\n                        \'-c:v\', \'libx264\',\\n                        \'-crf\', \'18\',\\n                        \'-preset\', \'medium\',\\n                        \'-c:a\', \'copy\',\\n                        normalized\\n                    ]\\n\\n                    self.run_ffmpeg(args)\\n                    normalized_files.append(normalized)\\n\\n            input_files = normalized_files\\n\\n        if transitions:\\n            # Concatenate with transitions\\n            self._concat_with_transitions(input_files, output_file, transitions)\\n        else:\\n            # Simple concatenation using concat demuxer\\n            concat_list = Path(\'concat_list.txt\')\\n\\n            with concat_list.open(\'w\') as f:\\n                for input_file in input_files:\\n                    f.write(f\\"file \'{Path(input_file).absolute()}\'\\\\n\\")\\n\\n            args = [\\n                \'-f\', \'concat\',\\n                \'-safe\', \'0\',\\n                \'-i\', str(concat_list),\\n                \'-c\', \'copy\',\\n                output_file\\n            ]\\n\\n            self.run_ffmpeg(args)\\n            concat_list.unlink()\\n\\n        # Cleanup normalized files\\n        if normalize_color:\\n            for f in input_files:\\n                if f.startswith(\'temp_normalized_\'):\\n                    Path(f).unlink()\\n\\n        print(f\\"Concatenation complete: {output_file}\\")\\n\\n    def _concat_with_transitions(\\n        self,\\n        input_files: List[str],\\n        output_file: str,\\n        transition: str,\\n        duration: float = 1.0\\n    ):\\n        \\"\\"\\"Concatenate with crossfade transitions\\"\\"\\"\\n        # Build filter_complex for xfade transitions\\n        filter_parts = []\\n\\n        for i in range(len(input_files) - 1):\\n            if i == 0:\\n                filter_parts.append(f\\"[0:v][1:v]xfade=transition={transition}:duration={duration}:offset=5[v01];\\")\\n            else:\\n                filter_parts.append(f\\"[v0{i}][{i+1}:v]xfade=transition={transition}:duration={duration}:offset=5[v0{i+1}];\\")\\n\\n        filter_complex = \'\'.join(filter_parts)\\n\\n        args = []\\n        for input_file in input_files:\\n            args += [\'-i\', input_file]\\n\\n        args += [\\n            \'-filter_complex\', filter_complex,\\n            \'-map\', f\\"[v0{len(input_files)-1}]\\",\\n            \'-c:v\', \'libx264\',\\n            \'-crf\', \'18\',\\n            \'-preset\', \'medium\',\\n            output_file\\n        ]\\n\\n        self.run_ffmpeg(args)\\n\\n    def add_audio(\\n        self,\\n        video_file: str,\\n        audio_file: str,\\n        output_file: str,\\n        mix: bool = False,\\n        volume: float = 1.0,\\n        sync_offset: float = 0.0\\n    ):\\n        \\"\\"\\"\\n        Add or replace audio track\\n\\n        Args:\\n            video_file: Input video path\\n            audio_file: Input audio path\\n            output_file: Output video path\\n            mix: Mix with original audio (vs replace)\\n            volume: Audio volume multiplier\\n            sync_offset: Audio delay in seconds (positive = delay, negative = advance)\\n        \\"\\"\\"\\n        if mix:\\n            # Mix original and new audio\\n            args = [\\n                \'-i\', video_file,\\n                \'-itsoffset\', str(sync_offset),\\n                \'-i\', audio_file,\\n                \'-filter_complex\', f\'[0:a][1:a]amix=inputs=2:duration=first[a]\',\\n                \'-map\', \'0:v\',\\n                \'-map\', \'[a]\',\\n                \'-c:v\', \'copy\',\\n                \'-c:a\', \'aac\',\\n                \'-b:a\', \'192k\',\\n                output_file\\n            ]\\n        else:\\n            # Replace audio\\n            args = [\\n                \'-i\', video_file,\\n                \'-itsoffset\', str(sync_offset),\\n                \'-i\', audio_file,\\n                \'-map\', \'0:v\',\\n                \'-map\', \'1:a\',\\n                \'-c:v\', \'copy\',\\n                \'-c:a\', \'aac\',\\n                \'-b:a\', \'192k\',\\n                \'-shortest\',\\n                output_file\\n            ]\\n\\n        self.run_ffmpeg(args)\\n        print(f\\"Audio added: {output_file}\\")\\n\\n    def add_subtitles(\\n        self,\\n        input_file: str,\\n        subtitle_file: str,\\n        output_file: str,\\n        burn: bool = True\\n    ):\\n        \\"\\"\\"\\n        Add subtitles to video\\n\\n        Args:\\n            input_file: Input video path\\n            subtitle_file: Subtitle file (.srt, .ass)\\n            output_file: Output video path\\n            burn: Burn subtitles into video (vs soft subs)\\n        \\"\\"\\"\\n        if burn:\\n            # Burn subtitles into video\\n            args = [\\n                \'-i\', input_file,\\n                \'-vf\', f\\"subtitles={subtitle_file}\\",\\n                \'-c:v\', \'libx264\',\\n                \'-crf\', \'18\',\\n                \'-preset\', \'medium\',\\n                \'-c:a\', \'copy\',\\n                output_file\\n            ]\\n        else:\\n            # Add soft subtitles\\n            args = [\\n                \'-i\', input_file,\\n                \'-i\', subtitle_file,\\n                \'-c:v\', \'copy\',\\n                \'-c:a\', \'copy\',\\n                \'-c:s\', \'mov_text\',\\n                \'-metadata:s:s:0\', \'language=eng\',\\n                output_file\\n            ]\\n\\n        self.run_ffmpeg(args)\\n        print(f\\"Subtitles added: {output_file}\\")\\n\\n    def color_grade(\\n        self,\\n        input_file: str,\\n        output_file: str,\\n        brightness: float = 0.0,\\n        contrast: float = 1.0,\\n        saturation: float = 1.0,\\n        gamma: float = 1.0\\n    ):\\n        \\"\\"\\"\\n        Apply color grading\\n\\n        Args:\\n            input_file: Input video path\\n            output_file: Output video path\\n            brightness: Brightness adjustment (-1.0 to 1.0)\\n            contrast: Contrast multiplier (0.0 to 2.0)\\n            saturation: Saturation multiplier (0.0 to 3.0)\\n            gamma: Gamma correction (0.1 to 10.0)\\n        \\"\\"\\"\\n        filters = []\\n\\n        if brightness != 0.0 or contrast != 1.0:\\n            filters.append(f\\"eq=brightness={brightness}:contrast={contrast}\\")\\n\\n        if saturation != 1.0:\\n            filters.append(f\\"eq=saturation={saturation}\\")\\n\\n        if gamma != 1.0:\\n            filters.append(f\\"eq=gamma={gamma}\\")\\n\\n        vf = \',\'.join(filters) if filters else \'null\'\\n\\n        args = [\\n            \'-i\', input_file,\\n            \'-vf\', vf,\\n            \'-c:v\', \'libx264\',\\n            \'-crf\', \'18\',\\n            \'-preset\', \'medium\',\\n            \'-c:a\', \'copy\',\\n            output_file\\n        ]\\n\\n        self.run_ffmpeg(args)\\n        print(f\\"Color grading complete: {output_file}\\")\\n\\n    def export_for_platform(\\n        self,\\n        input_file: str,\\n        output_file: str,\\n        platform: str,\\n        quality: str = \'high\'\\n    ):\\n        \\"\\"\\"\\n        Export optimized for specific platform\\n\\n        Platforms: youtube, instagram_story, instagram_reel, instagram_feed,\\n                   twitter, tiktok, linkedin, web\\n        Quality: draft, medium, high\\n        \\"\\"\\"\\n        presets = {\\n            \'youtube\': {\\n                \'resolution\': \'1920x1080\',\\n                \'fps\': 30,\\n                \'crf\': {\'draft\': 28, \'medium\': 23, \'high\': 18},\\n                \'preset\': {\'draft\': \'ultrafast\', \'medium\': \'medium\', \'high\': \'slow\'},\\n                \'audio_bitrate\': \'192k\'\\n            },\\n            \'instagram_story\': {\\n                \'resolution\': \'1080x1920\',\\n                \'fps\': 30,\\n                \'crf\': {\'draft\': 28, \'medium\': 23, \'high\': 20},\\n                \'preset\': {\'draft\': \'ultrafast\', \'medium\': \'medium\', \'high\': \'medium\'},\\n                \'audio_bitrate\': \'128k\',\\n                \'max_duration\': 15\\n            },\\n            \'instagram_reel\': {\\n                \'resolution\': \'1080x1920\',\\n                \'fps\': 30,\\n                \'crf\': {\'draft\': 28, \'medium\': 23, \'high\': 20},\\n                \'preset\': {\'draft\': \'ultrafast\', \'medium\': \'medium\', \'high\': \'medium\'},\\n                \'audio_bitrate\': \'128k\',\\n                \'max_duration\': 90\\n            },\\n            \'instagram_feed\': {\\n                \'resolution\': \'1080x1080\',\\n                \'fps\': 30,\\n                \'crf\': {\'draft\': 28, \'medium\': 23, \'high\': 20},\\n                \'preset\': {\'draft\': \'ultrafast\', \'medium\': \'medium\', \'high\': \'medium\'},\\n                \'audio_bitrate\': \'128k\'\\n            },\\n            \'twitter\': {\\n                \'resolution\': \'1280x720\',\\n                \'fps\': 30,\\n                \'crf\': {\'draft\': 28, \'medium\': 23, \'high\': 20},\\n                \'preset\': {\'draft\': \'ultrafast\', \'medium\': \'medium\', \'high\': \'medium\'},\\n                \'audio_bitrate\': \'128k\',\\n                \'maxrate\': \'5000k\',\\n                \'bufsize\': \'10000k\'\\n            },\\n            \'tiktok\': {\\n                \'resolution\': \'1080x1920\',\\n                \'fps\': 30,\\n                \'crf\': {\'draft\': 28, \'medium\': 23, \'high\': 20},\\n                \'preset\': {\'draft\': \'ultrafast\', \'medium\': \'medium\', \'high\': \'medium\'},\\n                \'audio_bitrate\': \'128k\'\\n            },\\n            \'web\': {\\n                \'resolution\': \'1920x1080\',\\n                \'fps\': 30,\\n                \'crf\': {\'draft\': 28, \'medium\': 23, \'high\': 20},\\n                \'preset\': {\'draft\': \'ultrafast\', \'medium\': \'medium\', \'high\': \'medium\'},\\n                \'audio_bitrate\': \'128k\',\\n                \'profile\': \'baseline\',\\n                \'level\': \'3.0\'\\n            }\\n        }\\n\\n        if platform not in presets:\\n            raise ValueError(f\\"Unknown platform: {platform}\\")\\n\\n        config = presets[platform]\\n\\n        args = [\\n            \'-i\', input_file,\\n            \'-c:v\', \'libx264\',\\n            \'-preset\', config[\'preset\'][quality],\\n            \'-crf\', str(config[\'crf\'][quality]),\\n            \'-s\', config[\'resolution\'],\\n            \'-r\', str(config[\'fps\']),\\n            \'-pix_fmt\', \'yuv420p\',\\n            \'-color_primaries\', \'bt709\',\\n            \'-color_trc\', \'bt709\',\\n            \'-colorspace\', \'bt709\',\\n            \'-movflags\', \'+faststart\',\\n            \'-c:a\', \'aac\',\\n            \'-b:a\', config[\'audio_bitrate\'],\\n            \'-ar\', \'48000\'\\n        ]\\n\\n        if \'maxrate\' in config:\\n            args += [\'-maxrate\', config[\'maxrate\'], \'-bufsize\', config[\'bufsize\']]\\n\\n        if \'profile\' in config:\\n            args += [\'-profile:v\', config[\'profile\']]\\n\\n        if \'level\' in config:\\n            args += [\'-level\', config[\'level\']]\\n\\n        if \'max_duration\' in config:\\n            args += [\'-t\', str(config[\'max_duration\'])]\\n\\n        args.append(output_file)\\n\\n        self.run_ffmpeg(args)\\n        print(f\\"Export complete for {platform}: {output_file}\\")\\n\\n\\ndef main():\\n    parser = argparse.ArgumentParser(description=\'Professional video editor\')\\n    parser.add_argument(\'-v\', \'--verbose\', action=\'store_true\', help=\'Verbose output\')\\n\\n    subparsers = parser.add_subparsers(dest=\'command\', required=True)\\n\\n    # Cut command\\n    cut_parser = subparsers.add_parser(\'cut\', help=\'Cut/trim video\')\\n    cut_parser.add_argument(\'input\', help=\'Input video file\')\\n    cut_parser.add_argument(\'-s\', \'--start\', type=float, required=True, help=\'Start time (seconds)\')\\n    cut_parser.add_argument(\'-e\', \'--end\', type=float, help=\'End time (seconds)\')\\n    cut_parser.add_argument(\'-o\', \'--output\', required=True, help=\'Output file\')\\n    cut_parser.add_argument(\'--precise\', action=\'store_true\', help=\'Frame-accurate cutting\')\\n    cut_parser.add_argument(\'--reencode\', action=\'store_true\', help=\'Force re-encoding\')\\n\\n    # Concat command\\n    concat_parser = subparsers.add_parser(\'concat\', help=\'Concatenate videos\')\\n    concat_parser.add_argument(\'inputs\', nargs=\'+\', help=\'Input video files\')\\n    concat_parser.add_argument(\'-o\', \'--output\', required=True, help=\'Output file\')\\n    concat_parser.add_argument(\'--no-normalize\', action=\'store_true\', help=\'Skip color normalization\')\\n    concat_parser.add_argument(\'-t\', \'--transition\', choices=[\'fade\', \'dissolve\', \'wipe\'], help=\'Transition type\')\\n\\n    # Audio command\\n    audio_parser = subparsers.add_parser(\'audio\', help=\'Add/replace audio\')\\n    audio_parser.add_argument(\'video\', help=\'Input video file\')\\n    audio_parser.add_argument(\'audio\', help=\'Input audio file\')\\n    audio_parser.add_argument(\'-o\', \'--output\', required=True, help=\'Output file\')\\n    audio_parser.add_argument(\'--mix\', action=\'store_true\', help=\'Mix with original audio\')\\n    audio_parser.add_argument(\'--volume\', type=float, default=1.0, help=\'Volume multiplier\')\\n    audio_parser.add_argument(\'--offset\', type=float, default=0.0, help=\'Sync offset (seconds)\')\\n\\n    # Subtitles command\\n    subs_parser = subparsers.add_parser(\'subtitles\', help=\'Add subtitles\')\\n    subs_parser.add_argument(\'video\', help=\'Input video file\')\\n    subs_parser.add_argument(\'subtitles\', help=\'Subtitle file (.srt, .ass)\')\\n    subs_parser.add_argument(\'-o\', \'--output\', required=True, help=\'Output file\')\\n    subs_parser.add_argument(\'--soft\', action=\'store_true\', help=\'Soft subtitles (not burned)\')\\n\\n    # Color grade command\\n    grade_parser = subparsers.add_parser(\'grade\', help=\'Color grading\')\\n    grade_parser.add_argument(\'input\', help=\'Input video file\')\\n    grade_parser.add_argument(\'-o\', \'--output\', required=True, help=\'Output file\')\\n    grade_parser.add_argument(\'--brightness\', type=float, default=0.0, help=\'Brightness (-1 to 1)\')\\n    grade_parser.add_argument(\'--contrast\', type=float, default=1.0, help=\'Contrast (0 to 2)\')\\n    grade_parser.add_argument(\'--saturation\', type=float, default=1.0, help=\'Saturation (0 to 3)\')\\n    grade_parser.add_argument(\'--gamma\', type=float, default=1.0, help=\'Gamma (0.1 to 10)\')\\n\\n    # Export command\\n    export_parser = subparsers.add_parser(\'export\', help=\'Export for platform\')\\n    export_parser.add_argument(\'platform\', choices=[\\n        \'youtube\', \'instagram_story\', \'instagram_reel\', \'instagram_feed\',\\n        \'twitter\', \'tiktok\', \'linkedin\', \'web\'\\n    ], help=\'Target platform\')\\n    export_parser.add_argument(\'input\', help=\'Input video file\')\\n    export_parser.add_argument(\'-o\', \'--output\', required=True, help=\'Output file\')\\n    export_parser.add_argument(\'-q\', \'--quality\', choices=[\'draft\', \'medium\', \'high\'],\\n                                default=\'high\', help=\'Export quality\')\\n\\n    args = parser.parse_args()\\n\\n    editor = VideoEditor(verbose=args.verbose)\\n\\n    try:\\n        if args.command == \'cut\':\\n            editor.cut(\\n                args.input,\\n                args.output,\\n                args.start,\\n                args.end,\\n                precise=args.precise,\\n                reencode=args.reencode\\n            )\\n\\n        elif args.command == \'concat\':\\n            editor.concat(\\n                args.inputs,\\n                args.output,\\n                normalize_color=not args.no_normalize,\\n                transitions=args.transition\\n            )\\n\\n        elif args.command == \'audio\':\\n            editor.add_audio(\\n                args.video,\\n                args.audio,\\n                args.output,\\n                mix=args.mix,\\n                volume=args.volume,\\n                sync_offset=args.offset\\n            )\\n\\n        elif args.command == \'subtitles\':\\n            editor.add_subtitles(\\n                args.video,\\n                args.subtitles,\\n                args.output,\\n                burn=not args.soft\\n            )\\n\\n        elif args.command == \'grade\':\\n            editor.color_grade(\\n                args.input,\\n                args.output,\\n                brightness=args.brightness,\\n                contrast=args.contrast,\\n                saturation=args.saturation,\\n                gamma=args.gamma\\n            )\\n\\n        elif args.command == \'export\':\\n            editor.export_for_platform(\\n                args.input,\\n                args.output,\\n                args.platform,\\n                quality=args.quality\\n            )\\n\\n    except Exception as e:\\n        print(f\\"Error: {e}\\", file=sys.stderr)\\n        sys.exit(1)\\n\\n\\nif __name__ == \'__main__\':\\n    main()\\n"}]},{"name":"SKILL.md","type":"file","path":"video-processing-editing/SKILL.md","size":16642,"content":"---\\nname: video-processing-editing\\ndescription: FFmpeg automation for cutting, trimming, concatenating videos. Audio mixing, timeline editing, transitions, effects. Export optimization for YouTube, social media. Subtitle handling, color grading, batch processing. Use for videogen projects, content creation, automated video production. Activate on \\"video editing\\", \\"FFmpeg\\", \\"trim video\\", \\"concatenate\\", \\"transitions\\", \\"export optimization\\". NOT for real-time video editing UI, 3D compositing, or motion graphics.\\nallowed-tools: Read,Write,Edit,Bash(ffmpeg*,ffprobe*,python*)\\n---\\n\\n# Video Processing & Editing\\n\\nExpert in FFmpeg-based video editing, processing automation, and export optimization for modern content creation workflows.\\n\\n## When to Use\\n\\n\u2705 **Use for**:\\n- Automated video editing pipelines (script-to-video)\\n- Cutting, trimming, concatenating clips\\n- Adding transitions, effects, overlays\\n- Audio mixing and normalization\\n- Subtitle/caption handling\\n- Export optimization for platforms\\n- Batch video processing\\n- Color grading and correction\\n\\n\u274c **NOT for**:\\n- Real-time video editing UI (use DaVinci Resolve/Premiere)\\n- 3D compositing (use After Effects/Blender)\\n- Motion graphics animation (use After Effects)\\n- Basic screen recording (use OBS)\\n\\n---\\n\\n## Technology Selection\\n\\n### Video Editing Tools\\n\\n| Tool | Speed | Features | Use Case |\\n|------|-------|----------|----------|\\n| FFmpeg | Very Fast | CLI automation | Production pipelines |\\n| MoviePy | Medium | Python API | Programmatic editing |\\n| PyAV | Fast | Low-level control | Custom processing |\\n| DaVinci Resolve | Slow | Full NLE | Manual editing |\\n\\n**Decision tree**:\\n```\\nNeed automation? \u2192 FFmpeg\\nNeed Python API? \u2192 MoviePy\\nNeed frame-level control? \u2192 PyAV\\nNeed manual editing? \u2192 DaVinci Resolve\\n```\\n\\n---\\n\\n## Common Anti-Patterns\\n\\n### Anti-Pattern 1: Not Using Keyframe-Aligned Cuts\\n\\n**Novice thinking**: \\"Just cut the video at any timestamp\\"\\n\\n**Problem**: Causes artifacts, black frames, and playback issues.\\n\\n**Wrong approach**:\\n```bash\\n# \u274c Cut at arbitrary timestamp (not keyframe-aligned)\\nffmpeg -i input.mp4 -ss 00:01:23.456 -to 00:02:45.678 -c copy output.mp4\\n\\n# Result: Black frames, artifacts, sync issues\\n```\\n\\n**Why wrong**:\\n- Video codecs use keyframes (I-frames) every 2-10 seconds\\n- Non-keyframe cuts require re-encoding\\n- Using `-c copy` (stream copy) without keyframe alignment breaks playback\\n- GOP (Group of Pictures) structure depends on keyframes\\n\\n**Correct approach 1**: Re-encode for precise cuts\\n```bash\\n# \u2705 Re-encode for frame-accurate cutting\\nffmpeg -i input.mp4 -ss 00:01:23.456 -to 00:02:45.678 \\\\\\n  -c:v libx264 -crf 18 -preset medium \\\\\\n  -c:a aac -b:a 192k \\\\\\n  output.mp4\\n\\n# Frame-accurate, but slower (re-encoding)\\n```\\n\\n**Correct approach 2**: Keyframe-aligned stream copy\\n```bash\\n# \u2705 Fast cutting with keyframe alignment\\n# Step 1: Find keyframes near cut points\\nffprobe -select_streams v -show_frames -show_entries frame=pkt_pts_time,key_frame \\\\\\n  -of csv input.mp4 | grep \\",1$\\" | awk -F\',\' \'{print $2}\'\\n\\n# Step 2: Cut at nearest keyframes (fast, no re-encoding)\\nffmpeg -i input.mp4 -ss 00:01:22.000 -to 00:02:46.000 -c copy output.mp4\\n\\n# Blazing fast, no quality loss, but not frame-accurate\\n```\\n\\n**Correct approach 3**: Two-pass for best of both worlds\\n```bash\\n# \u2705 Fast seek + precise cut\\nffmpeg -ss 00:01:20.000 -i input.mp4 \\\\\\n  -ss 00:00:03.456 -to 00:01:25.678 \\\\\\n  -c:v libx264 -crf 18 -preset medium \\\\\\n  -c:a aac -b:a 192k \\\\\\n  output.mp4\\n\\n# -ss BEFORE -i: Fast seek to keyframe (no decode)\\n# -ss AFTER -i: Precise trim (only decode needed portion)\\n```\\n\\n**Performance comparison**:\\n| Method | Time (1-hour video) | Accuracy | Quality |\\n|--------|---------------------|----------|---------|\\n| Stream copy (arbitrary) | 2s | \u274c Broken | \u274c Artifacts |\\n| Stream copy (keyframe) | 2s | \xb12s | \u2705 Perfect |\\n| Re-encode (simple) | 15min | \u2705 Frame | \u26a0\ufe0f Quality loss |\\n| Two-pass (optimal) | 3min | \u2705 Frame | \u2705 Perfect |\\n\\n**Timeline context**:\\n- 2010: FFmpeg required full re-encoding for cuts\\n- 2015: `-c copy` added for stream copying\\n- 2020: Two-pass cutting became best practice\\n- 2024: Hardware acceleration (NVENC) makes re-encoding viable\\n\\n---\\n\\n### Anti-Pattern 2: Re-encoding Unnecessarily\\n\\n**Novice thinking**: \\"Apply all edits in one FFmpeg command\\"\\n\\n**Problem**: Multiple re-encodings cause cumulative quality loss.\\n\\n**Wrong approach**:\\n```bash\\n# \u274c Re-encode for each operation (quality degradation)\\n# Operation 1: Trim\\nffmpeg -i input.mp4 -ss 00:01:00 -to 00:05:00 \\\\\\n  -c:v libx264 -crf 23 temp1.mp4\\n\\n# Operation 2: Add audio\\nffmpeg -i temp1.mp4 -i audio.mp3 -c:v libx264 -crf 23 \\\\\\n  -map 0:v -map 1:a temp2.mp4\\n\\n# Operation 3: Add subtitles\\nffmpeg -i temp2.mp4 -vf subtitles=subs.srt \\\\\\n  -c:v libx264 -crf 23 output.mp4\\n\\n# Result: 3x re-encoding = significant quality loss\\n```\\n\\n**Why wrong**:\\n- Each re-encode is lossy (even with high CRF)\\n- Cumulative quality loss (generation loss)\\n- 3x encoding time\\n- Wasted disk I/O\\n\\n**Correct approach 1**: Chain operations in single command\\n```bash\\n# \u2705 Single-pass encoding with all operations\\nffmpeg -ss 00:01:00 -i input.mp4 -i audio.mp3 \\\\\\n  -to 00:04:00 \\\\\\n  -vf \\"subtitles=subs.srt\\" \\\\\\n  -map 0:v -map 1:a \\\\\\n  -c:v libx264 -crf 18 -preset medium \\\\\\n  -c:a aac -b:a 192k \\\\\\n  output.mp4\\n\\n# Single re-encode, all operations applied at once\\n```\\n\\n**Correct approach 2**: Use stream copy when possible\\n```bash\\n# \u2705 Lossless operations with stream copy\\n# Trim (stream copy)\\nffmpeg -i input.mp4 -ss 00:01:00 -to 00:05:00 -c copy temp.mp4\\n\\n# Add audio (stream copy video, encode audio)\\nffmpeg -i temp.mp4 -i audio.mp3 \\\\\\n  -map 0:v -map 1:a \\\\\\n  -c:v copy -c:a aac -b:a 192k \\\\\\n  temp2.mp4\\n\\n# Burn subtitles (must re-encode video)\\nffmpeg -i temp2.mp4 -vf subtitles=subs.srt \\\\\\n  -c:v libx264 -crf 18 -preset medium \\\\\\n  -c:a copy \\\\\\n  output.mp4\\n\\n# Only 1 video re-encode (for subtitles)\\n```\\n\\n**Quality comparison**:\\n| Method | Encoding Passes | Quality (VMAF) | Time |\\n|--------|-----------------|----------------|------|\\n| 3x re-encode (CRF 23) | 3 | 82/100 | 45min |\\n| Single pass (CRF 23) | 1 | 91/100 | 15min |\\n| Stream copy + 1 encode | 1 | 95/100 | 18min |\\n| All stream copy | 0 | 100/100 | 30s |\\n\\n---\\n\\n### Anti-Pattern 3: Ignoring Color Space Conversions\\n\\n**Novice thinking**: \\"Just concatenate videos together\\"\\n\\n**Problem**: Color shifts, mismatched brightness, broken playback.\\n\\n**Wrong approach**:\\n```bash\\n# \u274c Concatenate videos with different color spaces\\n# clip1.mp4: BT.709 (HD), yuv420p\\n# clip2.mp4: BT.601 (SD), yuvj420p (full range)\\n# clip3.mp4: BT.2020 (HDR), yuv420p10le\\n\\n# Create concat list\\necho \\"file \'clip1.mp4\'\\" > list.txt\\necho \\"file \'clip2.mp4\'\\" >> list.txt\\necho \\"file \'clip3.mp4\'\\" >> list.txt\\n\\n# Concatenate without color normalization\\nffmpeg -f concat -safe 0 -i list.txt -c copy output.mp4\\n\\n# Result: Color shifts between clips, broken HDR metadata\\n```\\n\\n**Why wrong**:\\n- Different color spaces (BT.601 vs BT.709 vs BT.2020)\\n- Different pixel formats (yuv420p vs yuvj420p)\\n- Different color ranges (limited vs full)\\n- Metadata conflicts\\n\\n**Correct approach**:\\n```bash\\n# \u2705 Normalize color space before concatenation\\n\\n# Step 1: Analyze color space of each clip\\nffprobe -v error -select_streams v:0 \\\\\\n  -show_entries stream=color_space,color_transfer,color_primaries,pix_fmt \\\\\\n  -of default=noprint_wrappers=1 clip1.mp4\\n\\n# Step 2: Normalize all clips to common color space\\n# Target: BT.709 (HD), yuv420p, limited range\\n\\n# Normalize clip1 (already BT.709)\\nffmpeg -i clip1.mp4 -c copy clip1_normalized.mp4\\n\\n# Normalize clip2 (BT.601 SD \u2192 BT.709 HD)\\nffmpeg -i clip2.mp4 \\\\\\n  -vf \\"scale=in_range=full:out_range=limited,colorspace=bt709:iall=bt601:fast=1\\" \\\\\\n  -color_primaries bt709 \\\\\\n  -color_trc bt709 \\\\\\n  -colorspace bt709 \\\\\\n  -c:v libx264 -crf 18 -preset medium \\\\\\n  -c:a copy \\\\\\n  clip2_normalized.mp4\\n\\n# Normalize clip3 (BT.2020 HDR \u2192 BT.709 SDR)\\nffmpeg -i clip3.mp4 \\\\\\n  -vf \\"zscale=t=linear:npl=100,format=gbrpf32le,zscale=p=bt709,tonemap=hable:desat=0,zscale=t=bt709:m=bt709:r=limited,format=yuv420p\\" \\\\\\n  -color_primaries bt709 \\\\\\n  -color_trc bt709 \\\\\\n  -colorspace bt709 \\\\\\n  -c:v libx264 -crf 18 -preset medium \\\\\\n  -c:a copy \\\\\\n  clip3_normalized.mp4\\n\\n# Step 3: Concatenate normalized clips\\necho \\"file \'clip1_normalized.mp4\'\\" > list.txt\\necho \\"file \'clip2_normalized.mp4\'\\" >> list.txt\\necho \\"file \'clip3_normalized.mp4\'\\" >> list.txt\\n\\nffmpeg -f concat -safe 0 -i list.txt -c copy output.mp4\\n```\\n\\n**Color space guide**:\\n| Standard | Color Space | Transfer | Primaries | Use Case |\\n|----------|-------------|----------|-----------|----------|\\n| BT.601 | SD | bt470bg | bt470bg | Old SD content |\\n| BT.709 | HD | bt709 | bt709 | Modern HD/FHD |\\n| BT.2020 | UHD/HDR | smpte2084 | bt2020 | 4K HDR |\\n| sRGB | Web | iec61966-2-1 | bt709 | Web delivery |\\n\\n---\\n\\n### Anti-Pattern 4: Poor Audio Sync\\n\\n**Novice thinking**: \\"Video and audio are separate, just overlay them\\"\\n\\n**Problem**: Lip sync issues, audio drift, broken playback.\\n\\n**Wrong approach**:\\n```bash\\n# \u274c Replace audio without sync consideration\\nffmpeg -i video.mp4 -i audio.mp3 \\\\\\n  -map 0:v -map 1:a \\\\\\n  -c:v copy -c:a copy \\\\\\n  output.mp4\\n\\n# Problems:\\n# - Audio duration \u2260 video duration\\n# - No audio stretching/compression\\n# - Drift over time\\n```\\n\\n**Why wrong**:\\n- Audio and video have different durations\\n- No timebase synchronization\\n- No drift correction\\n- Ignores original audio sync\\n\\n**Correct approach 1**: Stretch/compress audio to match video\\n```bash\\n# \u2705 Adjust audio speed to match video duration\\n\\n# Get durations\\nVIDEO_DUR=$(ffprobe -v error -show_entries format=duration \\\\\\n  -of default=noprint_wrappers=1:nokey=1 video.mp4)\\nAUDIO_DUR=$(ffprobe -v error -show_entries format=duration \\\\\\n  -of default=noprint_wrappers=1:nokey=1 audio.mp3)\\n\\n# Calculate speed ratio\\nRATIO=$(echo \\"$VIDEO_DUR / $AUDIO_DUR\\" | bc -l)\\n\\n# Stretch audio to match video (with pitch correction)\\nffmpeg -i video.mp4 -i audio.mp3 \\\\\\n  -filter_complex \\"[1:a]atempo=${RATIO}[a]\\" \\\\\\n  -map 0:v -map \\"[a]\\" \\\\\\n  -c:v copy -c:a aac -b:a 192k \\\\\\n  output.mp4\\n```\\n\\n**Correct approach 2**: Precise offset and trim\\n```bash\\n# \u2705 Sync audio with offset and trim\\n\\n# Audio starts 0.5s late, trim to match video\\nffmpeg -i video.mp4 -itsoffset 0.5 -i audio.mp3 \\\\\\n  -map 0:v -map 1:a \\\\\\n  -shortest \\\\\\n  -c:v copy -c:a aac -b:a 192k \\\\\\n  output.mp4\\n\\n# -itsoffset: Delay audio by 0.5s\\n# -shortest: Trim to shortest stream\\n```\\n\\n**Correct approach 3**: Mix multiple audio tracks with sync\\n```bash\\n# \u2705 Mix dialogue, music, effects with precise timing\\n\\nffmpeg -i video.mp4 -i dialogue.wav -i music.mp3 -i sfx.wav \\\\\\n  -filter_complex \\"\\n    [1:a]adelay=0|0[dlg];\\n    [2:a]volume=0.3,adelay=500|500[mus];\\n    [3:a]adelay=1200|1200[sfx];\\n    [dlg][mus][sfx]amix=inputs=3:duration=first[a]\\n  \\" \\\\\\n  -map 0:v -map \\"[a]\\" \\\\\\n  -c:v copy -c:a aac -b:a 256k \\\\\\n  output.mp4\\n\\n# adelay: Precise millisecond timing\\n# amix: Mix multiple audio streams\\n# volume: Normalize levels\\n```\\n\\n**Audio sync checklist**:\\n```\\n\u25a1 Verify video and audio durations match\\n\u25a1 Use -shortest to prevent excess audio\\n\u25a1 Apply adelay for precise timing offsets\\n\u25a1 Use atempo for speed adjustment (maintains pitch)\\n\u25a1 Set audio bitrate appropriately (128k-256k)\\n\u25a1 Test lip sync at beginning, middle, end\\n```\\n\\n---\\n\\n### Anti-Pattern 5: Wrong Codec/Bitrate for Platform\\n\\n**Novice thinking**: \\"One export settings for everything\\"\\n\\n**Problem**: Wasted bandwidth, poor quality, rejected uploads, compatibility issues.\\n\\n**Wrong approach**:\\n```bash\\n# \u274c Export everything at 4K 50 Mbps\\nffmpeg -i input.mp4 \\\\\\n  -c:v libx264 -b:v 50M -s 3840x2160 \\\\\\n  -c:a aac -b:a 320k \\\\\\n  output.mp4\\n\\n# For Instagram story: 2 GB file, rejected (max 100 MB)\\n# For YouTube: Could use 10 Mbps and look identical\\n# For Twitter: Exceeds bitrate limits\\n```\\n\\n**Why wrong**:\\n- Platform-specific size/bitrate limits\\n- Over-encoding wastes bandwidth\\n- Wrong resolution for platform\\n- Incompatible codecs\\n\\n**Correct approach**: Platform-optimized exports\\n\\n**YouTube (recommended settings)**:\\n```bash\\n# \u2705 YouTube 1080p upload\\nffmpeg -i input.mp4 \\\\\\n  -c:v libx264 -preset slow -crf 18 \\\\\\n  -s 1920x1080 -r 30 \\\\\\n  -pix_fmt yuv420p \\\\\\n  -color_primaries bt709 -color_trc bt709 -colorspace bt709 \\\\\\n  -movflags +faststart \\\\\\n  -c:a aac -b:a 192k -ar 48000 \\\\\\n  youtube_1080p.mp4\\n\\n# YouTube 4K upload\\nffmpeg -i input.mp4 \\\\\\n  -c:v libx264 -preset slow -crf 18 \\\\\\n  -s 3840x2160 -r 60 \\\\\\n  -pix_fmt yuv420p \\\\\\n  -movflags +faststart \\\\\\n  -c:a aac -b:a 256k -ar 48000 \\\\\\n  youtube_4k.mp4\\n```\\n\\n**Instagram (Stories, Reels, Feed)**:\\n```bash\\n# \u2705 Instagram Story (9:16, max 100 MB, 15s)\\nffmpeg -i input.mp4 \\\\\\n  -c:v libx264 -preset medium -crf 23 \\\\\\n  -s 1080x1920 -r 30 -t 15 \\\\\\n  -pix_fmt yuv420p \\\\\\n  -movflags +faststart \\\\\\n  -c:a aac -b:a 128k \\\\\\n  instagram_story.mp4\\n\\n# \u2705 Instagram Reel (9:16, max 90s)\\nffmpeg -i input.mp4 \\\\\\n  -c:v libx264 -preset medium -crf 23 \\\\\\n  -s 1080x1920 -r 30 -t 90 \\\\\\n  -pix_fmt yuv420p \\\\\\n  -movflags +faststart \\\\\\n  -c:a aac -b:a 128k \\\\\\n  instagram_reel.mp4\\n\\n# \u2705 Instagram Feed (1:1 or 4:5)\\nffmpeg -i input.mp4 \\\\\\n  -c:v libx264 -preset medium -crf 23 \\\\\\n  -s 1080x1080 -r 30 \\\\\\n  -pix_fmt yuv420p \\\\\\n  -movflags +faststart \\\\\\n  -c:a aac -b:a 128k \\\\\\n  instagram_feed.mp4\\n```\\n\\n**Twitter/X**:\\n```bash\\n# \u2705 Twitter video (max 512 MB, 2:20)\\nffmpeg -i input.mp4 \\\\\\n  -c:v libx264 -preset medium -crf 23 \\\\\\n  -s 1280x720 -r 30 -t 140 \\\\\\n  -maxrate 5000k -bufsize 10000k \\\\\\n  -pix_fmt yuv420p \\\\\\n  -movflags +faststart \\\\\\n  -c:a aac -b:a 128k \\\\\\n  twitter.mp4\\n```\\n\\n**TikTok**:\\n```bash\\n# \u2705 TikTok (9:16, max 287 MB, 10 min)\\nffmpeg -i input.mp4 \\\\\\n  -c:v libx264 -preset medium -crf 23 \\\\\\n  -s 1080x1920 -r 30 -t 600 \\\\\\n  -pix_fmt yuv420p \\\\\\n  -movflags +faststart \\\\\\n  -c:a aac -b:a 128k \\\\\\n  tiktok.mp4\\n```\\n\\n**Web (HTML5 video)**:\\n```bash\\n# \u2705 Web optimized (fast load, broad compatibility)\\nffmpeg -i input.mp4 \\\\\\n  -c:v libx264 -preset medium -crf 23 \\\\\\n  -s 1920x1080 -r 30 \\\\\\n  -pix_fmt yuv420p \\\\\\n  -profile:v baseline -level 3.0 \\\\\\n  -movflags +faststart \\\\\\n  -c:a aac -b:a 128k -ar 48000 \\\\\\n  web.mp4\\n```\\n\\n**Platform specs table**:\\n| Platform | Max Size | Max Duration | Resolution | FPS | Bitrate | Codec |\\n|----------|----------|--------------|------------|-----|---------|-------|\\n| YouTube | Unlimited | Unlimited | 8K | 60 | Auto | H.264/VP9 |\\n| Instagram Story | 100 MB | 15s | 1080x1920 | 30 | ~5 Mbps | H.264 |\\n| Instagram Reel | 1 GB | 90s | 1080x1920 | 30 | ~8 Mbps | H.264 |\\n| Twitter | 512 MB | 2:20 | 1920x1080 | 60 | 5 Mbps | H.264 |\\n| TikTok | 287 MB | 10min | 1080x1920 | 30 | ~4 Mbps | H.264 |\\n| LinkedIn | 5 GB | 10min | 1920x1080 | 30 | 5 Mbps | H.264 |\\n| Web | Varies | Varies | 1920x1080 | 30 | 2-5 Mbps | H.264 |\\n\\n**Export optimization checklist**:\\n```\\n\u25a1 Use -movflags +faststart for web (progressive download)\\n\u25a1 Use -pix_fmt yuv420p for broad compatibility\\n\u25a1 Set -r 30 for most platforms (avoid variable framerate)\\n\u25a1 Use -preset slow for final exports (better quality)\\n\u25a1 Use -preset ultrafast for drafts\\n\u25a1 Apply -maxrate and -bufsize for streaming\\n\u25a1 Test playback on target platform before bulk export\\n```\\n\\n---\\n\\n## Production Checklist\\n\\n```\\n\u25a1 Align cuts to keyframes (or two-pass seek)\\n\u25a1 Chain operations in single FFmpeg command\\n\u25a1 Normalize color spaces before concatenating\\n\u25a1 Verify audio/video sync (test at multiple points)\\n\u25a1 Use platform-specific export presets\\n\u25a1 Apply -movflags +faststart for web delivery\\n\u25a1 Set proper color metadata (bt709 for HD)\\n\u25a1 Test output file on target platform\\n\u25a1 Keep lossless intermediate files (ProRes, FFV1)\\n\u25a1 Use hardware acceleration for batch jobs (NVENC, VideoToolbox)\\n```\\n\\n---\\n\\n## When to Use vs Avoid\\n\\n| Scenario | Appropriate? |\\n|----------|--------------|\\n| Automated video pipeline (script \u2192 video) | \u2705 Yes - FFmpeg automation |\\n| Batch process 100 videos | \u2705 Yes - parallel FFmpeg jobs |\\n| Trim/cut clips programmatically | \u2705 Yes - precise cutting |\\n| Add subtitles to videos | \u2705 Yes - burn or soft subs |\\n| Color grade footage | \u26a0\ufe0f Limited - basic only |\\n| Multi-cam editing | \u274c No - use DaVinci Resolve |\\n| Motion graphics | \u274c No - use After Effects |\\n| Real-time preview editing | \u274c No - use Premiere/Resolve |\\n\\n---\\n\\n## References\\n\\n- `/references/ffmpeg-guide.md` - Complete FFmpeg command reference\\n- `/references/timeline-editing.md` - Timeline concepts, multi-track editing\\n- `/references/export-optimization.md` - Platform-specific export settings\\n\\n## Scripts\\n\\n- `scripts/video_editor.py` - Cut, trim, concatenate, transitions, effects\\n- `scripts/batch_processor.py` - Parallel batch video processing\\n\\n---\\n\\n**This skill guides**: Video editing | FFmpeg | Timeline editing | Transitions | Export optimization | Audio mixing | Color grading | Automated video production\\n"}]}')}}]);