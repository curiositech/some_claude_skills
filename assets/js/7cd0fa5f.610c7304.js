"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[8883],{28453:(e,n,t)=>{t.d(n,{R:()=>c,x:()=>o});var s=t(96540);const r={},i=s.createContext(r);function c(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:c(e.components),s.createElement(i.Provider,{value:n},e.children)}},72358:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>p,frontMatter:()=>c,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"skills/photo_content_recognition_curation_expert/references/content-detection","title":"Content Detection Reference","description":"Pet Recognition & Clustering","source":"@site/docs/skills/photo_content_recognition_curation_expert/references/content-detection.md","sourceDirName":"skills/photo_content_recognition_curation_expert/references","slug":"/skills/photo_content_recognition_curation_expert/references/content-detection","permalink":"/docs/skills/photo_content_recognition_curation_expert/references/content-detection","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Content Detection Reference","sidebar_label":"Content Detection Reference","sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Photo Content Recognition Curation Expert","permalink":"/docs/skills/photo_content_recognition_curation_expert/"},"next":{"title":"Face Recognition & Clusteri...","permalink":"/docs/skills/photo_content_recognition_curation_expert/references/face-clustering"}}');var r=t(74848),i=t(28453);const c={title:"Content Detection Reference",sidebar_label:"Content Detection Reference",sidebar_position:1},o="Content Detection Reference",a={},l=[{value:"Pet Recognition &amp; Clustering",id:"pet-recognition--clustering",level:2},{value:"Burst Photo Selection",id:"burst-photo-selection",level:2},{value:"Screenshot vs Photo Classification",id:"screenshot-vs-photo-classification",level:2},{value:"Burst Selection Weights",id:"burst-selection-weights",level:2},{value:"Screenshot Detection Signals",id:"screenshot-detection-signals",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"content-detection-reference",children:"Content Detection Reference"})}),"\n",(0,r.jsx)(n.h2,{id:"pet-recognition--clustering",children:"Pet Recognition & Clustering"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from ultralytics import YOLO\nfrom transformers import CLIPProcessor, CLIPModel\n\nclass PetRecognizer:\n    \"\"\"Pet detection and clustering.\"\"\"\n\n    def __init__(self):\n        self.yolo = YOLO('yolov8n.pt')\n        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n    def detect_animals(self, image):\n        \"\"\"Detect animals in image.\"\"\"\n        results = self.yolo(image)\n\n        animals = []\n        animal_classes = ['cat', 'dog', 'horse', 'bird', 'cow', 'sheep',\n                         'elephant', 'bear', 'zebra', 'giraffe']\n\n        for result in results:\n            for box in result.boxes:\n                class_name = result.names[int(box.cls)]\n                if class_name in animal_classes:\n                    animals.append({\n                        'type': class_name,\n                        'bbox': box.xyxy[0].cpu().numpy(),\n                        'confidence': float(box.conf)\n                    })\n        return animals\n\n    def extract_pet_embedding(self, image, bbox):\n        \"\"\"Extract embedding for individual animal using CLIP.\"\"\"\n        x1, y1, x2, y2 = map(int, bbox)\n        crop = image.crop((x1, y1, x2, y2))\n\n        inputs = self.clip_processor(images=crop, return_tensors=\"pt\")\n        with torch.no_grad():\n            embedding = self.clip_model.get_image_features(**inputs)\n        return embedding.cpu().numpy().flatten()\n\n    def cluster_pets(self, pet_embeddings, min_cluster_size=5):\n        \"\"\"Cluster pet embeddings (same individual = cluster).\"\"\"\n        clusterer = hdbscan.HDBSCAN(\n            min_cluster_size=min_cluster_size,\n            min_samples=2,\n            metric='cosine'\n        )\n        return clusterer.fit_predict(np.array(pet_embeddings))\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"burst-photo-selection",children:"Burst Photo Selection"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Problem:"})," Burst mode creates 10-50 nearly identical photos. Need to select best frame."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solution:"})," Multi-criteria scoring: sharpness, face quality, aesthetics, composition."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class BurstPhotoSelector:\n    """Select best photo from camera burst sequence."""\n\n    def __init__(self):\n        self.face_detector = FaceEmbeddingExtractor()\n        self.aesthetic_scorer = NIMAPredictor()\n\n    def detect_bursts(self, photos_with_timestamps, max_gap_seconds=0.5):\n        """Detect burst sequences from timestamps."""\n        sorted_photos = sorted(photos_with_timestamps, key=lambda x: x[1])\n\n        bursts = []\n        current_burst = [sorted_photos[0]]\n\n        for photo in sorted_photos[1:]:\n            time_gap = (photo[1] - current_burst[-1][1]).total_seconds()\n            if time_gap <= max_gap_seconds:\n                current_burst.append(photo)\n            else:\n                if len(current_burst) >= 3:\n                    bursts.append(current_burst)\n                current_burst = [photo]\n\n        if len(current_burst) >= 3:\n            bursts.append(current_burst)\n\n        return bursts\n\n    def select_best_from_burst(self, burst_photos):\n        """\n        Select best photo from burst.\n        Criteria: Sharpness, Face quality, Aesthetics, Position, Exposure\n        """\n        scores = []\n\n        for idx, (photo_id, timestamp, image) in enumerate(burst_photos):\n            score = 0.0\n\n            # 1. SHARPNESS (30%)\n            gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n            sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()\n            score += min(1.0, sharpness / 1000) * 0.30\n\n            # 2. FACE QUALITY (35%)\n            faces = self.face_detector.extract_faces(image)\n            if faces:\n                face_scores = [self.assess_face_quality(f) for f in faces]\n                score += np.mean(face_scores) * 0.35\n            else:\n                score += 0.5 * 0.35\n\n            # 3. AESTHETIC SCORE (20%)\n            score += self.aesthetic_scorer.predict(image) * 0.20\n\n            # 4. POSITION BONUS - middle frames (10%)\n            position = idx / len(burst_photos)\n            center_bonus = 1.0 - abs(position - 0.5) * 2\n            score += center_bonus * 0.10\n\n            # 5. EXPOSURE (5%)\n            score += self.assess_exposure(image) * 0.05\n\n            scores.append((photo_id, score))\n\n        return max(scores, key=lambda x: x[1])[0]\n\n    def assess_face_quality(self, face_dict):\n        """Assess quality: eyes open, not blurry, smiling."""\n        face_crop = face_dict[\'crop\']\n        gray_face = cv2.cvtColor(np.array(face_crop), cv2.COLOR_RGB2GRAY)\n        face_sharpness = cv2.Laplacian(gray_face, cv2.CV_64F).var()\n        sharpness_score = min(1.0, face_sharpness / 500)\n\n        # Production: Use landmarks or emotion classifier\n        eyes_open_score = 0.8\n        smiling_score = 0.7\n\n        return (sharpness_score * 0.4 + eyes_open_score * 0.3 + smiling_score * 0.3)\n\n    def assess_exposure(self, image):\n        """Check if image is properly exposed."""\n        gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n        hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n        hist = hist.flatten() / hist.sum()\n\n        clipping = np.sum(hist[:20]) + np.sum(hist[235:])\n        return max(0, 1.0 - clipping / 0.05)\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"screenshot-vs-photo-classification",children:"Screenshot vs Photo Classification"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Signals:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"EXIF metadata (camera info missing)"}),"\n",(0,r.jsx)(n.li,{children:"UI elements (status bars, buttons)"}),"\n",(0,r.jsx)(n.li,{children:"Text density"}),"\n",(0,r.jsx)(n.li,{children:"Perfect rectangles"}),"\n",(0,r.jsx)(n.li,{children:"Device-specific aspect ratios"}),"\n",(0,r.jsx)(n.li,{children:"Sharpness (screenshots are perfectly sharp)"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"class ScreenshotDetector:\n    \"\"\"Classify image as screenshot vs photo.\"\"\"\n\n    def __init__(self):\n        self.text_detector = self.init_text_detector()\n        self.ui_detector = self.init_ui_detector()\n\n    def is_screenshot(self, image, metadata=None):\n        \"\"\"\n        Determine if image is screenshot.\n        Returns: (bool, confidence)\n        \"\"\"\n        signals = []\n\n        # SIGNAL 1: EXIF metadata\n        if metadata:\n            has_camera_info = any(k in metadata for k in\n                                ['Make', 'Model', 'LensModel', 'FocalLength'])\n            if not has_camera_info:\n                signals.append(('no_camera_exif', 0.6))\n        else:\n            signals.append(('no_metadata', 0.5))\n\n        # SIGNAL 2: UI elements\n        ui_elements = self.detect_ui_elements(image)\n        if ui_elements:\n            signals.append(('ui_elements', 0.85))\n\n        # SIGNAL 3: Text density\n        text_coverage = self.compute_text_coverage(image)\n        if text_coverage > 0.25:\n            signals.append(('high_text', 0.7))\n\n        # SIGNAL 4: Perfect rectangles\n        perfect_rects = self.detect_perfect_rectangles(image)\n        if perfect_rects > 5:\n            signals.append(('perfect_rects', 0.75))\n\n        # SIGNAL 5: Device aspect ratio\n        h, w = np.array(image).shape[:2]\n        aspect = w / h\n        device_aspects = [(16/9, 'standard'), (1125/2436, 'iphone_x'),\n                         (1080/1920, 'android_fhd'), (1440/2960, 'samsung_s8')]\n\n        for target_aspect, device_name in device_aspects:\n            if abs(aspect - target_aspect) < 0.01:\n                signals.append((f'device_aspect_{device_name}', 0.6))\n                break\n\n        # SIGNAL 6: Perfect sharpness\n        sharpness = self.compute_sharpness(image)\n        if sharpness > 2000:\n            signals.append(('perfect_sharpness', 0.5))\n\n        if not signals:\n            return False, 0.0\n\n        max_confidence = max(conf for _, conf in signals)\n        return max_confidence > 0.6, max_confidence\n\n    def detect_ui_elements(self, image):\n        \"\"\"Detect status bars, buttons, icons.\"\"\"\n        h, w = np.array(image).shape[:2]\n        top_strip = np.array(image)[:int(h * 0.05), :]\n        top_variance = np.var(top_strip)\n\n        if top_variance < 100:\n            return [{'type': 'status_bar', 'confidence': 0.8}]\n        return []\n\n    def compute_text_coverage(self, image):\n        \"\"\"Compute % of image covered by text.\"\"\"\n        import pytesseract\n        data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)\n\n        total_area = image.width * image.height\n        text_area = sum(\n            data['width'][i] * data['height'][i]\n            for i, conf in enumerate(data['conf']) if conf > 0\n        )\n        return text_area / total_area\n\n    def detect_perfect_rectangles(self, image):\n        \"\"\"Detect pixel-perfect rectangles (UI buttons).\"\"\"\n        gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n        edges = cv2.Canny(gray, 50, 150)\n        contours, _ = cv2.findContours(edges, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n        perfect_rects = 0\n        for contour in contours:\n            epsilon = 0.01 * cv2.arcLength(contour, True)\n            approx = cv2.approxPolyDP(contour, epsilon, True)\n            if len(approx) == 4:\n                perfect_rects += 1\n        return perfect_rects\n\n    def compute_sharpness(self, image):\n        gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n        return cv2.Laplacian(gray, cv2.CV_64F).var()\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"burst-selection-weights",children:"Burst Selection Weights"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Criterion"}),(0,r.jsx)(n.th,{children:"Weight"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Sharpness"}),(0,r.jsx)(n.td,{children:"30%"}),(0,r.jsx)(n.td,{children:"Laplacian variance"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Face Quality"}),(0,r.jsx)(n.td,{children:"35%"}),(0,r.jsx)(n.td,{children:"Eyes open, smiling, face sharpness"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Aesthetics"}),(0,r.jsx)(n.td,{children:"20%"}),(0,r.jsx)(n.td,{children:"NIMA score"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Position"}),(0,r.jsx)(n.td,{children:"10%"}),(0,r.jsx)(n.td,{children:"Middle frames bonus"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Exposure"}),(0,r.jsx)(n.td,{children:"5%"}),(0,r.jsx)(n.td,{children:"Not over/underexposed"})]})]})]}),"\n",(0,r.jsx)(n.h2,{id:"screenshot-detection-signals",children:"Screenshot Detection Signals"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Signal"}),(0,r.jsx)(n.th,{children:"Confidence"}),(0,r.jsx)(n.th,{children:"Description"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"UI elements"}),(0,r.jsx)(n.td,{children:"0.85"}),(0,r.jsx)(n.td,{children:"Status bars, buttons"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Perfect rectangles"}),(0,r.jsx)(n.td,{children:"0.75"}),(0,r.jsx)(n.td,{children:"UI buttons (4 corners, 90\xb0 angles)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"High text"}),(0,r.jsx)(n.td,{children:"0.70"}),(0,r.jsx)(n.td,{children:">25% text coverage"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"No camera EXIF"}),(0,r.jsx)(n.td,{children:"0.60"}),(0,r.jsx)(n.td,{children:"Missing Make/Model/Lens"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Device aspect"}),(0,r.jsx)(n.td,{children:"0.60"}),(0,r.jsx)(n.td,{children:"Exact phone screen ratio"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Perfect sharpness"}),(0,r.jsx)(n.td,{children:"0.50"}),(0,r.jsx)(n.td,{children:">2000 Laplacian variance"})]})]})]})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);