{
  "name": "skill-logger",
  "type": "folder",
  "path": "skill-logger",
  "children": [
    {
      "name": "references",
      "type": "folder",
      "path": "skill-logger/references",
      "children": [
        {
          "name": "scoring-rubric.md",
          "type": "file",
          "path": "skill-logger/references/scoring-rubric.md",
          "size": 14010,
          "content": "# Skill Scoring Rubric\n\n## Overview\n\nThis rubric defines how skill invocations are scored for quality, enabling data-driven improvement of the skill ecosystem.\n\n## Multi-Dimensional Scoring Model\n\n### Score Components\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                    SKILL QUALITY SCORE (0-100)                   │\n├─────────────────────────────────────────────────────────────────┤\n│                                                                  │\n│  COMPLETION (25%)        EFFICIENCY (20%)                        │\n│  ├─ Task completed?      ├─ Token economy                       │\n│  ├─ No errors?           ├─ Tool call efficiency                │\n│  └─ Graceful recovery?   └─ Response time                       │\n│                                                                  │\n│  OUTPUT QUALITY (30%)    USER SATISFACTION (25%)                 │\n│  ├─ Accuracy             ├─ Accepted without edits?             │\n│  ├─ Completeness         ├─ Follow-up needed?                   │\n│  └─ Code quality (if applicable) └─ Explicit feedback           │\n│                                                                  │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n## Component Breakdown\n\n### 1. Completion Score (25 points max)\n\n| Outcome | Points | Description |\n|---------|--------|-------------|\n| Full completion, no errors | 25 | Task completed exactly as requested |\n| Completion with recovery | 20 | Hit error but recovered gracefully |\n| Partial completion | 15 | Some of the task accomplished |\n| Completion with workaround | 10 | Achieved goal via alternative path |\n| Failed but informative | 5 | Couldn't complete but explained why |\n| Hard failure | 0 | Crashed, hung, or produced nothing |\n\n```python\ndef score_completion(invocation: dict) -> int:\n    \"\"\"Score task completion (0-25 points).\"\"\"\n\n    errors = invocation.get('errors', [])\n    recovered = invocation.get('recovered', False)\n    partial = invocation.get('partial_completion', False)\n\n    if not errors:\n        return 25  # Perfect completion\n\n    if recovered:\n        return 20  # Recovered from error\n\n    if partial:\n        return 15  # Partial completion\n\n    if invocation.get('workaround_used'):\n        return 10  # Alternative path\n\n    if invocation.get('failure_explained'):\n        return 5   # At least explained the issue\n\n    return 0  # Hard failure\n```\n\n### 2. Efficiency Score (20 points max)\n\n| Metric | Max Points | Calculation |\n|--------|------------|-------------|\n| Token efficiency | 8 | `8 * min(1, baseline_tokens / actual_tokens)` |\n| Tool call efficiency | 6 | `6 * min(1, baseline_calls / actual_calls)` |\n| Response time | 6 | `6 * min(1, baseline_time / actual_time)` |\n\n```python\n# Baseline values by skill category\nEFFICIENCY_BASELINES = {\n    'code_generation': {\n        'tokens_per_loc': 50,      # Tokens per line of code generated\n        'calls_per_file': 3,       # Tool calls per file modified\n        'time_per_task': 30_000,   # Milliseconds per typical task\n    },\n    'analysis': {\n        'tokens_per_insight': 200,\n        'calls_per_analysis': 5,\n        'time_per_task': 20_000,\n    },\n    'design': {\n        'tokens_per_component': 150,\n        'calls_per_design': 4,\n        'time_per_task': 45_000,\n    },\n    'research': {\n        'tokens_per_finding': 100,\n        'calls_per_search': 8,\n        'time_per_task': 60_000,\n    },\n}\n\ndef score_efficiency(invocation: dict, skill_category: str) -> int:\n    \"\"\"Score efficiency (0-20 points).\"\"\"\n\n    baselines = EFFICIENCY_BASELINES.get(skill_category, EFFICIENCY_BASELINES['analysis'])\n\n    # Token efficiency (0-8 points)\n    actual_tokens = invocation['tokens_used']\n    output_size = invocation.get('output_size', 1)  # LOC, components, etc.\n    expected_tokens = baselines['tokens_per_loc'] * output_size\n    token_score = 8 * min(1.0, expected_tokens / max(actual_tokens, 1))\n\n    # Tool call efficiency (0-6 points)\n    actual_calls = len(invocation.get('tool_calls', []))\n    expected_calls = baselines['calls_per_file'] * invocation.get('files_changed', 1)\n    call_score = 6 * min(1.0, expected_calls / max(actual_calls, 1))\n\n    # Response time (0-6 points)\n    actual_time = invocation['duration_ms']\n    expected_time = baselines['time_per_task']\n    time_score = 6 * min(1.0, expected_time / max(actual_time, 1))\n\n    return int(token_score + call_score + time_score)\n```\n\n### 3. Output Quality Score (30 points max)\n\n| Metric | Max Points | Description |\n|--------|------------|-------------|\n| Accuracy | 12 | Output is correct and appropriate |\n| Completeness | 10 | All aspects of request addressed |\n| Code quality | 8 | Clean, idiomatic, no obvious bugs |\n\n```python\ndef score_output_quality(invocation: dict, feedback: dict = None) -> int:\n    \"\"\"Score output quality (0-30 points).\"\"\"\n\n    total = 0\n\n    # Accuracy (0-12 points)\n    if feedback:\n        # Direct user feedback\n        if feedback.get('accurate') == True:\n            total += 12\n        elif feedback.get('mostly_accurate'):\n            total += 9\n        elif feedback.get('partially_accurate'):\n            total += 6\n    else:\n        # Heuristic scoring\n        if invocation.get('output_validated'):\n            total += 12\n        elif not invocation.get('errors'):\n            total += 8  # Assume reasonable accuracy if no errors\n\n    # Completeness (0-10 points)\n    requested_items = invocation.get('requested_items', 1)\n    delivered_items = invocation.get('delivered_items', 1)\n    completeness_ratio = delivered_items / max(requested_items, 1)\n    total += int(10 * min(1.0, completeness_ratio))\n\n    # Code quality (0-8 points) - if applicable\n    if invocation.get('output_type') == 'code':\n        quality_signals = invocation.get('code_quality', {})\n\n        # Linter passed\n        if quality_signals.get('linter_passed', True):\n            total += 3\n\n        # Type safe\n        if quality_signals.get('types_valid', True):\n            total += 2\n\n        # Tests pass (if tests were run)\n        if quality_signals.get('tests_passed'):\n            total += 3\n    else:\n        total += 8  # Full points for non-code output\n\n    return total\n```\n\n### 4. User Satisfaction Score (25 points max)\n\n| Signal | Max Points | Description |\n|--------|------------|-------------|\n| Accepted as-is | 10 | User used output without modification |\n| Edit ratio | 8 | `8 * (1 - edit_ratio)` |\n| No follow-up | 7 | User didn't need to ask for fixes |\n\n```python\ndef score_user_satisfaction(invocation: dict, follow_ups: list = None) -> int:\n    \"\"\"Score user satisfaction (0-25 points).\"\"\"\n\n    total = 0\n\n    # Accepted without changes (0-10 points)\n    if invocation.get('user_accepted'):\n        if invocation.get('user_edit_ratio', 0) < 0.05:\n            total += 10  # Accepted as-is\n        else:\n            total += 7   # Accepted with minor edits\n    elif invocation.get('user_used_output'):\n        total += 5       # Used but modified\n\n    # Edit ratio (0-8 points)\n    edit_ratio = invocation.get('user_edit_ratio', 0.5)\n    total += int(8 * (1 - min(edit_ratio, 1.0)))\n\n    # No follow-up needed (0-7 points)\n    if follow_ups is None or len(follow_ups) == 0:\n        total += 7\n    elif len(follow_ups) == 1:\n        total += 4  # One clarifying question is okay\n    elif all(f.get('resolved') for f in follow_ups):\n        total += 2  # Follow-ups were resolved\n\n    return total\n```\n\n## Score Interpretation\n\n### Quality Tiers\n\n| Score Range | Tier | Description | Action |\n|-------------|------|-------------|--------|\n| 90-100 | Excellent | Exceptional performance | Document as exemplar |\n| 75-89 | Good | Meets expectations | Monitor for consistency |\n| 60-74 | Acceptable | Room for improvement | Review for patterns |\n| 40-59 | Below Average | Significant issues | Prioritize improvements |\n| 20-39 | Poor | Major problems | Immediate attention needed |\n| 0-19 | Failing | Critical failure | Investigate root cause |\n\n### Trend Analysis\n\n```python\ndef analyze_skill_trends(skill_name: str, days: int = 30) -> dict:\n    \"\"\"Analyze quality trends for a skill.\"\"\"\n\n    # Get recent invocations\n    invocations = get_invocations(skill_name, days=days)\n\n    if len(invocations) < 10:\n        return {'status': 'insufficient_data'}\n\n    # Calculate rolling averages\n    scores = [inv['quality_score'] for inv in invocations]\n    recent_avg = np.mean(scores[-7:])    # Last week\n    previous_avg = np.mean(scores[:-7])  # Before that\n\n    # Trend detection\n    trend = 'stable'\n    change_pct = (recent_avg - previous_avg) / max(previous_avg, 1) * 100\n\n    if change_pct > 10:\n        trend = 'improving'\n    elif change_pct < -10:\n        trend = 'declining'\n\n    # Identify weak components\n    component_avgs = {\n        'completion': np.mean([inv['scores']['completion'] for inv in invocations]),\n        'efficiency': np.mean([inv['scores']['efficiency'] for inv in invocations]),\n        'quality': np.mean([inv['scores']['quality'] for inv in invocations]),\n        'satisfaction': np.mean([inv['scores']['satisfaction'] for inv in invocations]),\n    }\n\n    weak_component = min(component_avgs, key=lambda k: component_avgs[k] / COMPONENT_MAX[k])\n\n    return {\n        'current_avg': recent_avg,\n        'previous_avg': previous_avg,\n        'trend': trend,\n        'change_percent': change_pct,\n        'weak_component': weak_component,\n        'component_scores': component_avgs,\n        'recommendation': get_improvement_recommendation(weak_component, component_avgs[weak_component])\n    }\n\ndef get_improvement_recommendation(component: str, score: float) -> str:\n    \"\"\"Get specific improvement recommendation based on weak component.\"\"\"\n\n    recommendations = {\n        'completion': {\n            'low': 'Add more error handling and recovery patterns to SKILL.md',\n            'medium': 'Review common failure cases and add guidance',\n        },\n        'efficiency': {\n            'low': 'Reduce context size, consider progressive disclosure',\n            'medium': 'Optimize tool call patterns, reduce unnecessary reads',\n        },\n        'quality': {\n            'low': 'Add more examples and anti-patterns to skill',\n            'medium': 'Include validation steps in skill workflow',\n        },\n        'satisfaction': {\n            'low': 'Gather user feedback, analyze edit patterns',\n            'medium': 'Add clarifying questions to skill workflow',\n        },\n    }\n\n    level = 'low' if score < 50 else 'medium'\n    return recommendations.get(component, {}).get(level, 'Review skill performance')\n```\n\n## Automated Quality Gates\n\n```python\nQUALITY_GATES = {\n    'publish': {\n        'min_score': 70,\n        'min_invocations': 10,\n        'max_error_rate': 0.1,\n        'description': 'Minimum requirements to publish a skill'\n    },\n    'feature': {\n        'min_score': 85,\n        'min_invocations': 50,\n        'max_error_rate': 0.05,\n        'description': 'Requirements to be featured in showcase'\n    },\n    'deprecation': {\n        'max_score': 40,\n        'min_invocations': 20,\n        'max_age_without_improvement': 90,\n        'description': 'Triggers deprecation review'\n    }\n}\n\ndef check_quality_gate(skill_name: str, gate: str) -> dict:\n    \"\"\"Check if skill passes a quality gate.\"\"\"\n\n    gate_config = QUALITY_GATES[gate]\n    stats = get_skill_stats(skill_name)\n\n    passed = True\n    failures = []\n\n    if stats['avg_score'] < gate_config.get('min_score', 0):\n        passed = False\n        failures.append(f\"Score {stats['avg_score']:.1f} < {gate_config['min_score']}\")\n\n    if stats['invocation_count'] < gate_config.get('min_invocations', 0):\n        passed = False\n        failures.append(f\"Invocations {stats['invocation_count']} < {gate_config['min_invocations']}\")\n\n    if stats['error_rate'] > gate_config.get('max_error_rate', 1.0):\n        passed = False\n        failures.append(f\"Error rate {stats['error_rate']:.2%} > {gate_config['max_error_rate']:.2%}\")\n\n    return {\n        'gate': gate,\n        'passed': passed,\n        'failures': failures,\n        'stats': stats\n    }\n```\n\n## Dashboard Queries\n\n```sql\n-- Skill leaderboard\nSELECT\n    skill_name,\n    COUNT(*) as uses,\n    AVG(quality_score) as avg_score,\n    AVG(completion_score) as avg_completion,\n    AVG(efficiency_score) as avg_efficiency,\n    AVG(quality_score_component) as avg_quality,\n    AVG(satisfaction_score) as avg_satisfaction\nFROM skill_invocations\nWHERE timestamp > datetime('now', '-30 days')\nGROUP BY skill_name\nHAVING uses >= 5\nORDER BY avg_score DESC;\n\n-- Quality distribution\nSELECT\n    skill_name,\n    CASE\n        WHEN quality_score >= 90 THEN 'Excellent'\n        WHEN quality_score >= 75 THEN 'Good'\n        WHEN quality_score >= 60 THEN 'Acceptable'\n        WHEN quality_score >= 40 THEN 'Below Average'\n        ELSE 'Poor'\n    END as tier,\n    COUNT(*) as count\nFROM skill_invocations\nWHERE timestamp > datetime('now', '-30 days')\nGROUP BY skill_name, tier;\n\n-- Improvement opportunities\nSELECT\n    skill_name,\n    'completion' as weak_area,\n    AVG(completion_score) / 25.0 as normalized_score\nFROM skill_invocations\nWHERE timestamp > datetime('now', '-30 days')\nGROUP BY skill_name\nHAVING normalized_score < 0.7\n\nUNION ALL\n\nSELECT\n    skill_name,\n    'efficiency' as weak_area,\n    AVG(efficiency_score) / 20.0 as normalized_score\nFROM skill_invocations\nWHERE timestamp > datetime('now', '-30 days')\nGROUP BY skill_name\nHAVING normalized_score < 0.7\n\nORDER BY normalized_score ASC;\n```\n"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "skill-logger/SKILL.md",
      "size": 14250,
      "content": "---\nname: skill-logger\ndescription: Logs and scores skill usage quality, tracking output effectiveness, user satisfaction signals, and improvement opportunities. Expert in skill analytics, quality metrics, feedback loops, and continuous improvement. Activate on \"skill logging\", \"skill quality\", \"skill analytics\", \"skill scoring\", \"skill performance\", \"skill metrics\", \"track skill usage\", \"skill improvement\". NOT for creating skills (use agent-creator), skill documentation (use skill-coach), or runtime debugging (use debugger skills).\nallowed-tools: Read,Write,Edit,Bash,Grep,Glob\ncategory: Productivity & Meta\ntags:\n  - logging\n  - analytics\n  - metrics\n  - quality\n  - improvement\npairs-with:\n  - skill: automatic-stateful-prompt-improver\n    reason: Data for prompt optimization\n  - skill: skill-coach\n    reason: Quality tracking feeds coaching\n---\n\n# Skill Logger\n\nTrack, measure, and improve skill quality through systematic logging and scoring.\n\n## When to Use This Skill\n\n**Use for:**\n- Setting up skill usage logging\n- Defining quality metrics for skill outputs\n- Analyzing skill performance over time\n- Identifying skills that need improvement\n- Building feedback loops for skill enhancement\n- A/B testing skill variations\n\n**NOT for:**\n- Creating new skills → use agent-creator\n- Skill documentation → use skill-coach\n- Runtime debugging → use appropriate debugger skills\n- General logging/monitoring → use devops-automator\n\n## Core Logging Architecture\n\n```\n┌────────────────────────────────────────────────────────────────┐\n│                    SKILL LOGGING PIPELINE                       │\n├────────────────────────────────────────────────────────────────┤\n│                                                                 │\n│  1. CAPTURE          2. ANALYZE           3. SCORE              │\n│  ├─ Invocation       ├─ Output parse      ├─ Quality metrics    │\n│  ├─ Input context    ├─ Token usage       ├─ User satisfaction  │\n│  ├─ Output           ├─ Tool calls        ├─ Goal completion    │\n│  └─ Timing           └─ Error patterns    └─ Efficiency         │\n│                                                                 │\n│  4. AGGREGATE        5. ALERT             6. IMPROVE            │\n│  ├─ Per-skill stats  ├─ Quality drops     ├─ Identify patterns  │\n│  ├─ Trend analysis   ├─ Error spikes      ├─ Suggest changes    │\n│  └─ Comparisons      └─ Underuse          └─ Track experiments  │\n│                                                                 │\n└────────────────────────────────────────────────────────────────┘\n```\n\n## What to Log\n\n### Invocation Data\n\n```json\n{\n  \"invocation_id\": \"uuid\",\n  \"timestamp\": \"ISO8601\",\n  \"skill_name\": \"wedding-immortalist\",\n  \"skill_version\": \"1.2.0\",\n\n  \"input\": {\n    \"user_query\": \"Create a 3D model from my wedding photos\",\n    \"context_tokens\": 1500,\n    \"files_referenced\": [\"photos/\", \"config.json\"]\n  },\n\n  \"execution\": {\n    \"duration_ms\": 45000,\n    \"tool_calls\": [\n      {\"tool\": \"Bash\", \"count\": 5},\n      {\"tool\": \"Write\", \"count\": 3}\n    ],\n    \"tokens_used\": {\n      \"input\": 8500,\n      \"output\": 3200\n    },\n    \"errors\": []\n  },\n\n  \"output\": {\n    \"type\": \"code_generation\",\n    \"artifacts_created\": [\"pipeline.py\", \"config.yaml\"],\n    \"response_length\": 3200\n  }\n}\n```\n\n### Quality Signals\n\n```python\nQUALITY_SIGNALS = {\n    # Implicit signals (automated)\n    'completion': 'Did the skill complete without errors?',\n    'token_efficiency': 'Output quality per token used',\n    'tool_success_rate': 'Tool calls that succeeded',\n    'retry_count': 'How many retries needed?',\n\n    # Explicit signals (user feedback)\n    'user_edit_ratio': 'How much did user modify output?',\n    'user_accepted': 'Did user accept/use the output?',\n    'follow_up_needed': 'Did user need to ask for fixes?',\n    'explicit_rating': 'Thumbs up/down if available',\n\n    # Outcome signals (delayed)\n    'code_ran_successfully': 'Did generated code work?',\n    'tests_passed': 'Did it pass tests?',\n    'reverted': 'Was the output later reverted?',\n}\n```\n\n## Scoring Framework\n\n### Multi-Dimensional Quality Score\n\n```python\ndef calculate_skill_score(invocation_log):\n    \"\"\"Score a skill invocation 0-100.\"\"\"\n\n    scores = {\n        # Completion (25%)\n        'completion': (\n            25 if invocation_log['errors'] == [] else\n            15 if invocation_log['recovered'] else\n            0\n        ),\n\n        # Efficiency (20%)\n        'efficiency': min(20, 20 * (\n            BASELINE_TOKENS / invocation_log['tokens_used']\n        )),\n\n        # Output Quality (30%)\n        'quality': (\n            30 if invocation_log['user_accepted'] else\n            20 if invocation_log['user_edit_ratio'] < 0.2 else\n            10 if invocation_log['user_edit_ratio'] < 0.5 else\n            0\n        ),\n\n        # User Satisfaction (25%)\n        'satisfaction': (\n            25 if invocation_log['explicit_rating'] == 'positive' else\n            15 if invocation_log['no_follow_up'] else\n            5 if invocation_log['follow_up_resolved'] else\n            0\n        ),\n    }\n\n    return sum(scores.values())\n```\n\n### Score Interpretation\n\n| Score Range | Quality Level | Action |\n|-------------|---------------|--------|\n| 90-100 | Excellent | Document as exemplar |\n| 75-89 | Good | Monitor for consistency |\n| 50-74 | Acceptable | Review for improvements |\n| 25-49 | Poor | Prioritize fixes |\n| 0-24 | Failing | Immediate intervention |\n\n## Log Storage Schema\n\n### SQLite Schema (Local)\n\n```sql\nCREATE TABLE skill_invocations (\n    id TEXT PRIMARY KEY,\n    skill_name TEXT NOT NULL,\n    skill_version TEXT,\n    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,\n\n    -- Input\n    user_query TEXT,\n    context_tokens INTEGER,\n\n    -- Execution\n    duration_ms INTEGER,\n    tokens_input INTEGER,\n    tokens_output INTEGER,\n    tool_calls_json TEXT,\n    errors_json TEXT,\n\n    -- Output\n    output_type TEXT,\n    artifacts_json TEXT,\n    response_length INTEGER,\n\n    -- Quality signals\n    user_accepted BOOLEAN,\n    user_edit_ratio REAL,\n    follow_up_needed BOOLEAN,\n    explicit_rating TEXT,\n\n    -- Computed\n    quality_score REAL,\n\n    INDEX idx_skill_name (skill_name),\n    INDEX idx_timestamp (timestamp),\n    INDEX idx_quality (quality_score)\n);\n\nCREATE TABLE skill_aggregates (\n    skill_name TEXT,\n    period TEXT,  -- 'daily', 'weekly', 'monthly'\n    period_start DATE,\n\n    invocation_count INTEGER,\n    avg_quality_score REAL,\n    error_rate REAL,\n    avg_tokens_used INTEGER,\n    avg_duration_ms INTEGER,\n\n    PRIMARY KEY (skill_name, period, period_start)\n);\n```\n\n### JSON Log Format (Portable)\n\n```json\n{\n  \"logs_version\": \"1.0\",\n  \"skill_name\": \"wedding-immortalist\",\n  \"entries\": [\n    {\n      \"id\": \"uuid\",\n      \"timestamp\": \"2025-01-15T14:30:00Z\",\n      \"input\": {...},\n      \"execution\": {...},\n      \"output\": {...},\n      \"quality\": {\n        \"signals\": {...},\n        \"score\": 85,\n        \"computed_at\": \"2025-01-15T14:35:00Z\"\n      }\n    }\n  ]\n}\n```\n\n## Analytics Queries\n\n### Skill Performance Dashboard\n\n```sql\n-- Overall skill rankings\nSELECT\n    skill_name,\n    COUNT(*) as uses,\n    AVG(quality_score) as avg_quality,\n    AVG(tokens_output) as avg_tokens,\n    SUM(CASE WHEN errors_json != '[]' THEN 1 ELSE 0 END) * 100.0 / COUNT(*) as error_rate\nFROM skill_invocations\nWHERE timestamp > datetime('now', '-30 days')\nGROUP BY skill_name\nORDER BY avg_quality DESC;\n\n-- Quality trend (weekly)\nSELECT\n    skill_name,\n    strftime('%Y-%W', timestamp) as week,\n    AVG(quality_score) as avg_quality,\n    COUNT(*) as uses\nFROM skill_invocations\nGROUP BY skill_name, week\nORDER BY skill_name, week;\n\n-- Problem detection\nSELECT skill_name, COUNT(*) as failures\nFROM skill_invocations\nWHERE quality_score < 50\n  AND timestamp > datetime('now', '-7 days')\nGROUP BY skill_name\nHAVING failures >= 3\nORDER BY failures DESC;\n```\n\n### Improvement Opportunities\n\n```python\ndef identify_improvement_opportunities(skill_name, logs):\n    \"\"\"Analyze logs to suggest skill improvements.\"\"\"\n\n    opportunities = []\n\n    # Pattern 1: Common follow-up questions\n    follow_ups = extract_follow_up_patterns(logs)\n    if follow_ups:\n        opportunities.append({\n            'type': 'missing_capability',\n            'description': f'Users frequently ask: {follow_ups[0]}',\n            'suggestion': 'Add guidance for this common need'\n        })\n\n    # Pattern 2: High edit ratio in specific output types\n    edit_patterns = analyze_edit_patterns(logs)\n    if edit_patterns['code'] > 0.4:\n        opportunities.append({\n            'type': 'code_quality',\n            'description': 'Users frequently edit generated code',\n            'suggestion': 'Review code examples and templates'\n        })\n\n    # Pattern 3: Repeated errors\n    error_patterns = cluster_errors(logs)\n    for error_type, count in error_patterns:\n        if count >= 3:\n            opportunities.append({\n                'type': 'recurring_error',\n                'description': f'{error_type} occurred {count} times',\n                'suggestion': 'Add error handling or documentation'\n            })\n\n    return opportunities\n```\n\n## Implementation Guide\n\n### Basic Logger Hook\n\n```python\n# hooks/skill_logger.py\nimport json\nimport sqlite3\nfrom datetime import datetime\nfrom pathlib import Path\n\nLOG_DB = Path.home() / '.claude' / 'skill_logs.db'\n\ndef log_skill_invocation(\n    skill_name: str,\n    user_query: str,\n    output: str,\n    tool_calls: list,\n    duration_ms: int,\n    tokens: dict,\n    errors: list = None\n):\n    \"\"\"Log a skill invocation to the database.\"\"\"\n\n    conn = sqlite3.connect(LOG_DB)\n    cursor = conn.cursor()\n\n    cursor.execute('''\n        INSERT INTO skill_invocations\n        (id, skill_name, timestamp, user_query, duration_ms,\n         tokens_input, tokens_output, tool_calls_json, errors_json,\n         response_length)\n        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n    ''', (\n        str(uuid.uuid4()),\n        skill_name,\n        datetime.utcnow().isoformat(),\n        user_query,\n        duration_ms,\n        tokens.get('input', 0),\n        tokens.get('output', 0),\n        json.dumps(tool_calls),\n        json.dumps(errors or []),\n        len(output)\n    ))\n\n    conn.commit()\n    conn.close()\n```\n\n### Quality Signal Collection\n\n```python\ndef collect_quality_signals(invocation_id: str, signals: dict):\n    \"\"\"Update an invocation with quality signals.\"\"\"\n\n    conn = sqlite3.connect(LOG_DB)\n    cursor = conn.cursor()\n\n    # Update with user feedback\n    cursor.execute('''\n        UPDATE skill_invocations\n        SET user_accepted = ?,\n            user_edit_ratio = ?,\n            follow_up_needed = ?,\n            explicit_rating = ?,\n            quality_score = ?\n        WHERE id = ?\n    ''', (\n        signals.get('accepted'),\n        signals.get('edit_ratio'),\n        signals.get('follow_up'),\n        signals.get('rating'),\n        calculate_score(signals),\n        invocation_id\n    ))\n\n    conn.commit()\n    conn.close()\n```\n\n## Alerting & Notifications\n\n### Alert Conditions\n\n```python\nALERT_CONDITIONS = {\n    'quality_drop': {\n        'condition': 'avg_quality_7d < avg_quality_30d * 0.8',\n        'message': 'Skill {skill} quality dropped 20%+ in past week',\n        'severity': 'warning'\n    },\n    'error_spike': {\n        'condition': 'error_rate_24h > error_rate_7d * 2',\n        'message': 'Skill {skill} error rate doubled in past 24h',\n        'severity': 'critical'\n    },\n    'underused': {\n        'condition': 'uses_7d < uses_30d_avg * 0.5',\n        'message': 'Skill {skill} usage down 50%+ this week',\n        'severity': 'info'\n    },\n    'high_performer': {\n        'condition': 'avg_quality_7d > 90 AND uses_7d > 10',\n        'message': 'Skill {skill} performing excellently',\n        'severity': 'positive'\n    }\n}\n```\n\n## Anti-Patterns\n\n### \"Log Everything\"\n**Wrong**: Logging complete input/output for every invocation.\n**Why**: Privacy concerns, storage explosion, noise.\n**Right**: Log metadata, summaries, and opt-in detailed logging.\n\n### \"Score Once, Forget\"\n**Wrong**: Calculating quality score immediately after completion.\n**Why**: Misses delayed signals (did code work? was it reverted?).\n**Right**: Collect signals over time, recalculate periodically.\n\n### \"Averages Only\"\n**Wrong**: Only tracking average quality scores.\n**Why**: Hides distribution, misses failure modes.\n**Right**: Track percentiles, failure rates, and patterns.\n\n### \"No Baseline\"\n**Wrong**: Measuring quality without establishing baselines.\n**Why**: Can't detect improvement or regression.\n**Right**: Establish baselines per skill, compare trends.\n\n## Output Reports\n\n### Weekly Skill Health Report\n\n```markdown\n# Skill Health Report - Week of 2025-01-13\n\n## Overview\n- Total invocations: 247\n- Average quality: 78.3 (up 2.1 from last week)\n- Error rate: 4.2% (down 1.8%)\n\n## Top Performers\n1. **wedding-immortalist** - 92.1 avg quality, 18 uses\n2. **skill-coach** - 89.4 avg quality, 34 uses\n3. **api-architect** - 87.2 avg quality, 22 uses\n\n## Needs Attention\n1. **legacy-code-converter** - 52.3 avg quality (down 15%)\n   - Common issue: Missing dependency detection\n   - Suggested fix: Add dependency scanning step\n\n## Improvement Opportunities\n- `partner-text-coach`: Users frequently ask for tone adjustment\n- `yard-landscaper`: High edit ratio on plant recommendations\n```\n\n## Integration Points\n\n- **skill-coach**: Feed quality data for skill improvements\n- **agent-creator**: Use metrics when designing new skills\n- **automatic-stateful-prompt-improver**: Quality signals for prompt optimization\n\n---\n\n**Core Philosophy**: What gets measured gets improved. Skill logging transforms intuition about skill quality into actionable data, enabling continuous improvement of the entire skill ecosystem.\n"
    }
  ]
}